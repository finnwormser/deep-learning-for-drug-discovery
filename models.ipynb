{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549273e0",
   "metadata": {
    "id": "549273e0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### AI for Drug Discovery\n",
    "\n",
    "Drug discovery traditionally involves:\n",
    "1. Identifying a biological target (e.g., an enzyme in bacteria).\n",
    "2. Screening large collections of molecules to find those with activity against the target.\n",
    "3. Optimizing hits into drug-like molecules.\n",
    "\n",
    "AI reshapes this pipeline by:\n",
    "* Predicting properties (activity, toxicity, pharmacokinetics) from molecular structures.\n",
    "* Generating new molecules with desired properties.\n",
    "* Providing natural language interface to interact with scientists.\n",
    "\n",
    "Deep learning is particularly well-suited because it can automatically extract useful representations of molecules from raw formats (SMILES, molecular graphs, or 3D structures) rather than relying entirely on handcrafted features.\n",
    "\n",
    "---\n",
    "\n",
    "### Molecules and SMILES\n",
    "\n",
    "A molecule is a collection of atoms bonded together, forming the fundamental units of chemicals that make up the world around us. In drug discovery, molecules are potential small-molecule drugs that can interact with biological targets such as proteins, DNA, or membranes.\n",
    "\n",
    "To represent molecules in a form that computers can understand, one common format is SMILES (Simplified Molecular Input Line Entry System).\n",
    "* SMILES encodes the structure of a molecule as a text string, using characters to denote atoms (e.g., C for carbon, O for oxygen), symbols for bonds (e.g., = for double bond), and parentheses for branches.\n",
    "* Examples:\n",
    "  * Ethanol has SMILES CCO (two carbons and an oxygen).\n",
    "  * Benzene has SMILES c1ccccc1.\n",
    "\n",
    "SMILES are convenient because they are compact, human-readable, and directly usable in sequence models (CNNs, RNNs, Transformers). But they are not unique — the same molecule can have multiple valid SMILES strings — and they lose explicit 3D information. Therefore, we will also explore graph-based models (GNNs and EGNNs)\n",
    "\n",
    "---\n",
    "\n",
    "### Molecular Property Prediction\n",
    "\n",
    "Molecular property prediction is the task of predicting how a molecule behaves — its physical, chemical, or biological properties — directly from its structure. Examples include:\n",
    "* Solubility in water.\n",
    "* Toxicity to human cells.\n",
    "* Binding affinity to a protein target.\n",
    "* Biological activity, such as antibacterial activity.\n",
    "\n",
    "This problem is central to drug discovery, because experimentally measuring properties for millions of molecules is slow and expensive. AI models can act as filters, rapidly prioritizing which molecules are promising candidates for laboratory testing.\n",
    "\n",
    "---\n",
    "\n",
    "### Antibiotic Discovery\n",
    "\n",
    "In this project, I explore the use of deep learning models to predict whether a small molecule has antibacterial activity against Pseudomonas aeruginosa. The dataset includes SMILES strings paired with binary activity labels (1 = active, 0 = inactive), split into training, validation, and test sets.\n",
    "\n",
    "The goal is to compare several neural architectures for molecular property prediction, using PyTorch implementations built from scratch. The models include:\n",
    "\n",
    "A feedforward network trained on Morgan fingerprints\n",
    "\n",
    "1D CNNs applied to tokenized SMILES\n",
    "\n",
    "Recurrent models (GRU/LSTM) over SMILES sequences\n",
    "\n",
    "Transformer-based sequence models\n",
    "\n",
    "Graph neural networks (GCN, GIN, GraphSAGE) operating on molecular graphs\n",
    "\n",
    "Equivariant graph neural networks (EGNN) incorporating 3D structural information\n",
    "\n",
    "I built a full training pipeline with early stopping, checkpointing, and evaluation on a held-out test set. After training all models, I compared their predictive performance and analyzed how different input representations (fingerprints, sequences, 2D graphs, 3D graphs) influence accuracy on this antibacterial prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69398922-d40d-4853-b5e5-58936e263a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch, torch_geometric\n",
    "print(torch.__version__, torch_geometric.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40767762",
   "metadata": {
    "id": "40767762"
   },
   "outputs": [],
   "source": [
    "# %pip install torch torchvision #--index-url https://download.pytorch.org/whl/cu126  # uncomment this if you have a GPU\n",
    "# %pip install pandas numpy scikit-learn tqdm\n",
    "# %pip install rdkit\n",
    "# %pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv #-f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__.split('+')[0])\").html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f113642",
   "metadata": {
    "id": "1f113642"
   },
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffb227d",
   "metadata": {
    "id": "1ffb227d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_dir\": \"\",\n",
      "  \"smiles_col\": \"smiles\",\n",
      "  \"target_cols\": [\n",
      "    \"activity\"\n",
      "  ],\n",
      "  \"task_type\": \"classification\",\n",
      "  \"seed\": 42,\n",
      "  \"batch_size\": 8,\n",
      "  \"num_workers\": 0,\n",
      "  \"max_epochs\": 50,\n",
      "  \"patience\": 10,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"grad_clip\": 1.0,\n",
      "  \"fp_nbits\": 2048,\n",
      "  \"fp_radius\": 2,\n",
      "  \"max_len\": 256,\n",
      "  \"vocab_extra\": \"\",\n",
      "  \"embed_dim\": 256,\n",
      "  \"cnn_channels\": 256,\n",
      "  \"cnn_kernel_sizes\": [\n",
      "    3,\n",
      "    5,\n",
      "    7\n",
      "  ],\n",
      "  \"rnn_hidden\": 256,\n",
      "  \"rnn_layers\": 2,\n",
      "  \"transformer_heads\": 8,\n",
      "  \"transformer_layers\": 4,\n",
      "  \"transformer_ff\": 512,\n",
      "  \"dropout\": 0.1,\n",
      "  \"pyg_backend\": \"gcn\",\n",
      "  \"gnn_hidden\": 256,\n",
      "  \"gnn_layers\": 4,\n",
      "  \"gnn_dropout\": 0.1,\n",
      "  \"global_pool\": \"mean\",\n",
      "  \"egnn_hidden\": 128,\n",
      "  \"egnn_layers\": 4,\n",
      "  \"egnn_use_radial\": true,\n",
      "  \"egnn_cutoff\": 8.0,\n",
      "  \"egnn_conformers\": 1,\n",
      "  \"out_dir\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    data_dir: str = \"\"      # Insert path containing train.csv, val.csv, test.csv\n",
    "    smiles_col: str = \"smiles\"\n",
    "    target_cols: tuple = (\"activity\",)  # can be multiple targets: (\"y_0\",\"y_1\",...)\n",
    "    task_type: str = \"classification\"  # \"regression\" or \"classification\"\n",
    "\n",
    "    # General training\n",
    "    seed: int = 42\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 0\n",
    "    max_epochs: int = 50\n",
    "    patience: int = 10           # early stopping\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Fingerprints\n",
    "    fp_nbits: int = 2048\n",
    "    fp_radius: int = 2\n",
    "\n",
    "    # SMILES sequence models\n",
    "    max_len: int = 256\n",
    "    vocab_extra: str = \"\"        # optional custom tokens\n",
    "    embed_dim: int = 256\n",
    "    cnn_channels: int = 256\n",
    "    cnn_kernel_sizes: tuple = (3,5,7)\n",
    "    rnn_hidden: int = 256\n",
    "    rnn_layers: int = 2\n",
    "    transformer_heads: int = 8\n",
    "    transformer_layers: int = 4\n",
    "    transformer_ff: int = 512\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Graph models\n",
    "    pyg_backend: str = \"gcn\"     # \"gcn\", \"gin\", \"graphsage\"\n",
    "    gnn_hidden: int = 256\n",
    "    gnn_layers: int = 4\n",
    "    gnn_dropout: float = 0.1\n",
    "    global_pool: str = \"mean\"    # \"mean\", \"add\", \"max\"\n",
    "\n",
    "    # EGNN (3D)\n",
    "    egnn_hidden: int = 128\n",
    "    egnn_layers: int = 4\n",
    "    egnn_use_radial: bool = True\n",
    "    egnn_cutoff: float = 8.0\n",
    "    egnn_conformers: int = 1     # number of conformers to generate if 3D not given\n",
    "\n",
    "    # Logging/checkpoint\n",
    "    out_dir: str = \"\" #insert output dir\n",
    "\n",
    "cfg = Config()\n",
    "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(asdict(cfg), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fb37f",
   "metadata": {
    "id": "2c7fb37f"
   },
   "source": [
    "## 2) Utilities (Seed, Device, Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfa2f1e",
   "metadata": {
    "id": "7dfa2f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def classification_metrics(y_true, y_pred_proba):\n",
    "    metrics = {}\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred_proba = np.asarray(y_pred_proba)\n",
    "    metrics[\"ROC-AUC\"] = roc_auc_score(y_true, y_pred_proba)\n",
    "    metrics[\"AP\"] = average_precision_score(y_true, y_pred_proba)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa64cca",
   "metadata": {
    "id": "3fa64cca"
   },
   "source": [
    "## 3) Data Loading\n",
    "\n",
    "We support three representations:\n",
    "- **Fingerprints (Morgan/ECFP)** → fast MLP baseline.\n",
    "- **SMILES-as-sequence** → CNN/RNN/Transformer.\n",
    "- **Molecular graphs (± 3D)** → GNN/EGNN via RDKit + PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9236a1b",
   "metadata": {
    "id": "f9236a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 1421 474 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1=CC(=C(C=C1C(CN)O)O)O.[C@@H]([C@H](C(=O)O)O)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(=O)OC1=CC=C(C=C1)O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)NC(=O)C1=CC=C(C=C1)CNNC.Cl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC1=CC(=CC(=C1C(=O)O)O)O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1=CC=C(C=C1)CN=C=S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  activity\n",
       "0  C1=CC(=C(C=C1C(CN)O)O)O.[C@@H]([C@H](C(=O)O)O)...         0\n",
       "1                              CC(=O)OC1=CC=C(C=C1)O         0\n",
       "2                    CC(C)NC(=O)C1=CC=C(C=C1)CNNC.Cl         0\n",
       "3                           CC1=CC(=CC(=C1C(=O)O)O)O         0\n",
       "4                                C1=CC=C(C=C1)CN=C=S         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_split(csv_path, smiles_col, target_cols):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    assert smiles_col in df.columns, f\"missing {smiles_col}\"\n",
    "    for c in target_cols:\n",
    "        assert c in df.columns, f\"missing target col {c}\"\n",
    "    return df[[smiles_col] + list(target_cols)].copy()\n",
    "\n",
    "train_df = load_split(Path(cfg.data_dir)/\"train.csv\", cfg.smiles_col, cfg.target_cols)\n",
    "val_df = load_split(Path(cfg.data_dir)/\"val.csv\", cfg.smiles_col, cfg.target_cols)\n",
    "test_df = load_split(Path(cfg.data_dir)/\"test.csv\", cfg.smiles_col, cfg.target_cols)\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836d68a",
   "metadata": {
    "id": "e836d68a"
   },
   "source": [
    "### 3.1 SMILES Tokenization (Character-level)\n",
    "\n",
    "Build a vocabulary over the training set and write functions to encode (batch of) smiles to a vector (or a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5d3955",
   "metadata": {
    "id": "1c5d3955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 49\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training SMILES\n",
    "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
    "\n",
    "def build_vocab(smiles_list, extra=\"\"):\n",
    "    charset = set()\n",
    "    for s in smiles_list:\n",
    "        for ch in s:\n",
    "            charset.add(ch)\n",
    "    for ch in extra:\n",
    "        charset.add(ch)\n",
    "    vocab = [PAD, BOS, EOS, UNK] + sorted(list(charset))\n",
    "    stoi = {c:i for i,c in enumerate(vocab)}\n",
    "    itos = {i:c for c,i in stoi.items()}\n",
    "    return vocab, stoi, itos\n",
    "\n",
    "vocab, stoi, itos = build_vocab(train_df[cfg.smiles_col].tolist(), cfg.vocab_extra)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def encode_smiles(s, max_len, stoi):\n",
    "    seq = [stoi.get(ch, stoi[UNK]) for ch in s[:max_len-2]]\n",
    "    seq = [stoi[BOS]] + seq + [stoi[EOS]]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [stoi[PAD]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return np.array(seq, dtype=np.int64)\n",
    "\n",
    "def batch_collate_sequence(batch):\n",
    "    xs = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    ys = torch.tensor([b[1] for b in batch], dtype=torch.float32)\n",
    "    return xs.to(device), ys.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a8f02",
   "metadata": {},
   "source": [
    "### 3.2 Morgan Fingerprint featurization (ECFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5b44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem\n",
    "    has_rdkit = True\n",
    "except Exception as e:\n",
    "    print(\"RDKit not available:\", e)\n",
    "    has_rdkit = False\n",
    "\n",
    "def morgan_fp_from_smiles(smiles: str, n_bits: int, radius: int):\n",
    "    if not has_rdkit:\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    bv = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=np.float32)\n",
    "    from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "    ConvertToNumpyArray(bv, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687f42c",
   "metadata": {
    "id": "5687f42c"
   },
   "source": [
    "### 3.3 Graph Construction (RDKit → PyG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c80a9c8b",
   "metadata": {
    "id": "c80a9c8b"
   },
   "outputs": [],
   "source": [
    "# Minimal RDKit -> PyG conversion\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem\n",
    "    has_rdkit = True\n",
    "except Exception as e:\n",
    "    print(\"RDKit not available:\", e)\n",
    "    has_rdkit = False\n",
    "\n",
    "try:\n",
    "    import torch_geometric as tg\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "    from torch_geometric.nn import GCNConv, GINConv, SAGEConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "    has_pyg = True\n",
    "except Exception as e:\n",
    "    print(\"PyG not available:\", e)\n",
    "    has_pyg = False\n",
    "\n",
    "def atom_features(atom):\n",
    "    atom_types = ['C','N','O','F','P','S','Cl','Br','I']\n",
    "    elem_onehot = [int(atom.GetSymbol() == e) for e in atom_types]\n",
    "\n",
    "    degree = [int(atom.GetDegree() == i) for i in range(5)]\n",
    "\n",
    "    formal_charge = [atom.GetFormalCharge()]\n",
    "\n",
    "    hybs = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "    ]\n",
    "    hybridization = [int(atom.GetHybridization() == h) for h in hybs]\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    in_ring = [int(atom.IsInRing())]\n",
    "    implicit_h = [atom.GetTotalNumHs(includeNeighbors=True)]\n",
    "    chiral = [int(atom.HasProp(\"_ChiralityPossible\"))]\n",
    "\n",
    "    feats = (\n",
    "        elem_onehot\n",
    "        + degree\n",
    "        + formal_charge\n",
    "        + hybridization\n",
    "        + aromatic\n",
    "        + in_ring\n",
    "        + implicit_h\n",
    "        + chiral\n",
    "    )\n",
    "    return feats\n",
    "\n",
    "def bond_features(bond):\n",
    "    btype = [\n",
    "        int(bond.GetBondType() == Chem.rdchem.BondType.SINGLE),\n",
    "        int(bond.GetBondType() == Chem.rdchem.BondType.DOUBLE),\n",
    "        int(bond.GetBondType() == Chem.rdchem.BondType.TRIPLE),\n",
    "        int(bond.GetBondType() == Chem.rdchem.BondType.AROMATIC),\n",
    "    ]\n",
    "    conjugated = [int(bond.GetIsConjugated())]\n",
    "    in_ring = [int(bond.IsInRing())]\n",
    "    stereo = [\n",
    "        int(bond.GetStereo() == Chem.rdchem.BondStereo.STEREOZ),\n",
    "        int(bond.GetStereo() == Chem.rdchem.BondStereo.STEREOE),\n",
    "    ]\n",
    "    feats = btype + conjugated + in_ring + stereo\n",
    "    assert len(feats) == 8, f\"Inconsistent bond feature length: {len(feats)}\"\n",
    "    return feats\n",
    "\n",
    "\n",
    "def smiles_to_pyg(smiles, need_3d=False, egnn_confs=1):\n",
    "    if not has_rdkit:\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    mol = Chem.AddHs(mol)\n",
    "    pos = None\n",
    "    if need_3d:\n",
    "        try:\n",
    "            params = AllChem.ETKDGv3()\n",
    "            params.randomSeed = 0xf00d\n",
    "            AllChem.EmbedMultipleConfs(mol, numConfs=max(1, egnn_confs), params=params)\n",
    "            AllChem.UFFOptimizeMoleculeConfs(mol, maxIters=200)\n",
    "            conf = mol.GetConformer(id=0)\n",
    "            pos = []\n",
    "        except Exception:\n",
    "            pass\n",
    "    atoms = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(atoms, dtype=torch.float32)\n",
    "    edge_index = [[], []]\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        edge_index[0] += [u, v]\n",
    "        edge_index[1] += [v, u]\n",
    "        edge_attr += [bf, bf]\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float32) if len(edge_attr)>0 else None\n",
    "    data_kwargs = {\"x\": x, \"edge_index\": edge_index}\n",
    "    if edge_attr is None or len(edge_attr) == 0:\n",
    "        edge_attr = torch.zeros((0, 8), dtype=torch.float32)\n",
    "    data_kwargs[\"edge_attr\"] = edge_attr\n",
    "\n",
    "    if need_3d and pos is None and mol.GetNumConformers()>0:\n",
    "        conf = mol.GetConformer(id=0)\n",
    "        pos = torch.tensor([[conf.GetAtomPosition(i).x,\n",
    "                             conf.GetAtomPosition(i).y,\n",
    "                             conf.GetAtomPosition(i).z] for i in range(mol.GetNumAtoms())],\n",
    "                           dtype=torch.float32)\n",
    "        data_kwargs[\"pos\"] = pos\n",
    "    elif need_3d and pos is None:\n",
    "        # fallback zeros\n",
    "        data_kwargs[\"pos\"] = torch.zeros((mol.GetNumAtoms(),3), dtype=torch.float32)\n",
    "    return Data(**data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7f218",
   "metadata": {
    "id": "b4c7f218"
   },
   "source": [
    "### 3.3 Dataset & Loaders\n",
    "\n",
    "Write a Pytorch Dataset and DataLoader class for SMILES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc49e82a",
   "metadata": {
    "id": "bc49e82a"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Fingerprint dataset\n",
    "class FingerprintDataset(Dataset):\n",
    "    def __init__(self, df, smiles_col, target_cols, n_bits, radius):\n",
    "        assert has_rdkit, \"RDKit required for Morgan fingerprints\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.smiles_col = smiles_col\n",
    "        self.target_cols = list(target_cols)\n",
    "        self.n_bits = n_bits; self.radius = radius\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        arr = morgan_fp_from_smiles(row[self.smiles_col], self.n_bits, self.radius)\n",
    "        if arr is None: raise ValueError(f\"Failed to featurize SMILES: {row[self.smiles_col]}\")\n",
    "        x = torch.tensor(arr, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target_cols].values.astype(np.float32))\n",
    "        return x, y\n",
    "\n",
    "def batch_collate_fp(batch):\n",
    "    xs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    ys = torch.stack([b[1] for b in batch], dim=0)\n",
    "    return xs.to(device), ys.to(device)\n",
    "\n",
    "def get_fp_loaders(train_df, val_df, test_df, batch_size, n_bits, radius):\n",
    "    train_ds = FingerprintDataset(train_df, cfg.smiles_col, cfg.target_cols, n_bits, radius)\n",
    "    val_ds   = FingerprintDataset(val_df,   cfg.smiles_col, cfg.target_cols, n_bits, radius)\n",
    "    test_ds  = FingerprintDataset(test_df,  cfg.smiles_col, cfg.target_cols, n_bits, radius)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cfg.num_workers, collate_fn=batch_collate_fp)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, collate_fn=batch_collate_fp)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, collate_fn=batch_collate_fp)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Sequence dataset\n",
    "class SmilesSequenceDataset(Dataset):\n",
    "    def __init__(self, df, smiles_col, target_cols, stoi, max_len):\n",
    "        self.df = df.reset_index(drop=True); self.smiles_col=smiles_col\n",
    "        self.target_cols = list(target_cols); self.stoi=stoi; self.max_len=max_len\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x = encode_smiles(row[self.smiles_col], self.max_len, self.stoi)\n",
    "        y = row[self.target_cols].values.astype(np.float32)\n",
    "        return x, y\n",
    "\n",
    "def get_sequence_loaders(train_df, val_df, test_df, batch_size):\n",
    "    train_ds = SmilesSequenceDataset(train_df, cfg.smiles_col, cfg.target_cols, stoi, cfg.max_len)\n",
    "    val_ds   = SmilesSequenceDataset(val_df,   cfg.smiles_col, cfg.target_cols, stoi, cfg.max_len)\n",
    "    test_ds  = SmilesSequenceDataset(test_df,  cfg.smiles_col, cfg.target_cols, stoi, cfg.max_len)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cfg.num_workers, collate_fn=batch_collate_sequence)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, collate_fn=batch_collate_sequence)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, collate_fn=batch_collate_sequence)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Graph dataset\n",
    "class SmilesGraphDataset(Dataset):\n",
    "    def __init__(self, df, smiles_col, target_cols, need_3d=False):\n",
    "        assert has_rdkit, \"RDKit required for graph datasets\"\n",
    "        self.df = df.reset_index(drop=True); self.smiles_col=smiles_col\n",
    "        self.target_cols = list(target_cols); self.need_3d=need_3d\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        data = smiles_to_pyg(row[self.smiles_col], need_3d=self.need_3d, egnn_confs=cfg.egnn_conformers)\n",
    "        if data is None: raise ValueError(\"Failed to parse SMILES:\", row[self.smiles_col])\n",
    "        y = torch.tensor(row[self.target_cols].values.astype(np.float32)); data.y = y\n",
    "        return data\n",
    "\n",
    "def get_graph_loaders(train_df, val_df, test_df, batch_size, need_3d=False):\n",
    "    assert has_pyg, \"PyTorch Geometric required\"\n",
    "    train_ds = SmilesGraphDataset(train_df, cfg.smiles_col, cfg.target_cols, need_3d=need_3d)\n",
    "    val_ds   = SmilesGraphDataset(val_df,   cfg.smiles_col, cfg.target_cols, need_3d=need_3d)\n",
    "    test_ds  = SmilesGraphDataset(test_df,  cfg.smiles_col, cfg.target_cols, need_3d=need_3d)\n",
    "    train_loader = GeoDataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader   = GeoDataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    test_loader  = GeoDataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82d91b",
   "metadata": {
    "id": "3a82d91b"
   },
   "source": [
    "## 4) Models\n",
    "\n",
    "**TODO:** Implement for the model families (DNN, CNN, RNN, Transformer, GNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d600f46-9196-45c3-9ff7-7741d19da3be",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f9ff72-81f3-46f8-a552-ea8fb180f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MLPFingerprint(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple configurable MLP that maps a fingerprint vector -> single logit.\n",
    "    Keep class definition in the models cell. Instantiate in an experiment cell.\n",
    "    Args:\n",
    "        in_dim: input dimensionality (fp length)\n",
    "        hidden_sizes: tuple of hidden layer sizes, e.g. (512,256)\n",
    "        dropout: dropout probability applied after activation\n",
    "        use_bn: whether to apply BatchNorm1d between Linear -> ReLU\n",
    "    Output:\n",
    "        forward(x) returns logits shaped (B,) (one logit per example)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_sizes=(512,256), dropout: float = 0.1, use_bn: bool = False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = int(in_dim)\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = int(h)\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, D) float tensor\n",
    "        returns: logits of shape (B,)\n",
    "        \"\"\"\n",
    "        logits = self.net(x)\n",
    "        return logits.view(-1)\n",
    "\n",
    "\n",
    "# Weight init helper\n",
    "def init_mlp_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.BatchNorm1d):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "# Convenience builder: creates model, applies init, moves to device\n",
    "def build_mlp_fingerprint(in_dim, hidden_sizes=(512,256), dropout=0.1, use_bn=False, device=torch.device(\"cpu\")):\n",
    "    model = MLPFingerprint(in_dim=in_dim, hidden_sizes=hidden_sizes, dropout=dropout, use_bn=use_bn)\n",
    "    model.apply(init_mlp_weights)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30320bf-fb6f-4378-a652-0a5df3a74408",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc1bbe1-629e-4e9d-8cc0-bc748fe22946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNN over SMILES ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNFingerprint(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN model for SMILES sequences.\n",
    "    Each SMILES string is tokenized as integer IDs and embedded, then\n",
    "    processed through convolutional layers to learn local substructure patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int = 128,\n",
    "        num_filters: int = 128,\n",
    "        kernel_sizes=(3, 5, 7),\n",
    "        dropout: float = 0.3,\n",
    "        use_bn: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer for SMILES tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers (with different kernel sizes)\n",
    "        convs = []\n",
    "        for k in kernel_sizes:\n",
    "            conv = nn.Conv1d(embed_dim, num_filters, kernel_size=k, padding=k//2)\n",
    "            convs.append(conv)\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "        self.use_bn = use_bn\n",
    "        if use_bn:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(num_filters) for _ in kernel_sizes])\n",
    "        else:\n",
    "            self.bns = [nn.Identity() for _ in kernel_sizes]\n",
    "\n",
    "        # Fully connected classifier head\n",
    "        in_dim = num_filters * len(kernel_sizes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L) integer token indices\n",
    "        \"\"\"\n",
    "        # Embedding -> (B, L, E)\n",
    "        x = self.embedding(x)\n",
    "        # Rearrange for Conv1d: (B, E, L)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Apply each convolution + activation + global max pool\n",
    "        conv_outputs = []\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h = conv(x)                # (B, num_filters, L)\n",
    "            h = bn(h)\n",
    "            h = torch.relu(h)\n",
    "            h = torch.max(h, dim=2).values  # Global max pool over sequence length\n",
    "            conv_outputs.append(h)\n",
    "\n",
    "        # Concatenate features from all kernel sizes\n",
    "        z = torch.cat(conv_outputs, dim=1)\n",
    "\n",
    "        # Classification head -> single logit\n",
    "        logits = self.classifier(z)\n",
    "        return logits.view(-1)\n",
    "\n",
    "\n",
    "def build_cnn_fingerprint(vocab_size, embed_dim=128, num_filters=128,\n",
    "                          kernel_sizes=(3,5,7), dropout=0.3, use_bn=False,\n",
    "                          device=torch.device(\"cpu\")):\n",
    "    model = CNNFingerprint(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_filters=num_filters,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        dropout=dropout,\n",
    "        use_bn=use_bn\n",
    "    )\n",
    "    # Kaiming initialization for convs and linear layers\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aea49b-2abe-49e6-9a99-35eb0f446cc4",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d4b5fa-7651-4b6d-94a3-de6a7f1b4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RNN (LSTM) over SMILES ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNSmiles(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent model (LSTM/GRU) over SMILES sequences.\n",
    "    Captures sequential dependencies in tokenized SMILES strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        rnn_type: str = \"lstm\",\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        if rnn_type.lower() == \"gru\":\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                batch_first=True, dropout=dropout, bidirectional=bidirectional\n",
    "            )\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                batch_first=True, dropout=dropout, bidirectional=bidirectional\n",
    "            )\n",
    "\n",
    "        # Bidirectional doubles hidden size\n",
    "        self.rnn_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.rnn_out_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L) token indices\n",
    "        returns: logits (B,)\n",
    "        \"\"\"\n",
    "        emb = self.embed(x)  # (B, L, E)\n",
    "        rnn_out, hidden = self.rnn(emb)\n",
    "\n",
    "        if isinstance(hidden, tuple): \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        last_hidden = hidden[-2:] if self.rnn.bidirectional else hidden[-1:]\n",
    "        last_hidden = torch.cat(list(last_hidden), dim=-1).squeeze(0)  # (B, hidden_dim * num_dirs)\n",
    "\n",
    "        logits = self.classifier(last_hidden)\n",
    "        return logits.view(-1)\n",
    "\n",
    "\n",
    "def build_rnn_smiles(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2,\n",
    "                     rnn_type=\"lstm\", bidirectional=True, dropout=0.3,\n",
    "                     device=torch.device(\"cpu\")):\n",
    "    model = RNNSmiles(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        rnn_type=rnn_type,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339a53e-439f-4e39-b85d-09367da3278c",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4f0336-0c40-4eb8-b4b7-6c741710a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional encoding from Vaswani et al. (2017).\n",
    "    Adds position-dependent sin/cos patterns to token embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute sinusoidal encoding matrix [max_len, embed_dim]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, embed_dim)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (B, L, D)\n",
    "        returns: same shape, with position encodings added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerSmiles(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 256, nhead: int = 8,\n",
    "                 num_layers: int = 4, ff_dim: int = 512, dropout: float = 0.1,\n",
    "                 max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation=\"relu\",\n",
    "            batch_first=True  # ensures (B, L, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, L) long tensor of token IDs\n",
    "        output: (B,) logits\n",
    "        \"\"\"\n",
    "        mask = (x == 0)  # padding mask\n",
    "        x = self.embed(x) * math.sqrt(x.size(-1))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.mean(dim=1)  # mean pooling over sequence\n",
    "        logits = self.classifier(x)\n",
    "        return logits.view(-1)\n",
    "\n",
    "def build_transformer_smiles(vocab_size, device, embed_dim=256, nhead=8,\n",
    "                             num_layers=4, ff_dim=512, dropout=0.1, max_len=256):\n",
    "    model = TransformerSmiles(vocab_size, embed_dim, nhead, num_layers, ff_dim, dropout, max_len)\n",
    "    model.apply(lambda m: nn.init.xavier_uniform_(m.weight) if isinstance(m, nn.Linear) else None)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456e772-6b98-4bfb-ae18-db004e643522",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9dc0300-9b8d-45fd-b0a7-2033a6a4a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GNN over molecular graphs (GCN / GIN / GraphSAGE) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GINConv, SAGEConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "\n",
    "POOLERS = {\n",
    "    \"mean\": global_mean_pool,\n",
    "    \"add\":  global_add_pool,\n",
    "    \"max\":  global_max_pool,\n",
    "}\n",
    "\n",
    "class GraphBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kind=\"gcn\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        if kind == \"gcn\":\n",
    "            self.conv = GCNConv(in_dim, out_dim)\n",
    "        elif kind == \"graphsage\":\n",
    "            self.conv = SAGEConv(in_dim, out_dim)\n",
    "        elif kind == \"gin\":\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(out_dim, out_dim),\n",
    "            )\n",
    "            self.conv = GINConv(mlp)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown GNN kind: {kind}\")\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable GNN with N layers of {GCN, GIN, GraphSAGE} and a pooled classifier head.\n",
    "    Expects PyG Batch with .x (node features), .edge_index, .batch, optional .edge_attr ignored.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden=256, layers=3, kind=\"gcn\", pool=\"mean\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        dims = [in_dim] + [hidden] * layers\n",
    "        for i in range(layers):\n",
    "            self.blocks.append(GraphBlock(dims[i], dims[i+1], kind=kind, dropout=dropout))\n",
    "        self.pool = POOLERS.get(pool, global_mean_pool)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data: PyG Batch (x, edge_index, batch, [edge_attr], [pos])\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, edge_index)\n",
    "        g = self.pool(x, batch)          # (B, hidden)\n",
    "        logits = self.head(g)            # (B, 1)\n",
    "        return logits.view(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126191b3",
   "metadata": {
    "id": "126191b3"
   },
   "source": [
    "## 5) Training & Evaluation Loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7039190a",
   "metadata": {
    "id": "7039190a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')   # silence RDKit warnings\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Helpers for safe metric computation\n",
    "def safe_roc_auc(y_true, y_score):\n",
    "    try:\n",
    "        return float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def safe_average_precision(y_true, y_score):\n",
    "    try:\n",
    "        return float(average_precision_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def compute_classification_metrics(y_true, y_score, threshold=0.5):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    metrics = {}\n",
    "\n",
    "    # ROC-AUC & AP (may be NaN if only one class present)\n",
    "    try:\n",
    "        metrics[\"ROC-AUC\"] = float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        metrics[\"ROC-AUC\"] = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        metrics[\"AP\"] = float(average_precision_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        metrics[\"AP\"] = float(\"nan\")\n",
    "\n",
    "    # Binary preds at threshold\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    # Accuracy always defined\n",
    "    try:\n",
    "        metrics[\"Accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
    "    except Exception:\n",
    "        metrics[\"Accuracy\"] = float(\"nan\")\n",
    "\n",
    "    # Precision / recall: if positive class absent or zero division, set to nan or 0 depending on choice\n",
    "    try:\n",
    "        metrics[\"Precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    except Exception:\n",
    "        metrics[\"Precision\"] = float(\"nan\")\n",
    "    try:\n",
    "        metrics[\"Recall\"] = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    except Exception:\n",
    "        metrics[\"Recall\"] = float(\"nan\")\n",
    "\n",
    "    # Confusion matrix (as list)\n",
    "    try:\n",
    "        metrics[\"ConfusionMatrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
    "    except Exception:\n",
    "        metrics[\"ConfusionMatrix\"] = []\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, mode=\"max\", delta=0.0, verbose=False):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        self.patience = int(patience)\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "        self.is_better = (lambda a, b: a < b - delta) if mode==\"min\" else (lambda a, b: a > b + delta)\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best is None:\n",
    "            self.best = metric\n",
    "            self.num_bad = 0\n",
    "            return False\n",
    "        if self.is_better(metric, self.best):\n",
    "            self.best = metric\n",
    "            self.num_bad = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: metric did not improve ({self.num_bad}/{self.patience})\")\n",
    "            return self.num_bad >= self.patience\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, out_dir, monitor=\"val_metric\", mode=\"max\", save_best_only=True):\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best = None\n",
    "        self.is_better = (lambda a, b: a < b) if mode==\"min\" else (lambda a, b: a > b)\n",
    "\n",
    "    def save(self, model, optimizer, scheduler, epoch, metric, tag=\"best\"):\n",
    "        \"\"\"\n",
    "        Saves checkpoint. If save_best_only=True, only saves when metric improves wrt self.best.\n",
    "        Returns path to saved file or None if nothing was saved.\n",
    "        \"\"\"\n",
    "        # check metric improvement\n",
    "        if self.save_best_only:\n",
    "            if self.best is None or self.is_better(metric, self.best):\n",
    "                self.best = metric\n",
    "            else:\n",
    "                return None\n",
    "    \n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": optimizer.state_dict() if optimizer is not None else None,\n",
    "            \"sch_state\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"metric\": metric,\n",
    "        }\n",
    "        fn = self.out_dir / f\"checkpoint_{tag}.pt\"\n",
    "        torch.save(state, fn)\n",
    "        return fn\n",
    "\n",
    "def get_pos_weight_from_df(train_df, target_col):\n",
    "    # returns torch.tensor suitable for BCEWithLogitsLoss pos_weight\n",
    "    counts = train_df[target_col].value_counts().to_dict()\n",
    "    n_pos = counts.get(1, 0)\n",
    "    n_neg = counts.get(0, 0)\n",
    "    if n_pos == 0:\n",
    "        print(\"Warning: no positive examples in training set; pos_weight not defined. Returning 1.0\")\n",
    "        return torch.tensor(1.0, dtype=torch.float32)\n",
    "    pos_weight = max(1.0, float(n_neg) / float(n_pos))\n",
    "    return torch.tensor(pos_weight, dtype=torch.float32)\n",
    "\n",
    "def get_loss_fn(task_type=\"classification\", pos_weight=None, device=torch.device(\"cpu\")):\n",
    "    if task_type == \"classification\":\n",
    "        if pos_weight is not None:\n",
    "            pw = pos_weight.to(device) if isinstance(pos_weight, torch.Tensor) else torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
    "            return nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "        else:\n",
    "            return nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        return nn.MSELoss()\n",
    "\n",
    "def unpack_batch(batch):\n",
    "    \"\"\"\n",
    "    Returns (inputs, targets) tuples for common batch types:\n",
    "    - If batch is (x,y) tuple/list => returns as-is (tensors)\n",
    "    - If batch has attribute 'y' (torch_geometric Batch), returns (batch, batch.y)\n",
    "    - Else: raises\n",
    "    \"\"\"\n",
    "    # PyG Batch detection: has attribute 'y' and 'to'\n",
    "    if hasattr(batch, \"y\") and hasattr(batch, \"to\"):\n",
    "        return batch, batch.y\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "        return batch[0], batch[1]\n",
    "    raise ValueError(\"Unsupported batch type for unpacking. Expected (x,y) or PyG Batch with .y\")\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device, scaler=None, grad_clip=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_scores = []\n",
    "\n",
    "    for batch in loader:\n",
    "        inputs, targets = unpack_batch(batch)\n",
    "        if isinstance(inputs, torch.Tensor):\n",
    "            inputs = inputs.to(device)\n",
    "        if isinstance(targets, torch.Tensor):\n",
    "            targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs)\n",
    "                logits = logits.view(-1)\n",
    "                targets_float = targets.view(-1).float()\n",
    "                loss = loss_fn(logits, targets_float)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1)\n",
    "            targets_float = targets.view(-1).float()\n",
    "            loss = loss_fn(logits, targets_float)\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * targets_float.shape[0]\n",
    "        probs = torch.sigmoid(logits.detach()).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(targets_float.detach().cpu().numpy())\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0) if len(all_scores)>0 else np.array([])\n",
    "    all_targets = np.concatenate(all_targets, axis=0) if len(all_targets)>0 else np.array([])\n",
    "    avg_loss = running_loss / max(1, len(all_targets))\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def eval_model(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, targets = unpack_batch(batch)\n",
    "            if isinstance(inputs, torch.Tensor):\n",
    "                inputs = inputs.to(device)\n",
    "            if isinstance(targets, torch.Tensor):\n",
    "                targets = targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1)\n",
    "            targets_float = targets.view(-1).float()\n",
    "            loss = loss_fn(logits, targets_float)\n",
    "            running_loss += loss.item() * targets_float.shape[0]\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(targets_float.detach().cpu().numpy())\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0) if len(all_scores)>0 else np.array([])\n",
    "    all_targets = np.concatenate(all_targets, axis=0) if len(all_targets)>0 else np.array([])\n",
    "    avg_loss = running_loss / max(1, len(all_targets))\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def run_training(model, train_loader, val_loader, optimizer, scheduler, loss_fn, cfg, model_name=\"model\", device=torch.device(\"cpu\")):\n",
    "    out_dir = Path(cfg.out_dir) / model_name\n",
    "    ckpt_dir = out_dir / \"checkpoints\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Sch may be None\n",
    "    checkpoint = ModelCheckpoint(ckpt_dir, monitor=\"val_ROC-AUC\", mode=\"max\")\n",
    "    earlystop = EarlyStopping(patience=cfg.patience, mode=\"max\", verbose=True)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if (device.type==\"cuda\") else None\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    best_metric = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, cfg.max_epochs+1):\n",
    "        t0 = time.time()\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, loss_fn, device, scaler=scaler, grad_clip=cfg.grad_clip)\n",
    "        val_loss, val_metrics = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_metrics.get(\"ROC-AUC\", val_loss))\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        # Logging\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_ROC-AUC\"].append(train_metrics.get(\"ROC-AUC\"))\n",
    "        history[\"val_ROC-AUC\"].append(val_metrics.get(\"ROC-AUC\"))\n",
    "        history[\"train_AP\"].append(train_metrics.get(\"AP\"))\n",
    "        history[\"val_AP\"].append(val_metrics.get(\"AP\"))\n",
    "\n",
    "        # print\n",
    "        print(f\"Epoch {epoch}/{cfg.max_epochs} — train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_ROC-AUC={val_metrics.get('ROC-AUC'):.4f} val_AP={val_metrics.get('AP'):.4f} time={epoch_time:.1f}s\")\n",
    "\n",
    "        val_metric = val_metrics.get(\"ROC-AUC\", float(\"nan\"))\n",
    "\n",
    "        if not math.isnan(val_metric):\n",
    "            if best_metric is None or val_metric > best_metric:\n",
    "                best_metric = val_metric\n",
    "                best_epoch = epoch\n",
    "                checkpoint.save(model, optimizer, scheduler, epoch, best_metric, tag=\"best\")\n",
    "                print(f\"  -> new best val_ROC-AUC={best_metric:.4f} saved checkpoint\")\n",
    "        else:\n",
    "            print(\"  -> val_ROC-AUC is NaN for this epoch (likely single-class in val). Skipping checkpointing.\")\n",
    "\n",
    "        if math.isnan(val_metric):\n",
    "            stopped = earlystop.step(float(\"-inf\") if earlystop.mode==\"max\" else float(\"inf\"))\n",
    "        else:\n",
    "            stopped = earlystop.step(val_metric)\n",
    "        if stopped:\n",
    "            print(f\"Early stopping at epoch {epoch} (best epoch {best_epoch} val_ROC-AUC={best_metric})\")\n",
    "            break\n",
    "\n",
    "    # Save final history\n",
    "    (out_dir / \"training_history.json\").write_text(json.dumps(history, indent=2))\n",
    "    print(\"Training finished. Best epoch:\", best_epoch, \"best_metric:\", best_metric)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb743ae",
   "metadata": {
    "id": "edb743ae"
   },
   "source": [
    "## 6) Collect Results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384bc19",
   "metadata": {},
   "source": [
    "### 6.0 MLP over Morgan Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e241ded5-de83-42ab-8065-9133b8716e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created stratified internal split: 1207 214\n",
      "train_internal counts: {0: 1165, 1: 42}\n",
      "val_internal counts: {0: 207, 1: 7}\n",
      "provided_val counts: {0: 462, 1: 12}\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_internal_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_internal_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_fp.npz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# DataLoader: use sampler only if not None (sampler implies shuffle must be False)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_ds\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, sampler\u001b[38;5;241m=\u001b[39mtrain_sampler, num_workers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnum_workers)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnum_workers)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np, os, json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_base = Path(cfg.out_dir) / \"fp_baseline\"\n",
    "out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Stratified internal split\n",
    "target_col = cfg.target_cols[0]\n",
    "try:\n",
    "    train_internal_df, val_internal_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=0.15,\n",
    "        random_state=cfg.seed,\n",
    "        stratify=train_df[target_col]\n",
    "    )\n",
    "    print(\"Created stratified internal split:\", len(train_internal_df), len(val_internal_df))\n",
    "except Exception as e:\n",
    "    print(\"Stratified split failed:\", e)\n",
    "    # fallback: use original provided val_df for validation\n",
    "    train_internal_df = train_df.copy()\n",
    "    val_internal_df = val_df.copy()\n",
    "    print(\"Fallback: using provided val_df as validation set\")\n",
    "\n",
    "# Show class balance\n",
    "def print_class_counts(name, df):\n",
    "    counts = df[target_col].value_counts().to_dict()\n",
    "    print(f\"{name} counts:\", counts)\n",
    "\n",
    "print_class_counts(\"train_internal\", train_internal_df)\n",
    "print_class_counts(\"val_internal\", val_internal_df)\n",
    "print_class_counts(\"provided_val\", val_df)\n",
    "\n",
    "# 2) Precompute Morgan fingerprints for given df and cache to disk for speed\n",
    "def precompute_fps(df, fname, n_bits=cfg.fp_nbits, radius=cfg.fp_radius, smiles_col=cfg.smiles_col, target_col=target_col):\n",
    "    outp = out_base / fname\n",
    "    if outp.exists():\n",
    "        print(\"Loading cached fingerprints:\", outp)\n",
    "        data = np.load(outp)\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        return X, y\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    bad_indices = []\n",
    "    for i, row in df.reset_index(drop=True).iterrows():\n",
    "        s = str(row[smiles_col])\n",
    "        arr = morgan_fp_from_smiles(s, n_bits=n_bits, radius=radius)\n",
    "        if arr is None:\n",
    "            bad_indices.append((i, s))\n",
    "            continue\n",
    "        X_list.append(arr)\n",
    "        y_list.append(float(row[target_col]))\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError(\"No valid fingerprints computed for \" + str(fname))\n",
    "    X = np.stack(X_list, axis=0).astype(np.float32)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    np.savez_compressed(outp, X=X, y=y)\n",
    "    if len(bad_indices) > 0:\n",
    "        with open(out_base / f\"{fname}_bad_smiles.txt\", \"w\") as fh:\n",
    "            for i,s in bad_indices:\n",
    "                fh.write(f\"{i}\\t{s}\\n\")\n",
    "    print(f\"Saved fingerprints to {outp} (N={len(y)})\")\n",
    "    return X, y\n",
    "\n",
    "# Precompute for train_internal, val_internal, test_df\n",
    "X_tr, y_tr = precompute_fps(train_internal_df, \"train_internal_fp.npz\")\n",
    "X_val, y_val = precompute_fps(val_internal_df,   \"val_internal_fp.npz\")\n",
    "X_test, y_test = precompute_fps(test_df,          \"test_fp.npz\")\n",
    "\n",
    "# 3) WeightedRandomSampler for training \n",
    "def make_weighted_sampler_from_labels(y_array):\n",
    "    vals, counts = np.unique(y_array, return_counts=True)\n",
    "    class_counts = dict(zip(vals.tolist(), counts.tolist()))\n",
    "    if len(class_counts) <= 1:\n",
    "        return None\n",
    "    sample_weights = np.array([1.0 / class_counts[int(lbl)] for lbl in y_array], dtype=np.double)\n",
    "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "train_sampler = make_weighted_sampler_from_labels(y_tr)\n",
    "\n",
    "# DataLoader: use sampler only if not None\n",
    "if train_sampler is not None:\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=train_sampler, num_workers=cfg.num_workers)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "\n",
    "\n",
    "# 4) TensorDatasets + DataLoaders\n",
    "train_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=train_sampler, num_workers=cfg.num_workers)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size*2, shuffle=False, num_workers=cfg.num_workers)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size*2, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "print(\"DL sizes:\", len(train_loader), len(val_loader), len(test_loader))\n",
    "print(\"Train class counts (actual):\", np.unique(y_tr, return_counts=True))\n",
    "print(\"Val class counts (actual):\", np.unique(y_val, return_counts=True))\n",
    "\n",
    "\n",
    "# 6) Loss (use pos_weight computed from train_internal_df) and optimizer\n",
    "pos_weight = get_pos_weight_from_df(train_internal_df, target_col).to(device)\n",
    "print(\"Using pos_weight:\", float(pos_weight))\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "# 7) Train using run_training (uses cfg)\n",
    "cfg_local = copy.deepcopy(cfg)\n",
    "cfg_local.max_epochs = 50\n",
    "cfg_local.patience = 8\n",
    "history = run_training(model, train_loader, val_loader, optimizer, scheduler, loss_fn, cfg_local, model_name=\"fp_mlp\", device=device)\n",
    "\n",
    "# 8) After training, load best checkpoint and evaluate on test set\n",
    "ckpt_path = Path(cfg.out_dir) / \"fp_mlp\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(\"Loaded best checkpoint from epoch\", state.get(\"epoch\"), \"metric\", state.get(\"metric\"))\n",
    "else:\n",
    "    print(\"No best checkpoint found at\", ckpt_path, \"; evaluating last model state.\")\n",
    "\n",
    "# Evaluate on test_loader\n",
    "model.eval()\n",
    "all_scores = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(yb.detach().cpu().numpy())\n",
    "all_scores = np.concatenate(all_scores, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"=== TEST METRICS (fingerprint MLP) ===\")\n",
    "for k,v in metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# 9) Save metrics and plot learning curves\n",
    "with open(out_base / \"fp_mlp_test_metrics.json\", \"w\") as fh:\n",
    "    json.dump(metrics, fh, indent=2)\n",
    "# Plot learning curves (train/val loss + ROC-AUC)\n",
    "hist_path = Path(cfg.out_dir) / \"fp_mlp\" / \"training_history.json\"\n",
    "if hist_path.exists():\n",
    "    hist = json.load(open(hist_path))\n",
    "    epochs = hist[\"epoch\"]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_base / \"loss_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, hist[\"train_ROC-AUC\"], label=\"train_ROC-AUC\")\n",
    "    plt.plot(epochs, hist[\"val_ROC-AUC\"], label=\"val_ROC-AUC\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"ROC-AUC\"); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_base / \"roc_curve_epochs.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved learning curves to\", out_base)\n",
    "else:\n",
    "    print(\"No history file found at\", hist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8c29f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_full size: 1895  test size: 474\n",
      "training for fit: 1743 internal_val: 152\n",
      "Saved fingerprints to runs_hw1/fp_baseline/train_full_fp.npz (N=1743)\n",
      "Saved fingerprints to runs_hw1/fp_baseline/internal_val_fp.npz (N=152)\n",
      "Train/Val/Test loader lens: 218 5 15\n",
      "Train class counts: (array([0., 1.], dtype=float32), array([1687,   56]))\n",
      "pos_weight raw=30.12 capped=10.00\n",
      "Epoch 1/100 — train_loss=0.5347 val_loss=0.6003 val_ROC-AUC=0.8680 val_AP=0.7699 time=1.2s\n",
      "  -> new best val_ROC-AUC=0.8680 saved checkpoint\n",
      "Epoch 2/100 — train_loss=0.0113 val_loss=0.8523 val_ROC-AUC=0.8571 val_AP=0.7692 time=1.0s\n",
      "EarlyStopping: metric did not improve (1/10)\n",
      "Epoch 3/100 — train_loss=0.0053 val_loss=0.9074 val_ROC-AUC=0.8694 val_AP=0.7700 time=1.0s\n",
      "  -> new best val_ROC-AUC=0.8694 saved checkpoint\n",
      "Epoch 4/100 — train_loss=0.0060 val_loss=1.0332 val_ROC-AUC=0.8680 val_AP=0.8098 time=1.0s\n",
      "EarlyStopping: metric did not improve (1/10)\n",
      "Epoch 5/100 — train_loss=0.0086 val_loss=1.0612 val_ROC-AUC=0.8599 val_AP=0.7693 time=1.0s\n",
      "EarlyStopping: metric did not improve (2/10)\n",
      "Epoch 6/100 — train_loss=0.0045 val_loss=1.0837 val_ROC-AUC=0.8612 val_AP=0.7694 time=1.1s\n",
      "EarlyStopping: metric did not improve (3/10)\n",
      "Epoch 7/100 — train_loss=0.0036 val_loss=1.0788 val_ROC-AUC=0.8639 val_AP=0.8095 time=1.1s\n",
      "EarlyStopping: metric did not improve (4/10)\n",
      "Epoch 8/100 — train_loss=0.0021 val_loss=1.1486 val_ROC-AUC=0.8667 val_AP=0.7698 time=1.0s\n",
      "EarlyStopping: metric did not improve (5/10)\n",
      "Epoch 9/100 — train_loss=0.0096 val_loss=1.1455 val_ROC-AUC=0.8653 val_AP=0.7697 time=1.0s\n",
      "EarlyStopping: metric did not improve (6/10)\n",
      "Epoch 10/100 — train_loss=0.0001 val_loss=1.1322 val_ROC-AUC=0.8653 val_AP=0.7697 time=1.1s\n",
      "EarlyStopping: metric did not improve (7/10)\n",
      "Epoch 11/100 — train_loss=0.0052 val_loss=1.1310 val_ROC-AUC=0.8680 val_AP=0.7699 time=1.1s\n",
      "EarlyStopping: metric did not improve (8/10)\n",
      "Epoch 12/100 — train_loss=0.0001 val_loss=1.1530 val_ROC-AUC=0.8653 val_AP=0.7697 time=1.1s\n",
      "EarlyStopping: metric did not improve (9/10)\n",
      "Epoch 13/100 — train_loss=0.0038 val_loss=1.1597 val_ROC-AUC=0.8680 val_AP=0.7699 time=1.1s\n",
      "EarlyStopping: metric did not improve (10/10)\n",
      "Early stopping at epoch 13 (best epoch 3 val_ROC-AUC=0.8693877551020409)\n",
      "Training finished. Best epoch: 3 best_metric: 0.8693877551020409\n",
      "Loaded checkpoint epoch 3 metric 0.8693877551020409\n",
      "=== FINAL TEST METRICS ===\n",
      "{\n",
      "  \"ROC-AUC\": 0.6225858369098712,\n",
      "  \"AP\": 0.27136277787620605,\n",
      "  \"Accuracy\": 0.9746835443037974,\n",
      "  \"Precision\": 0.25,\n",
      "  \"Recall\": 0.25,\n",
      "  \"ConfusionMatrix\": [\n",
      "    [\n",
      "      460,\n",
      "      6\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      2\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Saved final artifacts to runs_hw1/final_mlp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out_final = Path(cfg.out_dir) / \"final_mlp\"\n",
    "out_final.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Prepare train_full\n",
    "train_full = pd.concat([train_df, val_df], axis=0).reset_index(drop=True)\n",
    "print(\"train_full size:\", len(train_full), \" test size:\", len(test_df))\n",
    "\n",
    "# small internal val percent\n",
    "internal_val_frac = 0.08\n",
    "if internal_val_frac > 0:\n",
    "    train_for_fit_df, internal_val_df = train_test_split(\n",
    "        train_full,\n",
    "        test_size=internal_val_frac,\n",
    "        random_state=cfg.seed,\n",
    "        stratify=train_full[cfg.target_cols[0]]\n",
    "    )\n",
    "else:\n",
    "    train_for_fit_df = train_full.copy()\n",
    "    internal_val_df = val_df.copy() \n",
    "\n",
    "print(\"training for fit:\", len(train_for_fit_df), \"internal_val:\", len(internal_val_df))\n",
    "\n",
    "# 2) Precompute (or load cached) fingerprints for both sets\n",
    "train_cache = out_final / \"train_full_fp.npz\"\n",
    "val_cache   = out_final / \"internal_val_fp.npz\"\n",
    "test_cache  = Path(cfg.out_dir) / \"fp_baseline\" / \"test_fp.npz\" \n",
    "\n",
    "if 'precompute_fps' in globals():\n",
    "    X_tr, y_tr = precompute_fps(train_for_fit_df, str(train_cache.name))\n",
    "    X_val, y_val = precompute_fps(internal_val_df, str(val_cache.name))\n",
    "    # prefer existing test cache\n",
    "    if test_cache.exists():\n",
    "        data = np.load(test_cache)\n",
    "        X_test, y_test = data[\"X\"].astype(np.float32), data[\"y\"].astype(np.float32)\n",
    "    else:\n",
    "        X_test, y_test = precompute_fps(test_df, \"test_fp.npz\")\n",
    "else:\n",
    "    raise RuntimeError(\"precompute_fps not found. Make sure your featurizer cell is run.\")\n",
    "\n",
    "# 3) Build DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def make_loader_from_numpy(X, y, batch_size, sampler=None, shuffle=False):\n",
    "    ds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n",
    "    if sampler is not None:\n",
    "        return DataLoader(ds, batch_size=batch_size, sampler=sampler, num_workers=cfg.num_workers)\n",
    "    else:\n",
    "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=cfg.num_workers)\n",
    "\n",
    "# add sampler guard\n",
    "train_sampler = None\n",
    "if 'make_weighted_sampler_from_labels' in globals():\n",
    "    train_sampler = make_weighted_sampler_from_labels(y_tr)\n",
    "train_loader = make_loader_from_numpy(X_tr, y_tr, batch_size=cfg.batch_size, sampler=train_sampler, shuffle=(train_sampler is None))\n",
    "val_loader   = make_loader_from_numpy(X_val, y_val, batch_size=max(32, cfg.batch_size*2), shuffle=False)\n",
    "test_loader  = make_loader_from_numpy(X_test, y_test, batch_size=max(32, cfg.batch_size*2), shuffle=False)\n",
    "\n",
    "print(\"Train/Val/Test loader lens:\", len(train_loader), len(val_loader), len(test_loader))\n",
    "print(\"Train class counts:\", np.unique(y_tr, return_counts=True))\n",
    "\n",
    "# 4) Instantiate final model\n",
    "hidden_sizes = (512, 256) \n",
    "dropout = cfg.dropout\n",
    "model = build_mlp_fingerprint(in_dim=X_tr.shape[1], hidden_sizes=hidden_sizes, dropout=dropout, use_bn=False, device=device)\n",
    "\n",
    "# 5) Loss / optimizer / scheduler\n",
    "raw_pw = float(get_pos_weight_from_df(pd.DataFrame({cfg.target_cols[0]: y_tr}), cfg.target_cols[0])) if False else float((y_tr==0).sum() / max(1.0, (y_tr==1).sum()))\n",
    "# safe compute directly:\n",
    "n_pos = float((y_tr==1).sum())\n",
    "n_neg = float((y_tr==0).sum())\n",
    "raw_pw = (n_neg / max(1.0, n_pos)) if n_pos>0 else 1.0\n",
    "pw_cap = 10.0\n",
    "pos_weight = torch.tensor(min(raw_pw, pw_cap), dtype=torch.float32, device=device)\n",
    "print(f\"pos_weight raw={raw_pw:.2f} capped={float(pos_weight):.2f}\")\n",
    "\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "# 6) Training: run_training (uses EarlyStopping / checkpoint)\n",
    "cfg_local = copy.deepcopy(cfg)\n",
    "cfg_local.max_epochs = 100\n",
    "cfg_local.patience = 10\n",
    "history = run_training(model, train_loader, val_loader, optimizer, scheduler, loss_fn, cfg_local, model_name=\"final_mlp\", device=device)\n",
    "\n",
    "# 7) Load best checkpoint and evaluate on test set (safe load)\n",
    "ckpt_path = Path(cfg.out_dir) / \"final_mlp\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    try:\n",
    "        ck = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ck[\"model_state\"])\n",
    "        print(\"Loaded checkpoint epoch\", ck.get(\"epoch\"), \"metric\", ck.get(\"metric\"))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load checkpoint:\", e)\n",
    "else:\n",
    "    print(\"No checkpoint found at\", ckpt_path, \" — using last model state.\")\n",
    "\n",
    "# evaluate on test\n",
    "model.eval()\n",
    "all_scores = []; all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(yb.numpy())\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"=== FINAL TEST METRICS ===\")\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# save final artifacts\n",
    "np.savez_compressed(out_final/\"final_test_probs.npz\", probs=all_scores, y=all_targets)\n",
    "(out_final/\"final_test_metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "print(\"Saved final artifacts to\", out_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f4e1d",
   "metadata": {
    "id": "4a4f4e1d"
   },
   "source": [
    "### 6.1 CNN over SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44891d7e",
   "metadata": {
    "id": "44891d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNFingerprint(\n",
      "  (embedding): Embedding(49, 128, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/7_s4kk210jbd3z8_j6y__04r0000gn/T/ipykernel_15118/3915914961.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  xs = torch.tensor([b[0] for b in batch], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 — train_loss=7.2241 val_loss=1.7274 val_ROC-AUC=0.9118 val_AP=0.7927 time=3.6s\n",
      "  -> new best val_ROC-AUC=0.9118 saved checkpoint\n",
      "Epoch 2/50 — train_loss=3.7612 val_loss=2.0447 val_ROC-AUC=0.8754 val_AP=0.7506 time=3.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/50 — train_loss=2.5746 val_loss=3.8311 val_ROC-AUC=0.8808 val_AP=0.7706 time=2.8s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/50 — train_loss=1.7028 val_loss=1.9174 val_ROC-AUC=0.8941 val_AP=0.7599 time=3.0s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/50 — train_loss=1.0878 val_loss=2.5945 val_ROC-AUC=0.8981 val_AP=0.7593 time=3.0s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/50 — train_loss=0.4508 val_loss=1.4469 val_ROC-AUC=0.9004 val_AP=0.7628 time=3.9s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/50 — train_loss=0.3555 val_loss=2.0389 val_ROC-AUC=0.8755 val_AP=0.7537 time=2.9s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/50 — train_loss=0.1180 val_loss=1.8857 val_ROC-AUC=0.8548 val_AP=0.7450 time=2.8s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/50 — train_loss=0.1595 val_loss=1.9234 val_ROC-AUC=0.8831 val_AP=0.7488 time=3.0s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9117965367965368)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9117965367965368\n",
      "Loaded best checkpoint from epoch 1 (val ROC-AUC=0.9118)\n",
      "\n",
      "=== TEST METRICS (CNN over SMILES) ===\n",
      "ROC-AUC: 0.6564\n",
      "AP: 0.2332\n",
      "Accuracy: 0.9852\n",
      "Precision: 1.0000\n",
      "Recall: 0.1250\n",
      "ConfusionMatrix: [[466, 0], [7, 1]]\n",
      "Saved plots to runs_hw1/cnn_smiles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_sequence_loaders(\n",
    "    train_df, val_df, test_df,\n",
    "    batch_size=cfg.batch_size\n",
    ")\n",
    "\n",
    "model = build_cnn_fingerprint(\n",
    "    vocab_size=len(stoi),\n",
    "    embed_dim=128,\n",
    "    num_filters=128,\n",
    "    kernel_sizes=(3, 5, 7),\n",
    "    dropout=cfg.dropout,\n",
    "    device=device\n",
    ")\n",
    "print(model)\n",
    "\n",
    "pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "cfg_local = copy.deepcopy(cfg)\n",
    "cfg_local.max_epochs = 50\n",
    "cfg_local.patience = 8\n",
    "\n",
    "history = run_training(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, scheduler, loss_fn,\n",
    "    cfg_local, model_name=\"cnn_smiles\", device=device\n",
    ")\n",
    "\n",
    "ckpt_path = Path(cfg.out_dir) / \"cnn_smiles\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(f\"Loaded best checkpoint from epoch {state.get('epoch')} (val ROC-AUC={state.get('metric'):.4f})\")\n",
    "else:\n",
    "    print(\"No best checkpoint found; using final model weights.\")\n",
    "\n",
    "model.eval()\n",
    "all_scores, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"\\n=== TEST METRICS (CNN over SMILES) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "out_dir = Path(cfg.out_dir) / \"cnn_smiles\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "json.dump(metrics, open(out_dir / \"test_metrics.json\", \"w\"), indent=2)\n",
    "\n",
    "hist_path = out_dir / \"training_history.json\"\n",
    "if hist_path.exists():\n",
    "    hist = json.load(open(hist_path))\n",
    "    epochs = hist[\"epoch\"]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"loss_curve.png\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, hist[\"train_ROC-AUC\"], label=\"train_ROC-AUC\")\n",
    "    plt.plot(epochs, hist[\"val_ROC-AUC\"], label=\"val_ROC-AUC\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"ROC-AUC\"); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"roc_curve.png\"); plt.close()\n",
    "    print(\"Saved plots to\", out_dir)\n",
    "else:\n",
    "    print(\"No history file found; skipping plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfbb3c",
   "metadata": {
    "id": "aecfbb3c"
   },
   "source": [
    "### 6.2 RNN (GRU/LSTM) over SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58fe18c",
   "metadata": {
    "id": "e58fe18c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNSmiles(\n",
      "  (embed): Embedding(49, 128, padding_idx=0)\n",
      "  (rnn): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/50 — train_loss=3.0529 val_loss=1.8281 val_ROC-AUC=0.5685 val_AP=0.0301 time=54.5s\n",
      "  -> new best val_ROC-AUC=0.5685 saved checkpoint\n",
      "Epoch 2/50 — train_loss=2.7703 val_loss=1.9847 val_ROC-AUC=0.5833 val_AP=0.0308 time=57.8s\n",
      "  -> new best val_ROC-AUC=0.5833 saved checkpoint\n",
      "Epoch 3/50 — train_loss=2.8226 val_loss=2.4385 val_ROC-AUC=0.6025 val_AP=0.0325 time=51.8s\n",
      "  -> new best val_ROC-AUC=0.6025 saved checkpoint\n",
      "Epoch 4/50 — train_loss=2.6010 val_loss=2.4479 val_ROC-AUC=0.6571 val_AP=0.0496 time=51.9s\n",
      "  -> new best val_ROC-AUC=0.6571 saved checkpoint\n",
      "Epoch 5/50 — train_loss=2.7636 val_loss=2.4938 val_ROC-AUC=0.6185 val_AP=0.0357 time=51.4s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 6/50 — train_loss=2.5315 val_loss=1.6293 val_ROC-AUC=0.6829 val_AP=0.0486 time=60.5s\n",
      "  -> new best val_ROC-AUC=0.6829 saved checkpoint\n",
      "Epoch 7/50 — train_loss=2.4768 val_loss=2.1942 val_ROC-AUC=0.7127 val_AP=0.0642 time=53.0s\n",
      "  -> new best val_ROC-AUC=0.7127 saved checkpoint\n",
      "Epoch 8/50 — train_loss=2.5512 val_loss=2.2930 val_ROC-AUC=0.7194 val_AP=0.0789 time=60.3s\n",
      "  -> new best val_ROC-AUC=0.7194 saved checkpoint\n",
      "Epoch 9/50 — train_loss=2.2758 val_loss=2.5308 val_ROC-AUC=0.6708 val_AP=0.0524 time=54.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 10/50 — train_loss=2.2496 val_loss=1.9000 val_ROC-AUC=0.6998 val_AP=0.0725 time=55.4s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 11/50 — train_loss=1.8175 val_loss=3.1712 val_ROC-AUC=0.7162 val_AP=0.1211 time=54.0s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 12/50 — train_loss=2.1435 val_loss=2.4158 val_ROC-AUC=0.7051 val_AP=0.0925 time=58.1s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 13/50 — train_loss=1.8125 val_loss=1.7673 val_ROC-AUC=0.7289 val_AP=0.1431 time=73.0s\n",
      "  -> new best val_ROC-AUC=0.7289 saved checkpoint\n",
      "Epoch 14/50 — train_loss=1.6279 val_loss=1.9341 val_ROC-AUC=0.7397 val_AP=0.2317 time=60.3s\n",
      "  -> new best val_ROC-AUC=0.7397 saved checkpoint\n",
      "Epoch 15/50 — train_loss=1.9506 val_loss=1.8071 val_ROC-AUC=0.7361 val_AP=0.3302 time=55.6s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 16/50 — train_loss=1.4641 val_loss=2.4377 val_ROC-AUC=0.7437 val_AP=0.3447 time=52.5s\n",
      "  -> new best val_ROC-AUC=0.7437 saved checkpoint\n",
      "Epoch 17/50 — train_loss=1.6551 val_loss=1.7550 val_ROC-AUC=0.7334 val_AP=0.3409 time=52.8s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 18/50 — train_loss=1.5661 val_loss=1.7149 val_ROC-AUC=0.7300 val_AP=0.3825 time=52.1s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 19/50 — train_loss=1.5105 val_loss=1.5830 val_ROC-AUC=0.7321 val_AP=0.4417 time=53.7s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 20/50 — train_loss=1.4538 val_loss=1.7111 val_ROC-AUC=0.7251 val_AP=0.3771 time=179.7s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 21/50 — train_loss=1.4037 val_loss=1.5390 val_ROC-AUC=0.7271 val_AP=0.4536 time=54.4s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 22/50 — train_loss=1.1610 val_loss=1.7907 val_ROC-AUC=0.7302 val_AP=0.3738 time=53.1s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 23/50 — train_loss=1.3286 val_loss=1.7287 val_ROC-AUC=0.7256 val_AP=0.3441 time=57.1s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 24/50 — train_loss=1.2462 val_loss=1.8804 val_ROC-AUC=0.7280 val_AP=0.3971 time=253.5s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 24 (best epoch 16 val_ROC-AUC=0.7436868686868686)\n",
      "Training finished. Best epoch: 16 best_metric: 0.7436868686868686\n",
      "Loaded best checkpoint from epoch 16 (val ROC-AUC=0.7437)\n",
      "\n",
      "=== TEST METRICS (RNN over SMILES) ===\n",
      "ROC-AUC: 0.7975\n",
      "AP: 0.2443\n",
      "Accuracy: 0.9852\n",
      "Precision: 0.6667\n",
      "Recall: 0.2500\n",
      "ConfusionMatrix: [[465, 1], [6, 2]]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_sequence_loaders(\n",
    "    train_df, val_df, test_df,\n",
    "    batch_size=cfg.batch_size\n",
    ")\n",
    "\n",
    "model = build_rnn_smiles(\n",
    "    vocab_size=len(stoi),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    rnn_type=\"lstm\",       # or \"gru\"\n",
    "    bidirectional=True,\n",
    "    dropout=cfg.dropout,\n",
    "    device=device\n",
    ")\n",
    "print(model)\n",
    "\n",
    "pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "cfg_local = copy.deepcopy(cfg)\n",
    "cfg_local.max_epochs = 50\n",
    "cfg_local.patience = 8\n",
    "\n",
    "history = run_training(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, scheduler, loss_fn,\n",
    "    cfg_local, model_name=\"rnn_smiles\", device=device\n",
    ")\n",
    "\n",
    "ckpt_path = Path(cfg.out_dir) / \"rnn_smiles\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(f\"Loaded best checkpoint from epoch {state.get('epoch')} (val ROC-AUC={state.get('metric'):.4f})\")\n",
    "\n",
    "model.eval()\n",
    "all_scores, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"\\n=== TEST METRICS (RNN over SMILES) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "out_dir = Path(cfg.out_dir) / \"rnn_smiles\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "json.dump(metrics, open(out_dir / \"test_metrics.json\", \"w\"), indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4159fb",
   "metadata": {
    "id": "6b4159fb"
   },
   "source": [
    "### 6.3 Transformer over SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c844981",
   "metadata": {
    "id": "8c844981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phineaswormser/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 — train_loss=3.7507 val_loss=1.2989 val_ROC-AUC=0.3896 val_AP=0.0207 time=15.2s\n",
      "  -> new best val_ROC-AUC=0.3896 saved checkpoint\n",
      "Epoch 2/20 — train_loss=3.2878 val_loss=1.5063 val_ROC-AUC=0.3526 val_AP=0.0196 time=14.4s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 3/20 — train_loss=3.3546 val_loss=1.3090 val_ROC-AUC=0.3667 val_AP=0.0201 time=14.2s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 4/20 — train_loss=3.1135 val_loss=1.2722 val_ROC-AUC=0.4435 val_AP=0.0229 time=16.2s\n",
      "  -> new best val_ROC-AUC=0.4435 saved checkpoint\n",
      "Epoch 5/20 — train_loss=3.0340 val_loss=1.3047 val_ROC-AUC=0.4046 val_AP=0.0213 time=13.0s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 6/20 — train_loss=2.9902 val_loss=1.3464 val_ROC-AUC=0.4722 val_AP=0.0244 time=14.2s\n",
      "  -> new best val_ROC-AUC=0.4722 saved checkpoint\n",
      "Epoch 7/20 — train_loss=2.6797 val_loss=1.2934 val_ROC-AUC=0.4293 val_AP=0.0222 time=15.1s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 8/20 — train_loss=2.6546 val_loss=1.4222 val_ROC-AUC=0.3907 val_AP=0.0208 time=16.5s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 9/20 — train_loss=2.5119 val_loss=1.2743 val_ROC-AUC=0.5236 val_AP=0.0292 time=16.3s\n",
      "  -> new best val_ROC-AUC=0.5236 saved checkpoint\n",
      "Epoch 10/20 — train_loss=2.7415 val_loss=1.3160 val_ROC-AUC=0.4392 val_AP=0.0227 time=13.2s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 11/20 — train_loss=2.4402 val_loss=1.2489 val_ROC-AUC=0.4408 val_AP=0.0228 time=13.6s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 12/20 — train_loss=2.3295 val_loss=1.2655 val_ROC-AUC=0.4628 val_AP=0.0250 time=12.8s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 13/20 — train_loss=2.0956 val_loss=1.2787 val_ROC-AUC=0.4787 val_AP=0.0254 time=13.0s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 14/20 — train_loss=2.0388 val_loss=1.2240 val_ROC-AUC=0.4903 val_AP=0.0269 time=14.3s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 14 (best epoch 9 val_ROC-AUC=0.5236291486291487)\n",
      "Training finished. Best epoch: 9 best_metric: 0.5236291486291487\n",
      "Loaded best checkpoint (epoch 9, val ROC-AUC=0.5236)\n",
      "\n",
      "=== TEST METRICS (Transformer over SMILES) ===\n",
      "ROC-AUC: 0.4633\n",
      "AP: 0.0364\n",
      "Accuracy: 0.9831\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[466, 0], [8, 0]]\n",
      "Saved results to runs_hw1/transformer_smiles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reuse SMILES sequence data loaders\n",
    "train_loader, val_loader, test_loader = get_sequence_loaders(\n",
    "    train_df, val_df, test_df,\n",
    "    batch_size=8  # keep small for speed\n",
    ")\n",
    "\n",
    "# Build model\n",
    "model = build_transformer_smiles(\n",
    "    vocab_size=len(vocab),\n",
    "    device=device,\n",
    "    embed_dim=128,  \n",
    "    nhead=4,       \n",
    "    num_layers=2,     \n",
    "    ff_dim=256,\n",
    "    dropout=0.1,\n",
    "    max_len=cfg.max_len\n",
    ")\n",
    "\n",
    "# Compute class imbalance weight\n",
    "pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "cfg_local = cfg\n",
    "cfg_local.max_epochs = 20  \n",
    "cfg_local.patience = 5\n",
    "\n",
    "# Train\n",
    "history = run_training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,\n",
    "    cfg_local,\n",
    "    model_name=\"transformer_smiles\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "ckpt_path = Path(cfg.out_dir) / \"transformer_smiles\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(f\"Loaded best checkpoint (epoch {state['epoch']}, val ROC-AUC={state['metric']:.4f})\")\n",
    "else:\n",
    "    print(\"No checkpoint found — evaluating last model state.\")\n",
    "\n",
    "model.eval()\n",
    "all_scores, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb).view(-1)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"\\n=== TEST METRICS (Transformer over SMILES) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Save final results\n",
    "out_dir = Path(cfg.out_dir) / \"transformer_smiles\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved results to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcab26",
   "metadata": {
    "id": "ddbcab26"
   },
   "source": [
    "### 6.4 GNN on Molecular Graphs (GCN/GIN/GraphSAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7bc8e43",
   "metadata": {
    "id": "a7bc8e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom feature dim: 22\n",
      "Bond feature dim: 8\n",
      "GNN node feature dim: 22\n",
      "GraphNet(\n",
      "  (blocks): ModuleList(\n",
      "    (0): GraphBlock(\n",
      "      (conv): GCNConv(22, 256)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1-3): 3 x GraphBlock(\n",
      "      (conv): GCNConv(256, 256)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/40 — train_loss=2.5742 val_loss=2.2860 val_ROC-AUC=0.7318 val_AP=0.0527 time=4.9s\n",
      "  -> new best val_ROC-AUC=0.7318 saved checkpoint\n",
      "Epoch 2/40 — train_loss=3.6497 val_loss=2.5782 val_ROC-AUC=0.6865 val_AP=0.0411 time=3.8s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/40 — train_loss=3.5010 val_loss=2.7467 val_ROC-AUC=0.6932 val_AP=0.0403 time=3.8s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/40 — train_loss=3.5828 val_loss=2.3435 val_ROC-AUC=0.7480 val_AP=0.0486 time=3.9s\n",
      "  -> new best val_ROC-AUC=0.7480 saved checkpoint\n",
      "Epoch 5/40 — train_loss=3.5394 val_loss=2.2484 val_ROC-AUC=0.7554 val_AP=0.0499 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.7554 saved checkpoint\n",
      "Epoch 6/40 — train_loss=3.4304 val_loss=2.7002 val_ROC-AUC=0.7834 val_AP=0.0565 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.7834 saved checkpoint\n",
      "Epoch 7/40 — train_loss=3.4043 val_loss=2.2753 val_ROC-AUC=0.8045 val_AP=0.0627 time=4.0s\n",
      "  -> new best val_ROC-AUC=0.8045 saved checkpoint\n",
      "Epoch 8/40 — train_loss=3.3016 val_loss=3.0273 val_ROC-AUC=0.7953 val_AP=0.0597 time=3.8s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 9/40 — train_loss=3.7346 val_loss=2.5959 val_ROC-AUC=0.8458 val_AP=0.0787 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.8458 saved checkpoint\n",
      "Epoch 10/40 — train_loss=3.2613 val_loss=2.5739 val_ROC-AUC=0.8562 val_AP=0.0835 time=3.9s\n",
      "  -> new best val_ROC-AUC=0.8562 saved checkpoint\n",
      "Epoch 11/40 — train_loss=3.6483 val_loss=2.3872 val_ROC-AUC=0.8694 val_AP=0.0920 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.8694 saved checkpoint\n",
      "Epoch 12/40 — train_loss=3.2106 val_loss=2.6808 val_ROC-AUC=0.9140 val_AP=0.1435 time=3.9s\n",
      "  -> new best val_ROC-AUC=0.9140 saved checkpoint\n",
      "Epoch 13/40 — train_loss=3.4377 val_loss=2.1570 val_ROC-AUC=0.9365 val_AP=0.2060 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.9365 saved checkpoint\n",
      "Epoch 14/40 — train_loss=3.2568 val_loss=1.7678 val_ROC-AUC=0.9560 val_AP=0.3787 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.9560 saved checkpoint\n",
      "Epoch 15/40 — train_loss=3.2109 val_loss=2.8012 val_ROC-AUC=0.9306 val_AP=0.2586 time=5.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 16/40 — train_loss=3.3532 val_loss=1.3738 val_ROC-AUC=0.9609 val_AP=0.6661 time=3.9s\n",
      "  -> new best val_ROC-AUC=0.9609 saved checkpoint\n",
      "Epoch 17/40 — train_loss=3.1210 val_loss=3.1010 val_ROC-AUC=0.9333 val_AP=0.3432 time=4.6s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 18/40 — train_loss=3.1402 val_loss=0.7579 val_ROC-AUC=0.9578 val_AP=0.5526 time=3.9s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 19/40 — train_loss=2.9558 val_loss=0.8983 val_ROC-AUC=0.9692 val_AP=0.7329 time=4.1s\n",
      "  -> new best val_ROC-AUC=0.9692 saved checkpoint\n",
      "Epoch 20/40 — train_loss=2.8072 val_loss=1.6617 val_ROC-AUC=0.9655 val_AP=0.7249 time=4.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 21/40 — train_loss=2.7758 val_loss=0.9728 val_ROC-AUC=0.9688 val_AP=0.7283 time=3.9s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 22/40 — train_loss=3.1940 val_loss=2.4658 val_ROC-AUC=0.9392 val_AP=0.5895 time=4.5s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 23/40 — train_loss=2.9783 val_loss=0.5314 val_ROC-AUC=0.9704 val_AP=0.6694 time=4.0s\n",
      "  -> new best val_ROC-AUC=0.9704 saved checkpoint\n",
      "Epoch 24/40 — train_loss=2.7386 val_loss=1.9571 val_ROC-AUC=0.9372 val_AP=0.6923 time=3.8s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 25/40 — train_loss=2.9217 val_loss=1.0086 val_ROC-AUC=0.9461 val_AP=0.6636 time=4.0s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 26/40 — train_loss=2.7001 val_loss=1.1347 val_ROC-AUC=0.9486 val_AP=0.6792 time=4.8s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 27/40 — train_loss=2.6532 val_loss=1.1270 val_ROC-AUC=0.9399 val_AP=0.6745 time=4.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 28/40 — train_loss=2.5513 val_loss=0.9191 val_ROC-AUC=0.9378 val_AP=0.7075 time=4.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 29/40 — train_loss=2.3544 val_loss=1.3347 val_ROC-AUC=0.9055 val_AP=0.7063 time=4.7s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 30/40 — train_loss=2.6537 val_loss=1.1583 val_ROC-AUC=0.9264 val_AP=0.7349 time=5.6s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 31/40 — train_loss=2.6644 val_loss=1.3829 val_ROC-AUC=0.9172 val_AP=0.7469 time=5.1s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 31 (best epoch 23 val_ROC-AUC=0.9704184704184704)\n",
      "Training finished. Best epoch: 23 best_metric: 0.9704184704184704\n",
      "Loaded best checkpoint from epoch 23 (val ROC-AUC=0.9704)\n",
      "\n",
      "=== TEST METRICS (GNN) ===\n",
      "ROC-AUC: 0.5606\n",
      "AP: 0.1615\n",
      "Accuracy: 0.9473\n",
      "Precision: 0.0952\n",
      "Recall: 0.2500\n",
      "ConfusionMatrix: [[447, 19], [6, 2]]\n",
      "Saved results to runs_hw1/gnn_gcn_mean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_graph_loaders(\n",
    "    train_df, val_df, test_df,\n",
    "    batch_size=cfg.batch_size,\n",
    "    need_3d=False \n",
    ")\n",
    "\n",
    "b = next(iter(train_df[cfg.smiles_col])) \n",
    "data = smiles_to_pyg(b)\n",
    "print(\"Atom feature dim:\", data.x.shape[1])\n",
    "print(\"Bond feature dim:\", data.edge_attr.shape[1])\n",
    "\n",
    "# Infer node feature size from one batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "in_dim = sample_batch.x.size(-1)\n",
    "print(f\"GNN node feature dim: {in_dim}\")\n",
    "\n",
    "# 2) Build model\n",
    "gnn_kind = getattr(cfg, \"pyg_backend\", \"gcn\")\n",
    "model = GraphNet(\n",
    "    in_dim=in_dim,\n",
    "    hidden=cfg.gnn_hidden,\n",
    "    layers=cfg.gnn_layers,\n",
    "    kind=gnn_kind,\n",
    "    pool=cfg.global_pool,\n",
    "    dropout=cfg.gnn_dropout\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# 3) Loss/opt/scheduler\n",
    "pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "# 4) Train\n",
    "cfg_local = copy.deepcopy(cfg)\n",
    "cfg_local.max_epochs = 40\n",
    "cfg_local.patience = 8\n",
    "run_name = f\"gnn_{gnn_kind}_{cfg.global_pool}\"\n",
    "\n",
    "history = run_training(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, scheduler, loss_fn,\n",
    "    cfg_local, model_name=run_name, device=device\n",
    ")\n",
    "\n",
    "# 5) Load best checkpoint and evaluate on test set\n",
    "ckpt_path = Path(cfg.out_dir) / run_name / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(f\"Loaded best checkpoint from epoch {state.get('epoch')} (val ROC-AUC={state.get('metric'):.4f})\")\n",
    "else:\n",
    "    print(\"No checkpoint found; evaluating last model state.\")\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "all_scores, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch).view(-1)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(batch.y.view(-1).cpu().numpy())\n",
    "\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "print(\"\\n=== TEST METRICS (GNN) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Save artifacts\n",
    "out_dir = Path(cfg.out_dir) / run_name\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved results to\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d731dd",
   "metadata": {
    "id": "e1d731dd"
   },
   "source": [
    "## 8) Experiments\n",
    "\n",
    "Various experiments with different hyperparameters and model architectures. Report can be found in repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "TXKzorD3aD0q",
   "metadata": {
    "id": "TXKzorD3aD0q"
   },
   "outputs": [],
   "source": [
    "def run_mlp_experiment(\n",
    "    name=\"exp\",\n",
    "    batch_size=32,\n",
    "    hidden_sizes=(512,256),\n",
    "    dropout=0.1,\n",
    "    weight_decay=1e-4,\n",
    "    lr=1e-4,\n",
    "    grad_clip=None,\n",
    "    scheduler_type=\"plateau\",\n",
    "    sampler_type=\"weighted\",  \n",
    "    pos_weight=True,\n",
    "    max_epochs=30,\n",
    "    patience=8,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic experiment runner for the fingerprint MLP.\n",
    "    Returns a dict of config + metrics (val/test ROC-AUC, AP, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(cfg.out_dir) / f\"experiments/{name}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    X_tr, y_tr = precompute_fps(train_df, \"train_exp_fp.npz\")\n",
    "    X_val, y_val = precompute_fps(val_df,   \"val_exp_fp.npz\")\n",
    "    X_test, y_test = precompute_fps(test_df, \"test_exp_fp.npz\")\n",
    "\n",
    "    if sampler_type == \"weighted\":\n",
    "        # balance classes by inverse frequency\n",
    "        vals, counts = np.unique(y_tr, return_counts=True)\n",
    "        class_w = {v: 1.0 / c for v,c in zip(vals, counts)}\n",
    "        sample_w = np.array([class_w[int(v)] for v in y_tr])\n",
    "        sampler = WeightedRandomSampler(\n",
    "            torch.from_numpy(sample_w), num_samples=len(sample_w), replacement=True\n",
    "        )\n",
    "        shuffle = False\n",
    "    else:\n",
    "        sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)),\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)),\n",
    "        batch_size=batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)),\n",
    "        batch_size=batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "\n",
    "    model = build_mlp_fingerprint(\n",
    "        in_dim=X_tr.shape[1],\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if pos_weight:\n",
    "        pw = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    else:\n",
    "        pw = None\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pw, device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if scheduler_type == \"plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "    elif scheduler_type == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    elif scheduler_type == \"step\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, max_epochs//3), gamma=0.5)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = max_epochs\n",
    "    cfg_local.patience = patience\n",
    "    cfg_local.grad_clip = grad_clip\n",
    "    history = run_training(model, train_loader, val_loader, optimizer, scheduler, loss_fn,\n",
    "                           cfg_local, model_name=name, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb).view(-1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "\n",
    "    results = {\n",
    "        \"config\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"hidden_sizes\": hidden_sizes,\n",
    "            \"dropout\": dropout,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": lr,\n",
    "            \"grad_clip\": grad_clip,\n",
    "            \"scheduler_type\": scheduler_type,\n",
    "            \"sampler_type\": sampler_type,\n",
    "            \"pos_weight\": pos_weight,\n",
    "        },\n",
    "        \"val_best\": {\n",
    "            \"ROC-AUC\": max(history[\"val_ROC-AUC\"]),\n",
    "            \"AP\": max(history[\"val_AP\"]),\n",
    "        },\n",
    "        \"test\": metrics,\n",
    "    }\n",
    "\n",
    "    with open(out_dir / \"result.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"=== {name} finished ===\")\n",
    "    print(\"Val best ROC-AUC:\", results[\"val_best\"][\"ROC-AUC\"])\n",
    "    print(\"Test ROC-AUC:\", results[\"test\"][\"ROC-AUC\"])\n",
    "    print(\"Test AP:\", results[\"test\"][\"AP\"])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eab5694-db41-46be-ad68-f6edf70189b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=2.3641 val_loss=1.7799 val_ROC-AUC=0.9767 val_AP=0.8591 time=1.1s\n",
      "  -> new best val_ROC-AUC=0.9767 saved checkpoint\n",
      "Epoch 2/30 — train_loss=0.7447 val_loss=0.7137 val_ROC-AUC=0.8979 val_AP=0.8323 time=1.0s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.2586 val_loss=0.5711 val_ROC-AUC=0.8849 val_AP=0.8316 time=1.0s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1027 val_loss=0.7194 val_ROC-AUC=0.8707 val_AP=0.8134 time=1.1s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0540 val_loss=0.8513 val_ROC-AUC=0.8692 val_AP=0.8226 time=1.4s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0382 val_loss=0.8879 val_ROC-AUC=0.8714 val_AP=0.8226 time=1.7s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0274 val_loss=0.9375 val_ROC-AUC=0.8709 val_AP=0.8226 time=1.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0267 val_loss=1.0120 val_ROC-AUC=0.8701 val_AP=0.8226 time=1.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0202 val_loss=1.0575 val_ROC-AUC=0.8694 val_AP=0.8226 time=2.5s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9767316017316018)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9767316017316018\n",
      "=== batch8 finished ===\n",
      "Val best ROC-AUC: 0.9767316017316018\n",
      "Test ROC-AUC: 0.5093884120171674\n",
      "Test AP: 0.22214481454231172\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.7453 val_loss=1.7694 val_ROC-AUC=0.9901 val_AP=0.6739 time=0.4s\n",
      "  -> new best val_ROC-AUC=0.9901 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5944 val_loss=1.9068 val_ROC-AUC=0.9968 val_AP=0.9277 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9968 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.1636 val_loss=1.5014 val_ROC-AUC=0.9924 val_AP=0.8927 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.8710 val_loss=1.0770 val_ROC-AUC=0.9755 val_AP=0.8584 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5933 val_loss=0.7659 val_ROC-AUC=0.9450 val_AP=0.8375 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.3842 val_loss=0.5780 val_ROC-AUC=0.9183 val_AP=0.8255 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.2745 val_loss=0.5433 val_ROC-AUC=0.9091 val_AP=0.8247 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.2064 val_loss=0.5269 val_ROC-AUC=0.9042 val_AP=0.8327 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1781 val_loss=0.5232 val_ROC-AUC=0.9012 val_AP=0.8325 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.1457 val_loss=0.5274 val_ROC-AUC=0.9003 val_AP=0.8324 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9967532467532468)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9967532467532468\n",
      "=== batch32 finished ===\n",
      "Val best ROC-AUC: 0.9967532467532468\n",
      "Test ROC-AUC: 0.5211909871244635\n",
      "Test AP: 0.2639820719100531\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=7.5317 val_loss=1.2285 val_ROC-AUC=0.9738 val_AP=0.5995 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9738 saved checkpoint\n",
      "Epoch 2/30 — train_loss=3.8539 val_loss=1.4482 val_ROC-AUC=0.9919 val_AP=0.8116 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9919 saved checkpoint\n",
      "Epoch 3/30 — train_loss=2.3363 val_loss=1.6675 val_ROC-AUC=0.9955 val_AP=0.8940 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9955 saved checkpoint\n",
      "Epoch 4/30 — train_loss=1.6920 val_loss=1.7705 val_ROC-AUC=0.9968 val_AP=0.9270 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9968 saved checkpoint\n",
      "Epoch 5/30 — train_loss=1.4524 val_loss=1.7720 val_ROC-AUC=0.9955 val_AP=0.9135 time=0.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 6/30 — train_loss=1.2818 val_loss=1.6826 val_ROC-AUC=0.9939 val_AP=0.9006 time=0.1s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 7/30 — train_loss=1.1649 val_loss=1.5549 val_ROC-AUC=0.9908 val_AP=0.8853 time=0.1s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 8/30 — train_loss=0.9964 val_loss=1.4050 val_ROC-AUC=0.9834 val_AP=0.8672 time=0.1s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 9/30 — train_loss=0.9604 val_loss=1.3308 val_ROC-AUC=0.9747 val_AP=0.8573 time=0.1s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 10/30 — train_loss=0.9210 val_loss=1.2538 val_ROC-AUC=0.9688 val_AP=0.8530 time=0.1s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 11/30 — train_loss=0.8247 val_loss=1.1801 val_ROC-AUC=0.9623 val_AP=0.8427 time=0.1s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 12/30 — train_loss=0.7633 val_loss=1.1048 val_ROC-AUC=0.9578 val_AP=0.8409 time=0.1s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 12 (best epoch 4 val_ROC-AUC=0.9967532467532467)\n",
      "Training finished. Best epoch: 4 best_metric: 0.9967532467532467\n",
      "=== batch128 finished ===\n",
      "Val best ROC-AUC: 0.9967532467532467\n",
      "Test ROC-AUC: 0.6386802575107295\n",
      "Test AP: 0.2913686976791535\n",
      "\n",
      "Batch Size Results:\n",
      "{'batch_size': 8, 'val_ROC-AUC': 0.9767316017316018, 'test_ROC-AUC': 0.5093884120171674, 'test_AP': 0.22214481454231172}\n",
      "{'batch_size': 32, 'val_ROC-AUC': 0.9967532467532468, 'test_ROC-AUC': 0.5211909871244635, 'test_AP': 0.2639820719100531}\n",
      "{'batch_size': 128, 'val_ROC-AUC': 0.9967532467532467, 'test_ROC-AUC': 0.6386802575107295, 'test_AP': 0.2913686976791535}\n"
     ]
    }
   ],
   "source": [
    "# === 8.2.1 Batch Size Experiment ===\n",
    "batch_sizes = [8, 32, 128]\n",
    "batch_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    res = run_mlp_experiment(name=f\"batch{bs}\", batch_size=bs)\n",
    "    batch_results.append({\n",
    "        \"batch_size\": bs,\n",
    "        \"val_ROC-AUC\": res[\"val_best\"][\"ROC-AUC\"],\n",
    "        \"test_ROC-AUC\": res[\"test\"][\"ROC-AUC\"],\n",
    "        \"test_AP\": res[\"test\"][\"AP\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nBatch Size Results:\")\n",
    "for r in batch_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a42c35cd-9beb-4588-a46b-efe6cb29b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.1352 val_loss=1.8805 val_ROC-AUC=0.9904 val_AP=0.7775 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9904 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4415 val_loss=1.7768 val_ROC-AUC=0.9908 val_AP=0.8870 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9908 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.0109 val_loss=1.3268 val_ROC-AUC=0.9668 val_AP=0.8486 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.7203 val_loss=0.8989 val_ROC-AUC=0.9241 val_AP=0.8348 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.4333 val_loss=0.6556 val_ROC-AUC=0.9013 val_AP=0.8242 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.2762 val_loss=0.5814 val_ROC-AUC=0.8826 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.1877 val_loss=0.5851 val_ROC-AUC=0.8801 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1425 val_loss=0.6030 val_ROC-AUC=0.8784 val_AP=0.8229 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1139 val_loss=0.6245 val_ROC-AUC=0.8777 val_AP=0.8229 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.1035 val_loss=0.6506 val_ROC-AUC=0.8775 val_AP=0.8229 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9908008658008659)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9908008658008659\n",
      "=== reg_do0.0_wd0.0_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9908008658008659\n",
      "Test ROC-AUC: 0.4991952789699571\n",
      "Test AP: 0.26331173307522326\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.0238 val_loss=2.0296 val_ROC-AUC=0.9931 val_AP=0.8978 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9931 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.2167 val_loss=1.1653 val_ROC-AUC=0.9751 val_AP=0.8497 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.4799 val_loss=0.5375 val_ROC-AUC=0.9113 val_AP=0.8249 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1490 val_loss=0.6319 val_ROC-AUC=0.8835 val_AP=0.8232 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0527 val_loss=0.7592 val_ROC-AUC=0.8829 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0430 val_loss=0.8213 val_ROC-AUC=0.8790 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0258 val_loss=0.8585 val_ROC-AUC=0.8793 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0191 val_loss=0.8849 val_ROC-AUC=0.8797 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0232 val_loss=0.9610 val_ROC-AUC=0.8757 val_AP=0.8228 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9931457431457431)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9931457431457431\n",
      "=== reg_do0.0_wd0.0_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9931457431457431\n",
      "Test ROC-AUC: 0.5128755364806866\n",
      "Test AP: 0.2640380449813158\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=3.9508 val_loss=1.8795 val_ROC-AUC=0.9962 val_AP=0.8852 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9962 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4133 val_loss=1.8030 val_ROC-AUC=0.9948 val_AP=0.9077 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.0330 val_loss=1.3173 val_ROC-AUC=0.9839 val_AP=0.8679 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.7231 val_loss=0.8860 val_ROC-AUC=0.9410 val_AP=0.8367 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.4639 val_loss=0.6314 val_ROC-AUC=0.9021 val_AP=0.8242 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.3111 val_loss=0.5887 val_ROC-AUC=0.8889 val_AP=0.8235 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.2325 val_loss=0.5722 val_ROC-AUC=0.8810 val_AP=0.8231 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.2111 val_loss=0.5785 val_ROC-AUC=0.8752 val_AP=0.8228 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.1613 val_loss=0.5879 val_ROC-AUC=0.8755 val_AP=0.8228 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9962121212121211)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9962121212121211\n",
      "=== reg_do0.0_wd1e-05_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9962121212121211\n",
      "Test ROC-AUC: 0.49007510729613735\n",
      "Test AP: 0.2628616273110003\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=3.8904 val_loss=2.1738 val_ROC-AUC=0.9885 val_AP=0.8707 time=0.4s\n",
      "  -> new best val_ROC-AUC=0.9885 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.1482 val_loss=1.0861 val_ROC-AUC=0.9394 val_AP=0.8370 time=0.4s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.4336 val_loss=0.5336 val_ROC-AUC=0.8882 val_AP=0.8318 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1474 val_loss=0.6602 val_ROC-AUC=0.8580 val_AP=0.8222 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0649 val_loss=0.7586 val_ROC-AUC=0.8644 val_AP=0.8224 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0342 val_loss=0.7998 val_ROC-AUC=0.8645 val_AP=0.8224 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0285 val_loss=0.8503 val_ROC-AUC=0.8635 val_AP=0.8224 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0211 val_loss=0.9095 val_ROC-AUC=0.8598 val_AP=0.8222 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0180 val_loss=0.9609 val_ROC-AUC=0.8591 val_AP=0.8222 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9884559884559885)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9884559884559885\n",
      "=== reg_do0.0_wd1e-05_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9884559884559885\n",
      "Test ROC-AUC: 0.5217274678111588\n",
      "Test AP: 0.2642051306270694\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.8110 val_loss=1.7606 val_ROC-AUC=0.9953 val_AP=0.8477 time=0.4s\n",
      "  -> new best val_ROC-AUC=0.9953 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4717 val_loss=1.7159 val_ROC-AUC=0.9930 val_AP=0.8955 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.0592 val_loss=1.2710 val_ROC-AUC=0.9775 val_AP=0.8592 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.7304 val_loss=0.8616 val_ROC-AUC=0.9493 val_AP=0.8459 time=0.4s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.4661 val_loss=0.5979 val_ROC-AUC=0.9333 val_AP=0.8355 time=0.4s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.3111 val_loss=0.5384 val_ROC-AUC=0.9291 val_AP=0.8350 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.2666 val_loss=0.5055 val_ROC-AUC=0.9203 val_AP=0.8340 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.1968 val_loss=0.5076 val_ROC-AUC=0.9114 val_AP=0.8332 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.1597 val_loss=0.5258 val_ROC-AUC=0.9058 val_AP=0.8328 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9953102453102454)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9953102453102454\n",
      "=== reg_do0.0_wd0.0001_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9953102453102454\n",
      "Test ROC-AUC: 0.5108637339055795\n",
      "Test AP: 0.22209038719436236\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.4009 val_loss=2.1345 val_ROC-AUC=0.9605 val_AP=0.8493 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9605 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.1732 val_loss=1.0617 val_ROC-AUC=0.9066 val_AP=0.8329 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.4162 val_loss=0.5628 val_ROC-AUC=0.8883 val_AP=0.8393 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1184 val_loss=0.6759 val_ROC-AUC=0.8716 val_AP=0.8310 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0467 val_loss=0.8014 val_ROC-AUC=0.8687 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0305 val_loss=0.8519 val_ROC-AUC=0.8683 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0330 val_loss=0.9239 val_ROC-AUC=0.8669 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0229 val_loss=0.9527 val_ROC-AUC=0.8680 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0249 val_loss=0.9978 val_ROC-AUC=0.8678 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9604978354978355)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9604978354978355\n",
      "=== reg_do0.0_wd0.0001_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9604978354978355\n",
      "Test ROC-AUC: 0.5187768240343348\n",
      "Test AP: 0.2640128013513611\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.6256 val_loss=1.6318 val_ROC-AUC=0.9594 val_AP=0.5574 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9594 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.7310 val_loss=1.8187 val_ROC-AUC=0.9470 val_AP=0.8497 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.2127 val_loss=1.5416 val_ROC-AUC=0.9295 val_AP=0.8456 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.8602 val_loss=1.1686 val_ROC-AUC=0.9066 val_AP=0.8408 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.6366 val_loss=0.8352 val_ROC-AUC=0.8817 val_AP=0.8315 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.4515 val_loss=0.7385 val_ROC-AUC=0.8790 val_AP=0.8230 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.3696 val_loss=0.6685 val_ROC-AUC=0.8761 val_AP=0.8229 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.3100 val_loss=0.6290 val_ROC-AUC=0.8772 val_AP=0.8229 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.2671 val_loss=0.6108 val_ROC-AUC=0.8737 val_AP=0.8228 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9594155844155844)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9594155844155844\n",
      "=== reg_do0.1_wd0.0_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9594155844155844\n",
      "Test ROC-AUC: 0.5284334763948498\n",
      "Test AP: 0.2649749943518473\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.1647 val_loss=2.1813 val_ROC-AUC=0.9966 val_AP=0.9291 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9966 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.2406 val_loss=1.3391 val_ROC-AUC=0.9901 val_AP=0.8985 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.5805 val_loss=0.4875 val_ROC-AUC=0.9650 val_AP=0.8377 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1727 val_loss=0.4406 val_ROC-AUC=0.9376 val_AP=0.8285 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0649 val_loss=0.5704 val_ROC-AUC=0.9190 val_AP=0.8259 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0395 val_loss=0.6170 val_ROC-AUC=0.9152 val_AP=0.8255 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0375 val_loss=0.6889 val_ROC-AUC=0.9100 val_AP=0.8250 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0270 val_loss=0.7188 val_ROC-AUC=0.9104 val_AP=0.8251 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0307 val_loss=0.7496 val_ROC-AUC=0.9107 val_AP=0.8251 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9965728715728716)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9965728715728716\n",
      "=== reg_do0.1_wd0.0_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9965728715728716\n",
      "Test ROC-AUC: 0.5160944206008584\n",
      "Test AP: 0.2636873662967669\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=3.9096 val_loss=1.9298 val_ROC-AUC=0.9580 val_AP=0.8255 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9580 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5077 val_loss=1.9857 val_ROC-AUC=0.9643 val_AP=0.8513 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9643 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.1413 val_loss=1.5110 val_ROC-AUC=0.9141 val_AP=0.8410 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.8242 val_loss=1.0564 val_ROC-AUC=0.8909 val_AP=0.8394 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5489 val_loss=0.7603 val_ROC-AUC=0.8748 val_AP=0.8387 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.3399 val_loss=0.6413 val_ROC-AUC=0.8683 val_AP=0.8384 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.2498 val_loss=0.6200 val_ROC-AUC=0.8676 val_AP=0.8384 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.2065 val_loss=0.6302 val_ROC-AUC=0.8633 val_AP=0.8307 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1663 val_loss=0.6483 val_ROC-AUC=0.8617 val_AP=0.8306 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.1294 val_loss=0.6705 val_ROC-AUC=0.8620 val_AP=0.8306 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9642857142857142)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9642857142857142\n",
      "=== reg_do0.1_wd1e-05_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9642857142857142\n",
      "Test ROC-AUC: 0.5131437768240343\n",
      "Test AP: 0.1891915172994639\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.4965 val_loss=2.0887 val_ROC-AUC=0.9540 val_AP=0.7104 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9540 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.2378 val_loss=1.2697 val_ROC-AUC=0.8957 val_AP=0.8399 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.5426 val_loss=0.6361 val_ROC-AUC=0.8754 val_AP=0.8311 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1842 val_loss=0.6527 val_ROC-AUC=0.8626 val_AP=0.8223 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0672 val_loss=0.8114 val_ROC-AUC=0.8586 val_AP=0.8222 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0433 val_loss=0.8605 val_ROC-AUC=0.8600 val_AP=0.8222 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0295 val_loss=0.9229 val_ROC-AUC=0.8586 val_AP=0.8222 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0299 val_loss=0.9772 val_ROC-AUC=0.8586 val_AP=0.8222 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0291 val_loss=1.0101 val_ROC-AUC=0.8597 val_AP=0.8222 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.954004329004329)\n",
      "Training finished. Best epoch: 1 best_metric: 0.954004329004329\n",
      "=== reg_do0.1_wd1e-05_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.954004329004329\n",
      "Test ROC-AUC: 0.5386266094420601\n",
      "Test AP: 0.20233593999911106\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.6242 val_loss=1.8083 val_ROC-AUC=0.9886 val_AP=0.8096 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9886 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5499 val_loss=1.8348 val_ROC-AUC=0.9917 val_AP=0.8978 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9917 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.1041 val_loss=1.4334 val_ROC-AUC=0.9852 val_AP=0.8825 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.8317 val_loss=1.0480 val_ROC-AUC=0.9551 val_AP=0.8501 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5528 val_loss=0.7278 val_ROC-AUC=0.9306 val_AP=0.8434 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.3554 val_loss=0.5519 val_ROC-AUC=0.9248 val_AP=0.8347 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.2602 val_loss=0.5183 val_ROC-AUC=0.9152 val_AP=0.8337 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.2009 val_loss=0.5079 val_ROC-AUC=0.9122 val_AP=0.8334 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1758 val_loss=0.5098 val_ROC-AUC=0.9100 val_AP=0.8332 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.1357 val_loss=0.5189 val_ROC-AUC=0.9082 val_AP=0.8330 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9917027417027416)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9917027417027416\n",
      "=== reg_do0.1_wd0.0001_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9917027417027416\n",
      "Test ROC-AUC: 0.4916845493562232\n",
      "Test AP: 0.26279395547701473\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.1615 val_loss=2.1313 val_ROC-AUC=0.9883 val_AP=0.8500 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9883 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.1946 val_loss=1.2600 val_ROC-AUC=0.9679 val_AP=0.8535 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.5493 val_loss=0.5828 val_ROC-AUC=0.9015 val_AP=0.8401 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.1592 val_loss=0.5630 val_ROC-AUC=0.8988 val_AP=0.8240 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.0801 val_loss=0.7262 val_ROC-AUC=0.8829 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0443 val_loss=0.7798 val_ROC-AUC=0.8829 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0405 val_loss=0.8329 val_ROC-AUC=0.8802 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0241 val_loss=0.8514 val_ROC-AUC=0.8811 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0305 val_loss=0.9156 val_ROC-AUC=0.8797 val_AP=0.8230 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9882756132756133)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9882756132756133\n",
      "=== reg_do0.1_wd0.0001_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9882756132756133\n",
      "Test ROC-AUC: 0.5254828326180258\n",
      "Test AP: 0.2646392285084977\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.4266 val_loss=1.6807 val_ROC-AUC=0.9610 val_AP=0.5651 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9610 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.8351 val_loss=2.0102 val_ROC-AUC=0.9717 val_AP=0.8518 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9717 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.3376 val_loss=1.8414 val_ROC-AUC=0.9655 val_AP=0.8474 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.0761 val_loss=1.5142 val_ROC-AUC=0.9295 val_AP=0.8274 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.8673 val_loss=1.1375 val_ROC-AUC=0.8927 val_AP=0.8237 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.6033 val_loss=0.8666 val_ROC-AUC=0.8750 val_AP=0.8228 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.4609 val_loss=0.7612 val_ROC-AUC=0.8687 val_AP=0.8225 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.3821 val_loss=0.7038 val_ROC-AUC=0.8620 val_AP=0.8223 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.3226 val_loss=0.6690 val_ROC-AUC=0.8622 val_AP=0.8223 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.2580 val_loss=0.6518 val_ROC-AUC=0.8635 val_AP=0.8224 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9716810966810967)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9716810966810967\n",
      "=== reg_do0.3_wd0.0_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9716810966810967\n",
      "Test ROC-AUC: 0.5421137339055795\n",
      "Test AP: 0.26616062440605753\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.1763 val_loss=2.2746 val_ROC-AUC=0.9751 val_AP=0.7385 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9751 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4192 val_loss=1.7748 val_ROC-AUC=0.9554 val_AP=0.8448 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.8454 val_loss=0.8918 val_ROC-AUC=0.9076 val_AP=0.8335 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.4051 val_loss=0.5887 val_ROC-AUC=0.8741 val_AP=0.8311 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.1496 val_loss=0.6835 val_ROC-AUC=0.8718 val_AP=0.8227 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0841 val_loss=0.7342 val_ROC-AUC=0.8714 val_AP=0.8227 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0638 val_loss=0.7879 val_ROC-AUC=0.8710 val_AP=0.8226 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0524 val_loss=0.8407 val_ROC-AUC=0.8703 val_AP=0.8226 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0322 val_loss=0.8767 val_ROC-AUC=0.8710 val_AP=0.8226 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.975108225108225)\n",
      "Training finished. Best epoch: 1 best_metric: 0.975108225108225\n",
      "=== reg_do0.3_wd0.0_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.975108225108225\n",
      "Test ROC-AUC: 0.5160944206008584\n",
      "Test AP: 0.26382973397224335\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.6433 val_loss=1.7152 val_ROC-AUC=0.9939 val_AP=0.8403 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9939 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.8707 val_loss=2.0023 val_ROC-AUC=0.9977 val_AP=0.9399 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9977 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.3872 val_loss=1.8051 val_ROC-AUC=0.9968 val_AP=0.9277 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.1098 val_loss=1.5925 val_ROC-AUC=0.9924 val_AP=0.8856 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.9157 val_loss=1.2266 val_ROC-AUC=0.9769 val_AP=0.8432 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.6924 val_loss=0.9191 val_ROC-AUC=0.9390 val_AP=0.8281 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.5403 val_loss=0.7859 val_ROC-AUC=0.9293 val_AP=0.8267 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.4659 val_loss=0.7012 val_ROC-AUC=0.9158 val_AP=0.8253 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.3764 val_loss=0.6319 val_ROC-AUC=0.9114 val_AP=0.8249 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.3385 val_loss=0.5873 val_ROC-AUC=0.9013 val_AP=0.8242 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9976551226551226)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9976551226551226\n",
      "=== reg_do0.3_wd1e-05_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9976551226551226\n",
      "Test ROC-AUC: 0.5244098712446352\n",
      "Test AP: 0.2640449351083859\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.9789 val_loss=1.9607 val_ROC-AUC=0.9895 val_AP=0.7329 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9895 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4988 val_loss=1.8758 val_ROC-AUC=0.9953 val_AP=0.9136 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9953 saved checkpoint\n",
      "Epoch 3/30 — train_loss=0.8965 val_loss=0.8795 val_ROC-AUC=0.9502 val_AP=0.8248 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.3808 val_loss=0.5015 val_ROC-AUC=0.9177 val_AP=0.8339 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.1464 val_loss=0.5707 val_ROC-AUC=0.9021 val_AP=0.8243 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.0746 val_loss=0.7061 val_ROC-AUC=0.8911 val_AP=0.8236 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.0465 val_loss=0.7658 val_ROC-AUC=0.8878 val_AP=0.8234 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.0378 val_loss=0.8325 val_ROC-AUC=0.8829 val_AP=0.8232 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.0300 val_loss=0.8757 val_ROC-AUC=0.8819 val_AP=0.8231 time=0.4s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0261 val_loss=0.9233 val_ROC-AUC=0.8810 val_AP=0.8231 time=0.5s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9953102453102454)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9953102453102454\n",
      "=== reg_do0.3_wd1e-05_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9953102453102454\n",
      "Test ROC-AUC: 0.48873390557939916\n",
      "Test AP: 0.151307235526834\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.1757 val_loss=1.7625 val_ROC-AUC=0.9603 val_AP=0.3520 time=0.4s\n",
      "  -> new best val_ROC-AUC=0.9603 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.8996 val_loss=2.0959 val_ROC-AUC=0.9912 val_AP=0.8493 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9912 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.3684 val_loss=1.9196 val_ROC-AUC=0.9935 val_AP=0.8877 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9935 saved checkpoint\n",
      "Epoch 4/30 — train_loss=1.1745 val_loss=1.5202 val_ROC-AUC=0.9863 val_AP=0.8597 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 5/30 — train_loss=0.9469 val_loss=1.2365 val_ROC-AUC=0.9605 val_AP=0.8335 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 6/30 — train_loss=0.6829 val_loss=0.9065 val_ROC-AUC=0.9289 val_AP=0.8267 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 7/30 — train_loss=0.4933 val_loss=0.6844 val_ROC-AUC=0.9111 val_AP=0.8249 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 8/30 — train_loss=0.3829 val_loss=0.6228 val_ROC-AUC=0.9102 val_AP=0.8248 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 9/30 — train_loss=0.3062 val_loss=0.5681 val_ROC-AUC=0.9096 val_AP=0.8248 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 10/30 — train_loss=0.2566 val_loss=0.5423 val_ROC-AUC=0.9085 val_AP=0.8247 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 11/30 — train_loss=0.2198 val_loss=0.5321 val_ROC-AUC=0.9069 val_AP=0.8246 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 11 (best epoch 3 val_ROC-AUC=0.9935064935064934)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9935064935064934\n",
      "=== reg_do0.3_wd0.0001_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9935064935064934\n",
      "Test ROC-AUC: 0.5724248927038627\n",
      "Test AP: 0.2714260762359608\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.8391 val_loss=1.9869 val_ROC-AUC=0.9737 val_AP=0.8498 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9737 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5429 val_loss=1.8789 val_ROC-AUC=0.9506 val_AP=0.8474 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=0.9463 val_loss=1.0836 val_ROC-AUC=0.8837 val_AP=0.8317 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.4634 val_loss=0.6396 val_ROC-AUC=0.8703 val_AP=0.8226 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.1770 val_loss=0.6749 val_ROC-AUC=0.8667 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.0965 val_loss=0.7314 val_ROC-AUC=0.8671 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.0706 val_loss=0.8069 val_ROC-AUC=0.8626 val_AP=0.8223 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.0518 val_loss=0.8660 val_ROC-AUC=0.8617 val_AP=0.8130 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.0452 val_loss=0.9192 val_ROC-AUC=0.8604 val_AP=0.8130 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9736652236652237)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9736652236652237\n",
      "=== reg_do0.3_wd0.0001_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9736652236652237\n",
      "Test ROC-AUC: 0.5126072961373391\n",
      "Test AP: 0.263796027238291\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.9657 val_loss=1.5841 val_ROC-AUC=0.9868 val_AP=0.7303 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9868 saved checkpoint\n",
      "Epoch 2/30 — train_loss=2.2001 val_loss=1.9740 val_ROC-AUC=0.9894 val_AP=0.8808 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9894 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.6241 val_loss=2.0176 val_ROC-AUC=0.9881 val_AP=0.8767 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.3486 val_loss=1.8644 val_ROC-AUC=0.9821 val_AP=0.8647 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=1.1852 val_loss=1.6593 val_ROC-AUC=0.9780 val_AP=0.8598 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=1.0332 val_loss=1.4473 val_ROC-AUC=0.9693 val_AP=0.8532 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.8903 val_loss=1.3087 val_ROC-AUC=0.9634 val_AP=0.8503 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.8200 val_loss=1.1844 val_ROC-AUC=0.9578 val_AP=0.8482 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.7104 val_loss=1.0653 val_ROC-AUC=0.9533 val_AP=0.8469 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.6701 val_loss=0.9478 val_ROC-AUC=0.9459 val_AP=0.8377 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9893578643578643)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9893578643578643\n",
      "=== reg_do0.5_wd0.0_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9893578643578643\n",
      "Test ROC-AUC: 0.5962982832618027\n",
      "Test AP: 0.23448393654075952\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.9621 val_loss=1.7836 val_ROC-AUC=0.9401 val_AP=0.4070 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9401 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.8948 val_loss=2.3133 val_ROC-AUC=0.9643 val_AP=0.8537 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9643 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.3224 val_loss=1.6952 val_ROC-AUC=0.9259 val_AP=0.8364 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.9286 val_loss=1.0851 val_ROC-AUC=0.8905 val_AP=0.8237 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5794 val_loss=0.7319 val_ROC-AUC=0.8820 val_AP=0.8315 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.3294 val_loss=0.6240 val_ROC-AUC=0.8622 val_AP=0.8223 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.2037 val_loss=0.6387 val_ROC-AUC=0.8660 val_AP=0.8224 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1363 val_loss=0.6994 val_ROC-AUC=0.8609 val_AP=0.8223 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1021 val_loss=0.7322 val_ROC-AUC=0.8629 val_AP=0.8223 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0699 val_loss=0.7729 val_ROC-AUC=0.8653 val_AP=0.8224 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9642857142857143)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9642857142857143\n",
      "=== reg_do0.5_wd0.0_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9642857142857143\n",
      "Test ROC-AUC: 0.5812768240343348\n",
      "Test AP: 0.26869834275146576\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.4397 val_loss=1.6474 val_ROC-AUC=0.9838 val_AP=0.5690 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9838 saved checkpoint\n",
      "Epoch 2/30 — train_loss=2.1887 val_loss=2.0674 val_ROC-AUC=0.9913 val_AP=0.8883 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9913 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.5838 val_loss=2.0506 val_ROC-AUC=0.9903 val_AP=0.8892 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.3753 val_loss=1.8609 val_ROC-AUC=0.9872 val_AP=0.8803 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=1.1861 val_loss=1.6263 val_ROC-AUC=0.9807 val_AP=0.8678 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=1.0014 val_loss=1.3120 val_ROC-AUC=0.9639 val_AP=0.8523 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.8572 val_loss=1.1800 val_ROC-AUC=0.9585 val_AP=0.8424 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.7467 val_loss=1.0796 val_ROC-AUC=0.9549 val_AP=0.8414 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.6705 val_loss=0.9719 val_ROC-AUC=0.9434 val_AP=0.8298 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.5907 val_loss=0.8584 val_ROC-AUC=0.9306 val_AP=0.8272 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9913419913419913)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9913419913419913\n",
      "=== reg_do0.5_wd1e-05_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9913419913419913\n",
      "Test ROC-AUC: 0.6024678111587982\n",
      "Test AP: 0.3564793891337542\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.8010 val_loss=1.9780 val_ROC-AUC=0.9722 val_AP=0.6088 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9722 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.6420 val_loss=2.1236 val_ROC-AUC=0.9803 val_AP=0.8676 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9803 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.2337 val_loss=1.5027 val_ROC-AUC=0.9637 val_AP=0.8476 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.8550 val_loss=0.8175 val_ROC-AUC=0.9383 val_AP=0.8380 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.4549 val_loss=0.5453 val_ROC-AUC=0.9228 val_AP=0.8351 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.2236 val_loss=0.5316 val_ROC-AUC=0.9028 val_AP=0.8246 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.1449 val_loss=0.5592 val_ROC-AUC=0.9028 val_AP=0.8246 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1064 val_loss=0.6178 val_ROC-AUC=0.8959 val_AP=0.8240 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.0745 val_loss=0.6648 val_ROC-AUC=0.8945 val_AP=0.8239 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0684 val_loss=0.6983 val_ROC-AUC=0.8947 val_AP=0.8239 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9803391053391053)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9803391053391053\n",
      "=== reg_do0.5_wd1e-05_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9803391053391053\n",
      "Test ROC-AUC: 0.5592811158798283\n",
      "Test AP: 0.2665057035063463\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.7266 val_loss=1.5990 val_ROC-AUC=0.9897 val_AP=0.6866 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9897 saved checkpoint\n",
      "Epoch 2/30 — train_loss=2.1893 val_loss=2.0607 val_ROC-AUC=0.9955 val_AP=0.9058 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9955 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.6882 val_loss=2.0924 val_ROC-AUC=0.9949 val_AP=0.9167 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.3553 val_loss=1.9032 val_ROC-AUC=0.9868 val_AP=0.8755 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=1.2135 val_loss=1.6897 val_ROC-AUC=0.9776 val_AP=0.8616 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.9918 val_loss=1.3864 val_ROC-AUC=0.9533 val_AP=0.8451 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.9215 val_loss=1.2374 val_ROC-AUC=0.9320 val_AP=0.8379 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.7803 val_loss=1.1251 val_ROC-AUC=0.9224 val_AP=0.8360 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.7229 val_loss=1.0290 val_ROC-AUC=0.9161 val_AP=0.8266 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.6230 val_loss=0.9237 val_ROC-AUC=0.9098 val_AP=0.8258 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9954906204906205)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9954906204906205\n",
      "=== reg_do0.5_wd0.0001_gcNone finished ===\n",
      "Val best ROC-AUC: 0.9954906204906205\n",
      "Test ROC-AUC: 0.6110515021459227\n",
      "Test AP: 0.3167129041639556\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.5577 val_loss=1.8475 val_ROC-AUC=0.9912 val_AP=0.8109 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9912 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.7841 val_loss=2.4104 val_ROC-AUC=0.9951 val_AP=0.9107 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9951 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.3338 val_loss=1.6551 val_ROC-AUC=0.9850 val_AP=0.8696 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.9042 val_loss=1.0751 val_ROC-AUC=0.9491 val_AP=0.8301 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5694 val_loss=0.6481 val_ROC-AUC=0.9089 val_AP=0.8247 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.2959 val_loss=0.5690 val_ROC-AUC=0.8833 val_AP=0.8232 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.1739 val_loss=0.6096 val_ROC-AUC=0.8759 val_AP=0.8228 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1278 val_loss=0.6633 val_ROC-AUC=0.8719 val_AP=0.8227 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1054 val_loss=0.7433 val_ROC-AUC=0.8667 val_AP=0.8225 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0786 val_loss=0.7766 val_ROC-AUC=0.8683 val_AP=0.8225 time=0.3s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9951298701298702)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9951298701298702\n",
      "=== reg_do0.5_wd0.0001_gc1.0 finished ===\n",
      "Val best ROC-AUC: 0.9951298701298702\n",
      "Test ROC-AUC: 0.5466738197424892\n",
      "Test AP: 0.26571744722072027\n",
      "\n",
      "Regularization Results:\n",
      "{'dropout': 0.0, 'weight_decay': 0.0, 'grad_clip': None, 'val_ROC-AUC': 0.9908008658008659, 'test_ROC-AUC': 0.4991952789699571, 'test_AP': 0.26331173307522326}\n",
      "{'dropout': 0.0, 'weight_decay': 0.0, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9931457431457431, 'test_ROC-AUC': 0.5128755364806866, 'test_AP': 0.2640380449813158}\n",
      "{'dropout': 0.0, 'weight_decay': 1e-05, 'grad_clip': None, 'val_ROC-AUC': 0.9962121212121211, 'test_ROC-AUC': 0.49007510729613735, 'test_AP': 0.2628616273110003}\n",
      "{'dropout': 0.0, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9884559884559885, 'test_ROC-AUC': 0.5217274678111588, 'test_AP': 0.2642051306270694}\n",
      "{'dropout': 0.0, 'weight_decay': 0.0001, 'grad_clip': None, 'val_ROC-AUC': 0.9953102453102454, 'test_ROC-AUC': 0.5108637339055795, 'test_AP': 0.22209038719436236}\n",
      "{'dropout': 0.0, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9604978354978355, 'test_ROC-AUC': 0.5187768240343348, 'test_AP': 0.2640128013513611}\n",
      "{'dropout': 0.1, 'weight_decay': 0.0, 'grad_clip': None, 'val_ROC-AUC': 0.9594155844155844, 'test_ROC-AUC': 0.5284334763948498, 'test_AP': 0.2649749943518473}\n",
      "{'dropout': 0.1, 'weight_decay': 0.0, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9965728715728716, 'test_ROC-AUC': 0.5160944206008584, 'test_AP': 0.2636873662967669}\n",
      "{'dropout': 0.1, 'weight_decay': 1e-05, 'grad_clip': None, 'val_ROC-AUC': 0.9642857142857142, 'test_ROC-AUC': 0.5131437768240343, 'test_AP': 0.1891915172994639}\n",
      "{'dropout': 0.1, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'val_ROC-AUC': 0.954004329004329, 'test_ROC-AUC': 0.5386266094420601, 'test_AP': 0.20233593999911106}\n",
      "{'dropout': 0.1, 'weight_decay': 0.0001, 'grad_clip': None, 'val_ROC-AUC': 0.9917027417027416, 'test_ROC-AUC': 0.4916845493562232, 'test_AP': 0.26279395547701473}\n",
      "{'dropout': 0.1, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9882756132756133, 'test_ROC-AUC': 0.5254828326180258, 'test_AP': 0.2646392285084977}\n",
      "{'dropout': 0.3, 'weight_decay': 0.0, 'grad_clip': None, 'val_ROC-AUC': 0.9716810966810967, 'test_ROC-AUC': 0.5421137339055795, 'test_AP': 0.26616062440605753}\n",
      "{'dropout': 0.3, 'weight_decay': 0.0, 'grad_clip': 1.0, 'val_ROC-AUC': 0.975108225108225, 'test_ROC-AUC': 0.5160944206008584, 'test_AP': 0.26382973397224335}\n",
      "{'dropout': 0.3, 'weight_decay': 1e-05, 'grad_clip': None, 'val_ROC-AUC': 0.9976551226551226, 'test_ROC-AUC': 0.5244098712446352, 'test_AP': 0.2640449351083859}\n",
      "{'dropout': 0.3, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9953102453102454, 'test_ROC-AUC': 0.48873390557939916, 'test_AP': 0.151307235526834}\n",
      "{'dropout': 0.3, 'weight_decay': 0.0001, 'grad_clip': None, 'val_ROC-AUC': 0.9935064935064934, 'test_ROC-AUC': 0.5724248927038627, 'test_AP': 0.2714260762359608}\n",
      "{'dropout': 0.3, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9736652236652237, 'test_ROC-AUC': 0.5126072961373391, 'test_AP': 0.263796027238291}\n",
      "{'dropout': 0.5, 'weight_decay': 0.0, 'grad_clip': None, 'val_ROC-AUC': 0.9893578643578643, 'test_ROC-AUC': 0.5962982832618027, 'test_AP': 0.23448393654075952}\n",
      "{'dropout': 0.5, 'weight_decay': 0.0, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9642857142857143, 'test_ROC-AUC': 0.5812768240343348, 'test_AP': 0.26869834275146576}\n",
      "{'dropout': 0.5, 'weight_decay': 1e-05, 'grad_clip': None, 'val_ROC-AUC': 0.9913419913419913, 'test_ROC-AUC': 0.6024678111587982, 'test_AP': 0.3564793891337542}\n",
      "{'dropout': 0.5, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9803391053391053, 'test_ROC-AUC': 0.5592811158798283, 'test_AP': 0.2665057035063463}\n",
      "{'dropout': 0.5, 'weight_decay': 0.0001, 'grad_clip': None, 'val_ROC-AUC': 0.9954906204906205, 'test_ROC-AUC': 0.6110515021459227, 'test_AP': 0.3167129041639556}\n",
      "{'dropout': 0.5, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'val_ROC-AUC': 0.9951298701298702, 'test_ROC-AUC': 0.5466738197424892, 'test_AP': 0.26571744722072027}\n"
     ]
    }
   ],
   "source": [
    "# === 8.2.2 Regularization Experiment ===\n",
    "dropouts = [0.0, 0.1, 0.3, 0.5]\n",
    "decays   = [0.0, 1e-5, 1e-4]\n",
    "gradclips = [None, 1.0]\n",
    "\n",
    "reg_results = []\n",
    "\n",
    "for do in dropouts:\n",
    "    for wd in decays:\n",
    "        for gc in gradclips:\n",
    "            name = f\"reg_do{do}_wd{wd}_gc{gc}\"\n",
    "            res = run_mlp_experiment(name=name, dropout=do, weight_decay=wd, grad_clip=gc)\n",
    "            reg_results.append({\n",
    "                \"dropout\": do,\n",
    "                \"weight_decay\": wd,\n",
    "                \"grad_clip\": gc,\n",
    "                \"val_ROC-AUC\": res[\"val_best\"][\"ROC-AUC\"],\n",
    "                \"test_ROC-AUC\": res[\"test\"][\"ROC-AUC\"],\n",
    "                \"test_AP\": res[\"test\"][\"AP\"]\n",
    "            })\n",
    "\n",
    "print(\"\\nRegularization Results:\")\n",
    "for r in reg_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a2a6b39-8b6d-4bad-b606-3c7b7648a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=10.0655 val_loss=1.0535 val_ROC-AUC=0.9264 val_AP=0.4831 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9264 saved checkpoint\n",
      "Epoch 2/30 — train_loss=7.3925 val_loss=1.0115 val_ROC-AUC=0.9751 val_AP=0.7761 time=0.0s\n",
      "  -> new best val_ROC-AUC=0.9751 saved checkpoint\n",
      "Epoch 3/30 — train_loss=5.2449 val_loss=0.9780 val_ROC-AUC=0.9809 val_AP=0.8671 time=0.0s\n",
      "  -> new best val_ROC-AUC=0.9809 saved checkpoint\n",
      "Epoch 4/30 — train_loss=4.0991 val_loss=0.9552 val_ROC-AUC=0.9848 val_AP=0.8746 time=0.0s\n",
      "  -> new best val_ROC-AUC=0.9848 saved checkpoint\n",
      "Epoch 5/30 — train_loss=2.9904 val_loss=0.9332 val_ROC-AUC=0.9841 val_AP=0.8739 time=0.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 6/30 — train_loss=2.4886 val_loss=0.8983 val_ROC-AUC=0.9859 val_AP=0.8758 time=0.0s\n",
      "  -> new best val_ROC-AUC=0.9859 saved checkpoint\n",
      "Epoch 7/30 — train_loss=2.0583 val_loss=0.8595 val_ROC-AUC=0.9816 val_AP=0.8700 time=0.0s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 8/30 — train_loss=1.6092 val_loss=0.8050 val_ROC-AUC=0.9791 val_AP=0.8679 time=0.0s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 9/30 — train_loss=1.4138 val_loss=0.7552 val_ROC-AUC=0.9738 val_AP=0.8591 time=0.0s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 10/30 — train_loss=1.1913 val_loss=0.7036 val_ROC-AUC=0.9661 val_AP=0.8529 time=0.0s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 11/30 — train_loss=1.1156 val_loss=0.6832 val_ROC-AUC=0.9601 val_AP=0.8499 time=0.0s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 12/30 — train_loss=1.0216 val_loss=0.6644 val_ROC-AUC=0.9565 val_AP=0.8485 time=0.0s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 13/30 — train_loss=1.0109 val_loss=0.6474 val_ROC-AUC=0.9524 val_AP=0.8473 time=0.0s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 14/30 — train_loss=0.8969 val_loss=0.6334 val_ROC-AUC=0.9464 val_AP=0.8457 time=0.0s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 14 (best epoch 6 val_ROC-AUC=0.985930735930736)\n",
      "Training finished. Best epoch: 6 best_metric: 0.985930735930736\n",
      "=== depth_(64,) finished ===\n",
      "Val best ROC-AUC: 0.985930735930736\n",
      "Test ROC-AUC: 0.5171673819742488\n",
      "Test AP: 0.2641427936010647\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=7.6325 val_loss=1.1029 val_ROC-AUC=0.9802 val_AP=0.6190 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9802 saved checkpoint\n",
      "Epoch 2/30 — train_loss=4.1243 val_loss=1.1214 val_ROC-AUC=0.9868 val_AP=0.8776 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9868 saved checkpoint\n",
      "Epoch 3/30 — train_loss=2.6013 val_loss=1.1217 val_ROC-AUC=0.9841 val_AP=0.8727 time=0.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=1.9180 val_loss=1.0888 val_ROC-AUC=0.9854 val_AP=0.8740 time=0.1s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=1.5104 val_loss=1.0241 val_ROC-AUC=0.9861 val_AP=0.8729 time=0.1s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=1.2941 val_loss=0.9447 val_ROC-AUC=0.9848 val_AP=0.8695 time=0.1s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=1.1172 val_loss=0.9117 val_ROC-AUC=0.9839 val_AP=0.8676 time=0.1s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=1.0195 val_loss=0.8698 val_ROC-AUC=0.9818 val_AP=0.8642 time=0.1s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.9790 val_loss=0.8359 val_ROC-AUC=0.9793 val_AP=0.8611 time=0.1s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.8738 val_loss=0.7940 val_ROC-AUC=0.9753 val_AP=0.8573 time=0.1s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9868326118326118)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9868326118326118\n",
      "=== depth_(128,) finished ===\n",
      "Val best ROC-AUC: 0.9868326118326118\n",
      "Test ROC-AUC: 0.6158798283261802\n",
      "Test AP: 0.22057352058549495\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.8587 val_loss=1.1816 val_ROC-AUC=0.9951 val_AP=0.8822 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9951 saved checkpoint\n",
      "Epoch 2/30 — train_loss=2.6974 val_loss=1.2508 val_ROC-AUC=0.9959 val_AP=0.9235 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9959 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.8140 val_loss=1.2206 val_ROC-AUC=0.9962 val_AP=0.9257 time=0.1s\n",
      "  -> new best val_ROC-AUC=0.9962 saved checkpoint\n",
      "Epoch 4/30 — train_loss=1.3663 val_loss=1.1235 val_ROC-AUC=0.9933 val_AP=0.9011 time=0.1s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 5/30 — train_loss=1.1034 val_loss=0.9901 val_ROC-AUC=0.9867 val_AP=0.8809 time=0.1s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 6/30 — train_loss=0.9179 val_loss=0.8755 val_ROC-AUC=0.9744 val_AP=0.8562 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 7/30 — train_loss=0.7959 val_loss=0.7690 val_ROC-AUC=0.9637 val_AP=0.8462 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 8/30 — train_loss=0.6600 val_loss=0.7261 val_ROC-AUC=0.9612 val_AP=0.8444 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 9/30 — train_loss=0.6203 val_loss=0.6827 val_ROC-AUC=0.9509 val_AP=0.8402 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 10/30 — train_loss=0.5662 val_loss=0.6463 val_ROC-AUC=0.9412 val_AP=0.8377 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 11/30 — train_loss=0.5154 val_loss=0.6179 val_ROC-AUC=0.9372 val_AP=0.8367 time=0.7s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 11 (best epoch 3 val_ROC-AUC=0.9962121212121213)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9962121212121213\n",
      "=== depth_(256,) finished ===\n",
      "Val best ROC-AUC: 0.9962121212121213\n",
      "Test ROC-AUC: 0.5552575107296137\n",
      "Test AP: 0.2654575714768298\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.3582 val_loss=1.8015 val_ROC-AUC=0.9592 val_AP=0.8324 time=0.8s\n",
      "  -> new best val_ROC-AUC=0.9592 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.4834 val_loss=1.8814 val_ROC-AUC=0.9569 val_AP=0.8523 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.1393 val_loss=1.4714 val_ROC-AUC=0.9242 val_AP=0.8434 time=0.3s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.8294 val_loss=1.0316 val_ROC-AUC=0.9001 val_AP=0.8404 time=0.3s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.5610 val_loss=0.7577 val_ROC-AUC=0.8909 val_AP=0.8320 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.3982 val_loss=0.6575 val_ROC-AUC=0.8887 val_AP=0.8318 time=0.4s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.3056 val_loss=0.6037 val_ROC-AUC=0.8819 val_AP=0.8315 time=0.3s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.2479 val_loss=0.5840 val_ROC-AUC=0.8806 val_AP=0.8231 time=0.3s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.1983 val_loss=0.5836 val_ROC-AUC=0.8802 val_AP=0.8231 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9592352092352093)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9592352092352093\n",
      "=== depth_(512, 256) finished ===\n",
      "Val best ROC-AUC: 0.9592352092352093\n",
      "Test ROC-AUC: 0.5257510729613735\n",
      "Test AP: 0.26399350185491616\n",
      "\n",
      "Model Depth/Width Results:\n",
      "{'hidden_sizes': (64,), 'val_ROC-AUC': 0.985930735930736, 'test_ROC-AUC': 0.5171673819742488, 'test_AP': 0.2641427936010647}\n",
      "{'hidden_sizes': (128,), 'val_ROC-AUC': 0.9868326118326118, 'test_ROC-AUC': 0.6158798283261802, 'test_AP': 0.22057352058549495}\n",
      "{'hidden_sizes': (256,), 'val_ROC-AUC': 0.9962121212121213, 'test_ROC-AUC': 0.5552575107296137, 'test_AP': 0.2654575714768298}\n",
      "{'hidden_sizes': (512, 256), 'val_ROC-AUC': 0.9592352092352093, 'test_ROC-AUC': 0.5257510729613735, 'test_AP': 0.26399350185491616}\n"
     ]
    }
   ],
   "source": [
    "hidden_variants = [\n",
    "    (64,),\n",
    "    (128,),\n",
    "    (256,),\n",
    "    (512, 256),\n",
    "]\n",
    "depth_results = []\n",
    "\n",
    "for hs in hidden_variants:\n",
    "    res = run_mlp_experiment(name=f\"depth_{hs}\", hidden_sizes=hs)\n",
    "    depth_results.append({\n",
    "        \"hidden_sizes\": hs,\n",
    "        \"val_ROC-AUC\": res[\"val_best\"][\"ROC-AUC\"],\n",
    "        \"test_ROC-AUC\": res[\"test\"][\"ROC-AUC\"],\n",
    "        \"test_AP\": res[\"test\"][\"AP\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nModel Depth/Width Results:\")\n",
    "for r in depth_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4dd1681-ecf1-4fc3-b155-771c2b1eac16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.1442 val_loss=1.8599 val_ROC-AUC=0.9785 val_AP=0.8621 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9785 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5848 val_loss=1.9293 val_ROC-AUC=0.9740 val_AP=0.8575 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.1750 val_loss=1.5969 val_ROC-AUC=0.9618 val_AP=0.8500 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.8616 val_loss=1.2163 val_ROC-AUC=0.9340 val_AP=0.8433 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.6149 val_loss=0.8345 val_ROC-AUC=0.8912 val_AP=0.8319 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.4273 val_loss=0.7194 val_ROC-AUC=0.8808 val_AP=0.8314 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.3518 val_loss=0.6482 val_ROC-AUC=0.8730 val_AP=0.8227 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.2656 val_loss=0.6215 val_ROC-AUC=0.8707 val_AP=0.8226 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.2160 val_loss=0.6133 val_ROC-AUC=0.8680 val_AP=0.8225 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9785353535353535)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9785353535353535\n",
      "=== lr_plateau finished ===\n",
      "Val best ROC-AUC: 0.9785353535353535\n",
      "Test ROC-AUC: 0.529774678111588\n",
      "Test AP: 0.22255815779208446\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=5.7907 val_loss=1.5544 val_ROC-AUC=0.9702 val_AP=0.7852 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9702 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.7890 val_loss=1.7964 val_ROC-AUC=0.9776 val_AP=0.8620 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9776 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.2382 val_loss=1.5769 val_ROC-AUC=0.9517 val_AP=0.8477 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.9000 val_loss=1.1892 val_ROC-AUC=0.9165 val_AP=0.8414 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.6240 val_loss=0.8299 val_ROC-AUC=0.8961 val_AP=0.8147 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.4599 val_loss=0.6610 val_ROC-AUC=0.8838 val_AP=0.8140 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.2749 val_loss=0.6136 val_ROC-AUC=0.8786 val_AP=0.8137 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.2046 val_loss=0.6235 val_ROC-AUC=0.8759 val_AP=0.8136 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1509 val_loss=0.6559 val_ROC-AUC=0.8732 val_AP=0.8135 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.1162 val_loss=0.6907 val_ROC-AUC=0.8721 val_AP=0.8227 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9776334776334776)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9776334776334776\n",
      "=== lr_cosine finished ===\n",
      "Val best ROC-AUC: 0.9776334776334776\n",
      "Test ROC-AUC: 0.5287017167381974\n",
      "Test AP: 0.2230763384340871\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=4.0592 val_loss=1.8039 val_ROC-AUC=0.9782 val_AP=0.8546 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9782 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5404 val_loss=1.8590 val_ROC-AUC=0.9785 val_AP=0.8608 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9785 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.1067 val_loss=1.3863 val_ROC-AUC=0.9554 val_AP=0.8478 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.7732 val_loss=0.9739 val_ROC-AUC=0.9084 val_AP=0.8330 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.5060 val_loss=0.6877 val_ROC-AUC=0.8891 val_AP=0.8318 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.3176 val_loss=0.5910 val_ROC-AUC=0.8835 val_AP=0.8315 time=0.3s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.1976 val_loss=0.5790 val_ROC-AUC=0.8790 val_AP=0.8313 time=0.3s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1532 val_loss=0.6017 val_ROC-AUC=0.8759 val_AP=0.8228 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.1024 val_loss=0.6394 val_ROC-AUC=0.8739 val_AP=0.8135 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0823 val_loss=0.6921 val_ROC-AUC=0.8694 val_AP=0.8133 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9785353535353535)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9785353535353535\n",
      "=== lr_step finished ===\n",
      "Val best ROC-AUC: 0.9785353535353535\n",
      "Test ROC-AUC: 0.514216738197425\n",
      "Test AP: 0.22203880690418967\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=3.6510 val_loss=2.0411 val_ROC-AUC=0.9823 val_AP=0.7775 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9823 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5125 val_loss=1.7813 val_ROC-AUC=0.9888 val_AP=0.8803 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9888 saved checkpoint\n",
      "Epoch 3/30 — train_loss=1.0690 val_loss=1.3533 val_ROC-AUC=0.9832 val_AP=0.8687 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.7378 val_loss=0.9305 val_ROC-AUC=0.9639 val_AP=0.8456 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.4764 val_loss=0.6153 val_ROC-AUC=0.9316 val_AP=0.8358 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.2808 val_loss=0.5181 val_ROC-AUC=0.9154 val_AP=0.8337 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.1855 val_loss=0.5070 val_ROC-AUC=0.9132 val_AP=0.8252 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.1314 val_loss=0.5589 val_ROC-AUC=0.9048 val_AP=0.8246 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.0887 val_loss=0.6110 val_ROC-AUC=0.9003 val_AP=0.8243 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0712 val_loss=0.6537 val_ROC-AUC=0.9001 val_AP=0.8242 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9888167388167388)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9888167388167388\n",
      "=== lr_none finished ===\n",
      "Val best ROC-AUC: 0.9888167388167388\n",
      "Test ROC-AUC: 0.5136802575107297\n",
      "Test AP: 0.26382404660055814\n",
      "\n",
      "LR Schedule Results:\n",
      "{'scheduler': 'plateau', 'val_ROC-AUC': 0.9785353535353535, 'test_ROC-AUC': 0.529774678111588, 'test_AP': 0.22255815779208446}\n",
      "{'scheduler': 'cosine', 'val_ROC-AUC': 0.9776334776334776, 'test_ROC-AUC': 0.5287017167381974, 'test_AP': 0.2230763384340871}\n",
      "{'scheduler': 'step', 'val_ROC-AUC': 0.9785353535353535, 'test_ROC-AUC': 0.514216738197425, 'test_AP': 0.22203880690418967}\n",
      "{'scheduler': 'none', 'val_ROC-AUC': 0.9888167388167388, 'test_ROC-AUC': 0.5136802575107297, 'test_AP': 0.26382404660055814}\n"
     ]
    }
   ],
   "source": [
    "schedulers = [\"plateau\", \"cosine\", \"step\", \"none\"]\n",
    "lr_results = []\n",
    "\n",
    "for sched in schedulers:\n",
    "    res = run_mlp_experiment(name=f\"lr_{sched}\", scheduler_type=sched)\n",
    "    lr_results.append({\n",
    "        \"scheduler\": sched,\n",
    "        \"val_ROC-AUC\": res[\"val_best\"][\"ROC-AUC\"],\n",
    "        \"test_ROC-AUC\": res[\"test\"][\"ROC-AUC\"],\n",
    "        \"test_AP\": res[\"test\"][\"AP\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nLR Schedule Results:\")\n",
    "for r in lr_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21c55966-1931-4024-8ff7-2fd3e720b7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=1.2733 val_loss=0.9797 val_ROC-AUC=0.8689 val_AP=0.6706 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.8689 saved checkpoint\n",
      "Epoch 2/30 — train_loss=0.9506 val_loss=0.7805 val_ROC-AUC=0.9093 val_AP=0.7825 time=0.3s\n",
      "  -> new best val_ROC-AUC=0.9093 saved checkpoint\n",
      "Epoch 3/30 — train_loss=0.6856 val_loss=0.6533 val_ROC-AUC=0.8959 val_AP=0.7816 time=0.3s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.4827 val_loss=0.5460 val_ROC-AUC=0.9113 val_AP=0.8249 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9113 saved checkpoint\n",
      "Epoch 5/30 — train_loss=0.3309 val_loss=0.5353 val_ROC-AUC=0.9071 val_AP=0.8153 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 6/30 — train_loss=0.2300 val_loss=0.5753 val_ROC-AUC=0.9012 val_AP=0.8149 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 7/30 — train_loss=0.1708 val_loss=0.6276 val_ROC-AUC=0.8983 val_AP=0.8147 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 8/30 — train_loss=0.1207 val_loss=0.6834 val_ROC-AUC=0.8966 val_AP=0.8146 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 9/30 — train_loss=0.0946 val_loss=0.7024 val_ROC-AUC=0.8981 val_AP=0.8147 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 10/30 — train_loss=0.0845 val_loss=0.7240 val_ROC-AUC=0.8981 val_AP=0.8147 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 11/30 — train_loss=0.0771 val_loss=0.7506 val_ROC-AUC=0.8979 val_AP=0.8147 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 12/30 — train_loss=0.0695 val_loss=0.7759 val_ROC-AUC=0.8966 val_AP=0.8146 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 12 (best epoch 4 val_ROC-AUC=0.9112554112554113)\n",
      "Training finished. Best epoch: 4 best_metric: 0.9112554112554113\n",
      "=== posweight finished ===\n",
      "Val best ROC-AUC: 0.9112554112554113\n",
      "Test ROC-AUC: 0.4919527896995708\n",
      "Test AP: 0.1513316477539165\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=0.5497 val_loss=0.4702 val_ROC-AUC=0.9201 val_AP=0.7894 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9201 saved checkpoint\n",
      "Epoch 2/30 — train_loss=0.3054 val_loss=0.2567 val_ROC-AUC=0.9311 val_AP=0.8309 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9311 saved checkpoint\n",
      "Epoch 3/30 — train_loss=0.1465 val_loss=0.1256 val_ROC-AUC=0.9255 val_AP=0.8279 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 4/30 — train_loss=0.0794 val_loss=0.0759 val_ROC-AUC=0.9120 val_AP=0.8166 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 5/30 — train_loss=0.0404 val_loss=0.0653 val_ROC-AUC=0.9104 val_AP=0.8165 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 6/30 — train_loss=0.0247 val_loss=0.0546 val_ROC-AUC=0.9035 val_AP=0.8157 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 7/30 — train_loss=0.0163 val_loss=0.0535 val_ROC-AUC=0.9062 val_AP=0.8160 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 8/30 — train_loss=0.0136 val_loss=0.0530 val_ROC-AUC=0.9066 val_AP=0.8160 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 9/30 — train_loss=0.0123 val_loss=0.0518 val_ROC-AUC=0.9040 val_AP=0.8157 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 10/30 — train_loss=0.0114 val_loss=0.0516 val_ROC-AUC=0.9040 val_AP=0.8157 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 10 (best epoch 2 val_ROC-AUC=0.9310966810966811)\n",
      "Training finished. Best epoch: 2 best_metric: 0.9310966810966811\n",
      "=== sampler finished ===\n",
      "Val best ROC-AUC: 0.9310966810966811\n",
      "Test ROC-AUC: 0.5340665236051502\n",
      "Test AP: 0.201939960232992\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/train_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/val_exp_fp.npz\n",
      "Loading cached fingerprints: runs_hw1/fp_baseline/test_exp_fp.npz\n",
      "Epoch 1/30 — train_loss=3.9360 val_loss=1.9656 val_ROC-AUC=0.9829 val_AP=0.6835 time=0.2s\n",
      "  -> new best val_ROC-AUC=0.9829 saved checkpoint\n",
      "Epoch 2/30 — train_loss=1.5330 val_loss=1.9380 val_ROC-AUC=0.9829 val_AP=0.8661 time=0.2s\n",
      "EarlyStopping: metric did not improve (1/8)\n",
      "Epoch 3/30 — train_loss=1.1386 val_loss=1.4128 val_ROC-AUC=0.9585 val_AP=0.8485 time=0.2s\n",
      "EarlyStopping: metric did not improve (2/8)\n",
      "Epoch 4/30 — train_loss=0.8014 val_loss=0.9540 val_ROC-AUC=0.9120 val_AP=0.8409 time=0.2s\n",
      "EarlyStopping: metric did not improve (3/8)\n",
      "Epoch 5/30 — train_loss=0.4836 val_loss=0.6696 val_ROC-AUC=0.8900 val_AP=0.8394 time=0.2s\n",
      "EarlyStopping: metric did not improve (4/8)\n",
      "Epoch 6/30 — train_loss=0.3094 val_loss=0.6067 val_ROC-AUC=0.8833 val_AP=0.8315 time=0.2s\n",
      "EarlyStopping: metric did not improve (5/8)\n",
      "Epoch 7/30 — train_loss=0.2414 val_loss=0.5813 val_ROC-AUC=0.8797 val_AP=0.8313 time=0.2s\n",
      "EarlyStopping: metric did not improve (6/8)\n",
      "Epoch 8/30 — train_loss=0.2043 val_loss=0.5766 val_ROC-AUC=0.8759 val_AP=0.8312 time=0.2s\n",
      "EarlyStopping: metric did not improve (7/8)\n",
      "Epoch 9/30 — train_loss=0.1592 val_loss=0.5947 val_ROC-AUC=0.8730 val_AP=0.8310 time=0.2s\n",
      "EarlyStopping: metric did not improve (8/8)\n",
      "Early stopping at epoch 9 (best epoch 1 val_ROC-AUC=0.9828643578643579)\n",
      "Training finished. Best epoch: 1 best_metric: 0.9828643578643579\n",
      "=== both finished ===\n",
      "Val best ROC-AUC: 0.9828643578643579\n",
      "Test ROC-AUC: 0.506706008583691\n",
      "Test AP: 0.2640506009750323\n",
      "\n",
      "Class Imbalance Results:\n",
      "{'config': 'posweight', 'val_ROC-AUC': 0.9112554112554113, 'test_ROC-AUC': 0.4919527896995708, 'test_AP': 0.1513316477539165}\n",
      "{'config': 'sampler', 'val_ROC-AUC': 0.9310966810966811, 'test_ROC-AUC': 0.5340665236051502, 'test_AP': 0.201939960232992}\n",
      "{'config': 'both', 'val_ROC-AUC': 0.9828643578643579, 'test_ROC-AUC': 0.506706008583691, 'test_AP': 0.2640506009750323}\n"
     ]
    }
   ],
   "source": [
    "imbalance_configs = [\n",
    "    {\"pos_weight\": True, \"sampler_type\": \"plain\",   \"name\": \"posweight\"},\n",
    "    {\"pos_weight\": False,\"sampler_type\": \"weighted\",\"name\": \"sampler\"},\n",
    "    {\"pos_weight\": True, \"sampler_type\": \"weighted\",\"name\": \"both\"},\n",
    "]\n",
    "\n",
    "imbalance_results = []\n",
    "\n",
    "for cfg_i in imbalance_configs:\n",
    "    res = run_mlp_experiment(name=cfg_i[\"name\"],\n",
    "                             pos_weight=cfg_i[\"pos_weight\"],\n",
    "                             sampler_type=cfg_i[\"sampler_type\"])\n",
    "    imbalance_results.append({\n",
    "        \"config\": cfg_i[\"name\"],\n",
    "        \"val_ROC-AUC\": res[\"val_best\"][\"ROC-AUC\"],\n",
    "        \"test_ROC-AUC\": res[\"test\"][\"ROC-AUC\"],\n",
    "        \"test_AP\": res[\"test\"][\"AP\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nClass Imbalance Results:\")\n",
    "for r in imbalance_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8969dc51-d74a-4a9b-82f3-7c09f9216c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence loaders ready: 178 60 60\n"
     ]
    }
   ],
   "source": [
    "batch_size = cfg.batch_size  \n",
    "train_loader_seq, val_loader_seq, test_loader_seq = get_sequence_loaders(\n",
    "    train_df, val_df, test_df, batch_size\n",
    ")\n",
    "print(\"Sequence loaders ready:\", len(train_loader_seq), len(val_loader_seq), len(test_loader_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cefbe38-f69d-4903-abff-a8247feaf580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training RNN with num_layers=1 ===\n",
      "Epoch 1/20 — train_loss=2.9288 val_loss=2.2309 val_ROC-AUC=0.4986 val_AP=0.0263 time=29.8s\n",
      "  -> new best val_ROC-AUC=0.4986 saved checkpoint\n",
      "Epoch 2/20 — train_loss=3.4729 val_loss=2.8186 val_ROC-AUC=0.5317 val_AP=0.0280 time=27.7s\n",
      "  -> new best val_ROC-AUC=0.5317 saved checkpoint\n",
      "Epoch 3/20 — train_loss=3.5048 val_loss=2.5929 val_ROC-AUC=0.5503 val_AP=0.0292 time=28.3s\n",
      "  -> new best val_ROC-AUC=0.5503 saved checkpoint\n",
      "Epoch 4/20 — train_loss=3.2802 val_loss=2.9068 val_ROC-AUC=0.5402 val_AP=0.0283 time=29.3s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/20 — train_loss=3.5956 val_loss=2.1097 val_ROC-AUC=0.5741 val_AP=0.0381 time=27.9s\n",
      "  -> new best val_ROC-AUC=0.5741 saved checkpoint\n",
      "Epoch 6/20 — train_loss=3.2550 val_loss=3.2856 val_ROC-AUC=0.5819 val_AP=0.0380 time=27.3s\n",
      "  -> new best val_ROC-AUC=0.5819 saved checkpoint\n",
      "Epoch 7/20 — train_loss=3.6956 val_loss=2.6207 val_ROC-AUC=0.6387 val_AP=0.0544 time=27.2s\n",
      "  -> new best val_ROC-AUC=0.6387 saved checkpoint\n",
      "Epoch 8/20 — train_loss=3.4627 val_loss=2.4252 val_ROC-AUC=0.6189 val_AP=0.0368 time=27.4s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 9/20 — train_loss=3.0926 val_loss=2.9954 val_ROC-AUC=0.6147 val_AP=0.0335 time=28.0s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 10/20 — train_loss=3.8369 val_loss=3.2444 val_ROC-AUC=0.6346 val_AP=0.0352 time=28.7s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 11/20 — train_loss=3.2537 val_loss=2.7683 val_ROC-AUC=0.6383 val_AP=0.0355 time=37.1s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 12/20 — train_loss=3.1837 val_loss=3.1560 val_ROC-AUC=0.6084 val_AP=0.0341 time=42.8s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 12 (best epoch 7 val_ROC-AUC=0.6387085137085137)\n",
      "Training finished. Best epoch: 7 best_metric: 0.6387085137085137\n",
      "\n",
      "=== Training RNN with num_layers=2 ===\n",
      "Epoch 1/20 — train_loss=3.7176 val_loss=2.7152 val_ROC-AUC=0.5204 val_AP=0.0311 time=82.5s\n",
      "  -> new best val_ROC-AUC=0.5204 saved checkpoint\n",
      "Epoch 2/20 — train_loss=3.8376 val_loss=2.8051 val_ROC-AUC=0.5514 val_AP=0.0287 time=85.9s\n",
      "  -> new best val_ROC-AUC=0.5514 saved checkpoint\n",
      "Epoch 3/20 — train_loss=3.4146 val_loss=2.7092 val_ROC-AUC=0.5788 val_AP=0.0424 time=84.9s\n",
      "  -> new best val_ROC-AUC=0.5788 saved checkpoint\n",
      "Epoch 4/20 — train_loss=3.5575 val_loss=2.7539 val_ROC-AUC=0.5624 val_AP=0.0293 time=84.4s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/20 — train_loss=3.5311 val_loss=2.7748 val_ROC-AUC=0.5801 val_AP=0.0306 time=85.3s\n",
      "  -> new best val_ROC-AUC=0.5801 saved checkpoint\n",
      "Epoch 6/20 — train_loss=3.5865 val_loss=2.9303 val_ROC-AUC=0.5786 val_AP=0.0304 time=88.0s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 7/20 — train_loss=3.3878 val_loss=3.2071 val_ROC-AUC=0.5844 val_AP=0.0305 time=91.3s\n",
      "  -> new best val_ROC-AUC=0.5844 saved checkpoint\n",
      "Epoch 8/20 — train_loss=3.4264 val_loss=2.6717 val_ROC-AUC=0.5970 val_AP=0.0328 time=72.2s\n",
      "  -> new best val_ROC-AUC=0.5970 saved checkpoint\n",
      "Epoch 9/20 — train_loss=3.2423 val_loss=2.1179 val_ROC-AUC=0.6470 val_AP=0.0360 time=80.5s\n",
      "  -> new best val_ROC-AUC=0.6470 saved checkpoint\n",
      "Epoch 10/20 — train_loss=3.0609 val_loss=2.7650 val_ROC-AUC=0.6319 val_AP=0.0360 time=73.0s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 11/20 — train_loss=3.0671 val_loss=2.9349 val_ROC-AUC=0.6524 val_AP=0.0377 time=76.0s\n",
      "  -> new best val_ROC-AUC=0.6524 saved checkpoint\n",
      "Epoch 12/20 — train_loss=2.8977 val_loss=3.0024 val_ROC-AUC=0.6382 val_AP=0.0378 time=75.6s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 13/20 — train_loss=2.6935 val_loss=3.4479 val_ROC-AUC=0.5777 val_AP=0.0325 time=69.6s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 14/20 — train_loss=2.9650 val_loss=2.8749 val_ROC-AUC=0.6576 val_AP=0.0427 time=61.9s\n",
      "  -> new best val_ROC-AUC=0.6576 saved checkpoint\n",
      "Epoch 15/20 — train_loss=2.7884 val_loss=3.0037 val_ROC-AUC=0.5940 val_AP=0.0460 time=57.9s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 16/20 — train_loss=2.8126 val_loss=2.3840 val_ROC-AUC=0.5972 val_AP=0.0391 time=72.3s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 17/20 — train_loss=2.6149 val_loss=3.2004 val_ROC-AUC=0.5976 val_AP=0.0513 time=65.3s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 18/20 — train_loss=2.5849 val_loss=3.2540 val_ROC-AUC=0.5821 val_AP=0.0759 time=62.4s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 19/20 — train_loss=2.4852 val_loss=3.0033 val_ROC-AUC=0.5135 val_AP=0.1060 time=54.9s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 19 (best epoch 14 val_ROC-AUC=0.6576479076479077)\n",
      "Training finished. Best epoch: 14 best_metric: 0.6576479076479077\n",
      "\n",
      "=== RNN Depth Results (Test set) ===\n",
      "1-layer: ROC-AUC=0.5719, AP=0.0846, Precision=0.000, Recall=0.000\n",
      "2-layer: ROC-AUC=0.6384, AP=0.0393, Precision=0.000, Recall=0.000\n"
     ]
    }
   ],
   "source": [
    "results_rnn_depth = {}\n",
    "for num_layers in [1, 2]:\n",
    "    print(f\"\\n=== Training RNN with num_layers={num_layers} ===\")\n",
    "    \n",
    "    model = RNNSmiles(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=128,\n",
    "        hidden_dim=256,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 20\n",
    "    cfg_local.patience = 5\n",
    "\n",
    "    history = run_training(\n",
    "        model, train_loader_seq, val_loader_seq,\n",
    "        optimizer, scheduler, loss_fn, cfg_local,\n",
    "        model_name=f\"rnn_depth_{num_layers}L\", device=device\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint and evaluate\n",
    "    ckpt_path = Path(cfg.out_dir) / f\"rnn_depth_{num_layers}L\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_seq:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb).view(-1)\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    results_rnn_depth[num_layers] = metrics\n",
    "\n",
    "# Summarize\n",
    "print(\"\\n=== RNN Depth Results (Test set) ===\")\n",
    "for layers, m in results_rnn_depth.items():\n",
    "    print(f\"{layers}-layer: ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, \"\n",
    "          f\"Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23468708-390c-4b8c-92a5-5caacfb7601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer (1 layers) ===\n",
      "Epoch 1/14 — train_loss=2.6777 val_loss=1.1693 val_ROC-AUC=0.3535 val_AP=0.0196 time=12.4s\n",
      "  -> new best val_ROC-AUC=0.3535 saved checkpoint\n",
      "Epoch 2/14 — train_loss=3.3586 val_loss=1.1654 val_ROC-AUC=0.3615 val_AP=0.0199 time=8.2s\n",
      "  -> new best val_ROC-AUC=0.3615 saved checkpoint\n",
      "Epoch 3/14 — train_loss=3.3263 val_loss=1.1677 val_ROC-AUC=0.3665 val_AP=0.0200 time=8.5s\n",
      "  -> new best val_ROC-AUC=0.3665 saved checkpoint\n",
      "Epoch 4/14 — train_loss=3.2113 val_loss=1.1747 val_ROC-AUC=0.3730 val_AP=0.0202 time=8.5s\n",
      "  -> new best val_ROC-AUC=0.3730 saved checkpoint\n",
      "Epoch 5/14 — train_loss=3.2536 val_loss=1.1678 val_ROC-AUC=0.3842 val_AP=0.0205 time=8.7s\n",
      "  -> new best val_ROC-AUC=0.3842 saved checkpoint\n",
      "Epoch 6/14 — train_loss=3.4119 val_loss=1.1698 val_ROC-AUC=0.3900 val_AP=0.0207 time=8.9s\n",
      "  -> new best val_ROC-AUC=0.3900 saved checkpoint\n",
      "Epoch 7/14 — train_loss=3.1225 val_loss=1.1716 val_ROC-AUC=0.3992 val_AP=0.0210 time=11.5s\n",
      "  -> new best val_ROC-AUC=0.3992 saved checkpoint\n",
      "Epoch 8/14 — train_loss=3.1943 val_loss=1.1544 val_ROC-AUC=0.4251 val_AP=0.0219 time=8.5s\n",
      "  -> new best val_ROC-AUC=0.4251 saved checkpoint\n",
      "Epoch 9/14 — train_loss=3.3640 val_loss=1.1511 val_ROC-AUC=0.4331 val_AP=0.0223 time=8.3s\n",
      "  -> new best val_ROC-AUC=0.4331 saved checkpoint\n",
      "Epoch 10/14 — train_loss=3.2035 val_loss=1.1515 val_ROC-AUC=0.4435 val_AP=0.0227 time=8.5s\n",
      "  -> new best val_ROC-AUC=0.4435 saved checkpoint\n",
      "Epoch 11/14 — train_loss=3.3980 val_loss=1.1540 val_ROC-AUC=0.4441 val_AP=0.0227 time=9.0s\n",
      "  -> new best val_ROC-AUC=0.4441 saved checkpoint\n",
      "Epoch 12/14 — train_loss=3.2315 val_loss=1.1749 val_ROC-AUC=0.4277 val_AP=0.0221 time=9.1s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 13/14 — train_loss=3.2947 val_loss=1.1408 val_ROC-AUC=0.4894 val_AP=0.0248 time=9.4s\n",
      "  -> new best val_ROC-AUC=0.4894 saved checkpoint\n",
      "Epoch 14/14 — train_loss=3.1765 val_loss=1.1407 val_ROC-AUC=0.4991 val_AP=0.0256 time=8.5s\n",
      "  -> new best val_ROC-AUC=0.4991 saved checkpoint\n",
      "Training finished. Best epoch: 14 best_metric: 0.4990981240981241\n",
      "Loaded best checkpoint (epoch 14, val ROC-AUC=0.4991)\n",
      "=== TEST METRICS (Transformer 1L) ===\n",
      "ROC-AUC: 0.4759\n",
      "AP: 0.1425\n",
      "Accuracy: 0.9831\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[466, 0], [8, 0]]\n",
      "\n",
      "=== Training Transformer (2 layers) ===\n",
      "Epoch 1/14 — train_loss=2.5804 val_loss=1.2222 val_ROC-AUC=0.3465 val_AP=0.0194 time=16.1s\n",
      "  -> new best val_ROC-AUC=0.3465 saved checkpoint\n",
      "Epoch 2/14 — train_loss=3.4921 val_loss=1.1968 val_ROC-AUC=0.3577 val_AP=0.0197 time=16.9s\n",
      "  -> new best val_ROC-AUC=0.3577 saved checkpoint\n",
      "Epoch 3/14 — train_loss=3.3916 val_loss=1.1822 val_ROC-AUC=0.3710 val_AP=0.0201 time=16.7s\n",
      "  -> new best val_ROC-AUC=0.3710 saved checkpoint\n",
      "Epoch 4/14 — train_loss=3.4105 val_loss=1.1808 val_ROC-AUC=0.3869 val_AP=0.0206 time=19.4s\n",
      "  -> new best val_ROC-AUC=0.3869 saved checkpoint\n",
      "Epoch 5/14 — train_loss=3.3630 val_loss=1.1341 val_ROC-AUC=0.5204 val_AP=0.0262 time=16.9s\n",
      "  -> new best val_ROC-AUC=0.5204 saved checkpoint\n",
      "Epoch 6/14 — train_loss=3.0868 val_loss=1.1821 val_ROC-AUC=0.4253 val_AP=0.0220 time=19.9s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 7/14 — train_loss=3.1046 val_loss=1.1685 val_ROC-AUC=0.4674 val_AP=0.0237 time=17.3s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 8/14 — train_loss=3.1591 val_loss=1.1678 val_ROC-AUC=0.4491 val_AP=0.0230 time=16.6s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 9/14 — train_loss=2.8316 val_loss=1.2152 val_ROC-AUC=0.4021 val_AP=0.0212 time=17.2s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 10/14 — train_loss=2.8460 val_loss=1.1938 val_ROC-AUC=0.4457 val_AP=0.0228 time=16.8s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 10 (best epoch 5 val_ROC-AUC=0.5203823953823954)\n",
      "Training finished. Best epoch: 5 best_metric: 0.5203823953823954\n",
      "Loaded best checkpoint (epoch 5, val ROC-AUC=0.5204)\n",
      "=== TEST METRICS (Transformer 2L) ===\n",
      "ROC-AUC: 0.4804\n",
      "AP: 0.0217\n",
      "Accuracy: 0.9831\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[466, 0], [8, 0]]\n",
      "\n",
      "=== Training Transformer (3 layers) ===\n",
      "Epoch 1/14 — train_loss=3.1327 val_loss=1.2257 val_ROC-AUC=0.3400 val_AP=0.0192 time=23.2s\n",
      "  -> new best val_ROC-AUC=0.3400 saved checkpoint\n",
      "Epoch 2/14 — train_loss=3.4539 val_loss=1.2341 val_ROC-AUC=0.3364 val_AP=0.0191 time=24.2s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 3/14 — train_loss=3.4281 val_loss=1.2176 val_ROC-AUC=0.3543 val_AP=0.0196 time=24.2s\n",
      "  -> new best val_ROC-AUC=0.3543 saved checkpoint\n",
      "Epoch 4/14 — train_loss=3.5286 val_loss=1.2199 val_ROC-AUC=0.3611 val_AP=0.0198 time=24.4s\n",
      "  -> new best val_ROC-AUC=0.3611 saved checkpoint\n",
      "Epoch 5/14 — train_loss=3.3571 val_loss=1.1879 val_ROC-AUC=0.3986 val_AP=0.0210 time=24.4s\n",
      "  -> new best val_ROC-AUC=0.3986 saved checkpoint\n",
      "Epoch 6/14 — train_loss=3.3025 val_loss=1.1545 val_ROC-AUC=0.4594 val_AP=0.0233 time=25.6s\n",
      "  -> new best val_ROC-AUC=0.4594 saved checkpoint\n",
      "Epoch 7/14 — train_loss=3.3255 val_loss=1.1489 val_ROC-AUC=0.4969 val_AP=0.0252 time=23.9s\n",
      "  -> new best val_ROC-AUC=0.4969 saved checkpoint\n",
      "Epoch 8/14 — train_loss=3.0614 val_loss=1.1019 val_ROC-AUC=0.7049 val_AP=0.0442 time=28.8s\n",
      "  -> new best val_ROC-AUC=0.7049 saved checkpoint\n",
      "Epoch 9/14 — train_loss=3.0798 val_loss=1.1313 val_ROC-AUC=0.6117 val_AP=0.0377 time=25.4s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 10/14 — train_loss=3.0542 val_loss=1.1219 val_ROC-AUC=0.6539 val_AP=0.0506 time=26.8s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 11/14 — train_loss=2.7154 val_loss=1.1399 val_ROC-AUC=0.6261 val_AP=0.0871 time=25.5s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 12/14 — train_loss=2.7826 val_loss=1.0449 val_ROC-AUC=0.7141 val_AP=0.1928 time=24.1s\n",
      "  -> new best val_ROC-AUC=0.7141 saved checkpoint\n",
      "Epoch 13/14 — train_loss=2.7508 val_loss=1.0751 val_ROC-AUC=0.6549 val_AP=0.1506 time=24.7s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 14/14 — train_loss=2.2434 val_loss=1.1197 val_ROC-AUC=0.6387 val_AP=0.1377 time=24.8s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Training finished. Best epoch: 12 best_metric: 0.714105339105339\n",
      "Loaded best checkpoint (epoch 12, val ROC-AUC=0.7141)\n",
      "=== TEST METRICS (Transformer 3L) ===\n",
      "ROC-AUC: 0.5668\n",
      "AP: 0.3205\n",
      "Accuracy: 0.9726\n",
      "Precision: 0.2727\n",
      "Recall: 0.3750\n",
      "ConfusionMatrix: [[458, 8], [5, 3]]\n",
      "\n",
      "=== Transformer Depth Results (Test set) ===\n",
      "1-layer: ROC-AUC=0.4759, AP=0.1425, Precision=0.000, Recall=0.000\n",
      "2-layer: ROC-AUC=0.4804, AP=0.0217, Precision=0.000, Recall=0.000\n",
      "3-layer: ROC-AUC=0.5668, AP=0.3205, Precision=0.273, Recall=0.375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader_seq, val_loader_seq, test_loader_seq = get_sequence_loaders(\n",
    "    train_df, val_df, test_df, batch_size=cfg.batch_size\n",
    ")\n",
    "\n",
    "class TransformerSmiles(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, nhead=4, ff_dim=256,\n",
    "                 num_layers=2, dropout=0.1, max_len=cfg.max_len):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead,\n",
    "            dim_feedforward=ff_dim, dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == 0)\n",
    "        x = self.embed(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.classifier(x).view(-1)\n",
    "\n",
    "\n",
    "depth_results = {}\n",
    "\n",
    "for n_layers in [1, 2, 3]:\n",
    "    print(f\"\\n=== Training Transformer ({n_layers} layers) ===\")\n",
    "    model = TransformerSmiles(vocab_size=len(vocab), num_layers=n_layers, dropout=0.1).to(device)\n",
    "\n",
    "    pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 14\n",
    "    cfg_local.patience = 5\n",
    "\n",
    "    hist = run_training(model, train_loader_seq, val_loader_seq,\n",
    "                        opt, sch, loss_fn, cfg_local,\n",
    "                        model_name=f\"transformer_{n_layers}L\", device=device)\n",
    "\n",
    "    # Load best checkpoint\n",
    "    ckpt_path = Path(cfg.out_dir) / f\"transformer_{n_layers}L\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "    if ckpt_path.exists():\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state\"])\n",
    "        print(f\"Loaded best checkpoint (epoch {state['epoch']}, val ROC-AUC={state['metric']:.4f})\")\n",
    "\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_seq:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "\n",
    "    depth_results[n_layers] = metrics\n",
    "    print(f\"=== TEST METRICS (Transformer {n_layers}L) ===\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Transformer Depth Results (Test set) ===\")\n",
    "for L, m in depth_results.items():\n",
    "    print(f\"{L}-layer: ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, \"\n",
    "          f\"Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e132e8d-53ac-4858-ba30-197f743c9500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training GCN (1 layers) ===\n",
      "Epoch 1/30 — train_loss=1.9254 val_loss=1.7811 val_ROC-AUC=0.5368 val_AP=0.0274 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.5368 saved checkpoint\n",
      "Epoch 2/30 — train_loss=2.6926 val_loss=2.1311 val_ROC-AUC=0.5602 val_AP=0.0287 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.5602 saved checkpoint\n",
      "Epoch 3/30 — train_loss=2.6326 val_loss=2.1162 val_ROC-AUC=0.5776 val_AP=0.0297 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.5776 saved checkpoint\n",
      "Epoch 4/30 — train_loss=2.7195 val_loss=2.3498 val_ROC-AUC=0.5963 val_AP=0.0310 time=2.3s\n",
      "  -> new best val_ROC-AUC=0.5963 saved checkpoint\n",
      "Epoch 5/30 — train_loss=2.8986 val_loss=1.8407 val_ROC-AUC=0.6304 val_AP=0.0337 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.6304 saved checkpoint\n",
      "Epoch 6/30 — train_loss=2.9256 val_loss=1.9147 val_ROC-AUC=0.6575 val_AP=0.0362 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.6575 saved checkpoint\n",
      "Epoch 7/30 — train_loss=2.6198 val_loss=1.7676 val_ROC-AUC=0.6806 val_AP=0.0388 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.6806 saved checkpoint\n",
      "Epoch 8/30 — train_loss=2.4318 val_loss=1.9633 val_ROC-AUC=0.7121 val_AP=0.0433 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.7121 saved checkpoint\n",
      "Epoch 9/30 — train_loss=2.6875 val_loss=1.8421 val_ROC-AUC=0.7336 val_AP=0.0466 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.7336 saved checkpoint\n",
      "Epoch 10/30 — train_loss=2.4893 val_loss=1.5937 val_ROC-AUC=0.7428 val_AP=0.0479 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.7428 saved checkpoint\n",
      "Epoch 11/30 — train_loss=2.5926 val_loss=1.8993 val_ROC-AUC=0.7507 val_AP=0.0497 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.7507 saved checkpoint\n",
      "Epoch 12/30 — train_loss=2.6117 val_loss=1.8369 val_ROC-AUC=0.7688 val_AP=0.0534 time=2.3s\n",
      "  -> new best val_ROC-AUC=0.7688 saved checkpoint\n",
      "Epoch 13/30 — train_loss=2.3985 val_loss=1.6188 val_ROC-AUC=0.7765 val_AP=0.0550 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.7765 saved checkpoint\n",
      "Epoch 14/30 — train_loss=2.3594 val_loss=2.1113 val_ROC-AUC=0.7704 val_AP=0.0538 time=2.3s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 15/30 — train_loss=2.7428 val_loss=1.2833 val_ROC-AUC=0.7955 val_AP=0.0605 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.7955 saved checkpoint\n",
      "Epoch 16/30 — train_loss=2.3245 val_loss=1.9555 val_ROC-AUC=0.7655 val_AP=0.0525 time=2.3s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 17/30 — train_loss=2.4464 val_loss=1.5370 val_ROC-AUC=0.7991 val_AP=0.0614 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.7991 saved checkpoint\n",
      "Epoch 18/30 — train_loss=2.5668 val_loss=1.6159 val_ROC-AUC=0.8021 val_AP=0.0622 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.8021 saved checkpoint\n",
      "Epoch 19/30 — train_loss=2.5562 val_loss=1.6705 val_ROC-AUC=0.8189 val_AP=0.0682 time=2.3s\n",
      "  -> new best val_ROC-AUC=0.8189 saved checkpoint\n",
      "Epoch 20/30 — train_loss=2.2858 val_loss=1.9563 val_ROC-AUC=0.8211 val_AP=0.0690 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.8211 saved checkpoint\n",
      "Epoch 21/30 — train_loss=2.5780 val_loss=1.6262 val_ROC-AUC=0.8303 val_AP=0.0733 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.8303 saved checkpoint\n",
      "Epoch 22/30 — train_loss=2.4798 val_loss=1.5849 val_ROC-AUC=0.8369 val_AP=0.0767 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.8369 saved checkpoint\n",
      "Epoch 23/30 — train_loss=2.4093 val_loss=1.8911 val_ROC-AUC=0.8360 val_AP=0.0755 time=2.3s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 24/30 — train_loss=2.4437 val_loss=1.8345 val_ROC-AUC=0.8514 val_AP=0.0842 time=2.3s\n",
      "  -> new best val_ROC-AUC=0.8514 saved checkpoint\n",
      "Epoch 25/30 — train_loss=2.4673 val_loss=1.4320 val_ROC-AUC=0.8804 val_AP=0.1048 time=2.3s\n",
      "  -> new best val_ROC-AUC=0.8804 saved checkpoint\n",
      "Epoch 26/30 — train_loss=2.3818 val_loss=1.7059 val_ROC-AUC=0.8627 val_AP=0.0909 time=2.4s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 27/30 — train_loss=2.3421 val_loss=2.0414 val_ROC-AUC=0.8671 val_AP=0.0947 time=4.7s\n",
      "EarlyStopping: metric did not improve (2/6)\n",
      "Epoch 28/30 — train_loss=2.5856 val_loss=1.0638 val_ROC-AUC=0.9024 val_AP=0.1307 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.9024 saved checkpoint\n",
      "Epoch 29/30 — train_loss=2.3461 val_loss=1.6241 val_ROC-AUC=0.9021 val_AP=0.1298 time=2.4s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 30/30 — train_loss=2.5440 val_loss=1.5517 val_ROC-AUC=0.9091 val_AP=0.1396 time=2.4s\n",
      "  -> new best val_ROC-AUC=0.9091 saved checkpoint\n",
      "Training finished. Best epoch: 30 best_metric: 0.9090909090909091\n",
      "Loaded best checkpoint (epoch 30, val ROC-AUC=0.9091)\n",
      "=== TEST METRICS (GCN 1L) ===\n",
      "ROC-AUC: 0.6666\n",
      "AP: 0.0314\n",
      "Accuracy: 0.9684\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[459, 7], [8, 0]]\n",
      "\n",
      "=== Training GCN (2 layers) ===\n",
      "Epoch 1/30 — train_loss=1.6778 val_loss=3.2155 val_ROC-AUC=0.5269 val_AP=0.0266 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.5269 saved checkpoint\n",
      "Epoch 2/30 — train_loss=3.1725 val_loss=2.8524 val_ROC-AUC=0.5543 val_AP=0.0282 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.5543 saved checkpoint\n",
      "Epoch 3/30 — train_loss=3.1814 val_loss=1.9119 val_ROC-AUC=0.5841 val_AP=0.0301 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.5841 saved checkpoint\n",
      "Epoch 4/30 — train_loss=2.9704 val_loss=2.1243 val_ROC-AUC=0.6010 val_AP=0.0314 time=3.0s\n",
      "  -> new best val_ROC-AUC=0.6010 saved checkpoint\n",
      "Epoch 5/30 — train_loss=3.1462 val_loss=2.2018 val_ROC-AUC=0.6607 val_AP=0.0368 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.6607 saved checkpoint\n",
      "Epoch 6/30 — train_loss=3.0247 val_loss=2.3172 val_ROC-AUC=0.7092 val_AP=0.0429 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.7092 saved checkpoint\n",
      "Epoch 7/30 — train_loss=3.0025 val_loss=1.7615 val_ROC-AUC=0.7287 val_AP=0.0453 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.7287 saved checkpoint\n",
      "Epoch 8/30 — train_loss=2.8032 val_loss=2.2693 val_ROC-AUC=0.7372 val_AP=0.0467 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.7372 saved checkpoint\n",
      "Epoch 9/30 — train_loss=2.9015 val_loss=2.0614 val_ROC-AUC=0.7460 val_AP=0.0484 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.7460 saved checkpoint\n",
      "Epoch 10/30 — train_loss=2.8855 val_loss=1.9678 val_ROC-AUC=0.7756 val_AP=0.0548 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.7756 saved checkpoint\n",
      "Epoch 11/30 — train_loss=2.8205 val_loss=1.6360 val_ROC-AUC=0.7763 val_AP=0.0551 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.7763 saved checkpoint\n",
      "Epoch 12/30 — train_loss=2.7623 val_loss=1.9391 val_ROC-AUC=0.7807 val_AP=0.0560 time=3.0s\n",
      "  -> new best val_ROC-AUC=0.7807 saved checkpoint\n",
      "Epoch 13/30 — train_loss=2.6355 val_loss=2.3554 val_ROC-AUC=0.8027 val_AP=0.0616 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8027 saved checkpoint\n",
      "Epoch 14/30 — train_loss=2.8867 val_loss=1.5152 val_ROC-AUC=0.8092 val_AP=0.0640 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8092 saved checkpoint\n",
      "Epoch 15/30 — train_loss=2.8648 val_loss=1.3692 val_ROC-AUC=0.8281 val_AP=0.0710 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.8281 saved checkpoint\n",
      "Epoch 16/30 — train_loss=2.5555 val_loss=1.9590 val_ROC-AUC=0.8281 val_AP=0.0705 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.8281 saved checkpoint\n",
      "Epoch 17/30 — train_loss=2.8012 val_loss=1.6769 val_ROC-AUC=0.8391 val_AP=0.0754 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8391 saved checkpoint\n",
      "Epoch 18/30 — train_loss=2.8254 val_loss=2.0381 val_ROC-AUC=0.8418 val_AP=0.0762 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.8418 saved checkpoint\n",
      "Epoch 19/30 — train_loss=2.8436 val_loss=2.2941 val_ROC-AUC=0.8425 val_AP=0.0763 time=3.6s\n",
      "  -> new best val_ROC-AUC=0.8425 saved checkpoint\n",
      "Epoch 20/30 — train_loss=2.7877 val_loss=1.8177 val_ROC-AUC=0.8553 val_AP=0.0831 time=3.5s\n",
      "  -> new best val_ROC-AUC=0.8553 saved checkpoint\n",
      "Epoch 21/30 — train_loss=2.7111 val_loss=1.8816 val_ROC-AUC=0.8633 val_AP=0.0875 time=3.4s\n",
      "  -> new best val_ROC-AUC=0.8633 saved checkpoint\n",
      "Epoch 22/30 — train_loss=2.7773 val_loss=1.7395 val_ROC-AUC=0.8541 val_AP=0.0813 time=3.9s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 23/30 — train_loss=2.7465 val_loss=1.9553 val_ROC-AUC=0.8737 val_AP=0.0934 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8737 saved checkpoint\n",
      "Epoch 24/30 — train_loss=2.6789 val_loss=2.2148 val_ROC-AUC=0.8734 val_AP=0.0924 time=3.0s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 25/30 — train_loss=2.9202 val_loss=1.7661 val_ROC-AUC=0.8934 val_AP=0.1074 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8934 saved checkpoint\n",
      "Epoch 26/30 — train_loss=2.7548 val_loss=1.4628 val_ROC-AUC=0.9046 val_AP=0.1183 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9046 saved checkpoint\n",
      "Epoch 27/30 — train_loss=2.7464 val_loss=1.5571 val_ROC-AUC=0.9158 val_AP=0.1338 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9158 saved checkpoint\n",
      "Epoch 28/30 — train_loss=2.8456 val_loss=1.6535 val_ROC-AUC=0.9098 val_AP=0.1267 time=2.6s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 29/30 — train_loss=2.6902 val_loss=1.5366 val_ROC-AUC=0.9396 val_AP=0.1903 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9396 saved checkpoint\n",
      "Epoch 30/30 — train_loss=2.6723 val_loss=1.8776 val_ROC-AUC=0.9432 val_AP=0.1987 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9432 saved checkpoint\n",
      "Training finished. Best epoch: 30 best_metric: 0.9431818181818182\n",
      "Loaded best checkpoint (epoch 30, val ROC-AUC=0.9432)\n",
      "=== TEST METRICS (GCN 2L) ===\n",
      "ROC-AUC: 0.7328\n",
      "AP: 0.0478\n",
      "Accuracy: 0.9705\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[460, 6], [8, 0]]\n",
      "\n",
      "=== Training GCN (3 layers) ===\n",
      "Epoch 1/30 — train_loss=2.4288 val_loss=2.6846 val_ROC-AUC=0.5929 val_AP=0.0308 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.5929 saved checkpoint\n",
      "Epoch 2/30 — train_loss=3.4732 val_loss=2.3772 val_ROC-AUC=0.6427 val_AP=0.0348 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.6427 saved checkpoint\n",
      "Epoch 3/30 — train_loss=3.1830 val_loss=2.2087 val_ROC-AUC=0.7026 val_AP=0.0414 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.7026 saved checkpoint\n",
      "Epoch 4/30 — train_loss=3.3212 val_loss=2.1608 val_ROC-AUC=0.7282 val_AP=0.0451 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.7282 saved checkpoint\n",
      "Epoch 5/30 — train_loss=3.2363 val_loss=2.2890 val_ROC-AUC=0.7567 val_AP=0.0502 time=2.9s\n",
      "  -> new best val_ROC-AUC=0.7567 saved checkpoint\n",
      "Epoch 6/30 — train_loss=3.2080 val_loss=2.6633 val_ROC-AUC=0.7848 val_AP=0.0566 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.7848 saved checkpoint\n",
      "Epoch 7/30 — train_loss=3.1406 val_loss=2.6195 val_ROC-AUC=0.8090 val_AP=0.0636 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8090 saved checkpoint\n",
      "Epoch 8/30 — train_loss=3.0824 val_loss=1.9294 val_ROC-AUC=0.7974 val_AP=0.0605 time=2.6s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 9/30 — train_loss=3.2074 val_loss=1.9404 val_ROC-AUC=0.8169 val_AP=0.0667 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.8169 saved checkpoint\n",
      "Epoch 10/30 — train_loss=3.0387 val_loss=2.3090 val_ROC-AUC=0.8077 val_AP=0.0633 time=2.7s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 11/30 — train_loss=3.0153 val_loss=2.4486 val_ROC-AUC=0.8303 val_AP=0.0711 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8303 saved checkpoint\n",
      "Epoch 12/30 — train_loss=2.9632 val_loss=2.5771 val_ROC-AUC=0.8427 val_AP=0.0762 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8427 saved checkpoint\n",
      "Epoch 13/30 — train_loss=2.9695 val_loss=2.3672 val_ROC-AUC=0.8543 val_AP=0.0817 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8543 saved checkpoint\n",
      "Epoch 14/30 — train_loss=2.9706 val_loss=1.4121 val_ROC-AUC=0.8728 val_AP=0.0924 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8728 saved checkpoint\n",
      "Epoch 15/30 — train_loss=2.9508 val_loss=1.7794 val_ROC-AUC=0.8979 val_AP=0.1134 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.8979 saved checkpoint\n",
      "Epoch 16/30 — train_loss=2.8595 val_loss=1.7456 val_ROC-AUC=0.8725 val_AP=0.0936 time=2.6s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 17/30 — train_loss=3.0429 val_loss=1.4489 val_ROC-AUC=0.9073 val_AP=0.1282 time=2.9s\n",
      "  -> new best val_ROC-AUC=0.9073 saved checkpoint\n",
      "Epoch 18/30 — train_loss=2.9297 val_loss=1.3798 val_ROC-AUC=0.9159 val_AP=0.1477 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.9159 saved checkpoint\n",
      "Epoch 19/30 — train_loss=2.8805 val_loss=2.2458 val_ROC-AUC=0.9437 val_AP=0.2074 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9437 saved checkpoint\n",
      "Epoch 20/30 — train_loss=2.9426 val_loss=1.6264 val_ROC-AUC=0.9453 val_AP=0.2459 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.9453 saved checkpoint\n",
      "Epoch 21/30 — train_loss=2.7713 val_loss=1.8822 val_ROC-AUC=0.9531 val_AP=0.2745 time=2.9s\n",
      "  -> new best val_ROC-AUC=0.9531 saved checkpoint\n",
      "Epoch 22/30 — train_loss=2.8185 val_loss=2.5309 val_ROC-AUC=0.9473 val_AP=0.2549 time=2.7s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 23/30 — train_loss=3.0385 val_loss=2.2520 val_ROC-AUC=0.9571 val_AP=0.3046 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.9571 saved checkpoint\n",
      "Epoch 24/30 — train_loss=2.7131 val_loss=1.5193 val_ROC-AUC=0.9585 val_AP=0.3979 time=2.8s\n",
      "  -> new best val_ROC-AUC=0.9585 saved checkpoint\n",
      "Epoch 25/30 — train_loss=3.0777 val_loss=1.2668 val_ROC-AUC=0.9573 val_AP=0.3964 time=2.7s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 26/30 — train_loss=2.8068 val_loss=1.8746 val_ROC-AUC=0.9609 val_AP=0.3943 time=2.7s\n",
      "  -> new best val_ROC-AUC=0.9609 saved checkpoint\n",
      "Epoch 27/30 — train_loss=2.5430 val_loss=0.9490 val_ROC-AUC=0.9562 val_AP=0.4842 time=2.6s\n",
      "EarlyStopping: metric did not improve (1/6)\n",
      "Epoch 28/30 — train_loss=2.9160 val_loss=2.1002 val_ROC-AUC=0.9369 val_AP=0.4075 time=2.5s\n",
      "EarlyStopping: metric did not improve (2/6)\n",
      "Epoch 29/30 — train_loss=3.0726 val_loss=0.7891 val_ROC-AUC=0.9551 val_AP=0.5932 time=2.7s\n",
      "EarlyStopping: metric did not improve (3/6)\n",
      "Epoch 30/30 — train_loss=2.8864 val_loss=1.0720 val_ROC-AUC=0.9571 val_AP=0.4823 time=2.6s\n",
      "EarlyStopping: metric did not improve (4/6)\n",
      "Training finished. Best epoch: 26 best_metric: 0.9608585858585859\n",
      "Loaded best checkpoint (epoch 26, val ROC-AUC=0.9609)\n",
      "=== TEST METRICS (GCN 3L) ===\n",
      "ROC-AUC: 0.6558\n",
      "AP: 0.0449\n",
      "Accuracy: 0.9662\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[458, 8], [8, 0]]\n",
      "\n",
      "=== GNN Depth Results (Test set) ===\n",
      "1-layer: ROC-AUC=0.6666, AP=0.0314, Precision=0.000, Recall=0.000\n",
      "2-layer: ROC-AUC=0.7328, AP=0.0478, Precision=0.000, Recall=0.000\n",
      "3-layer: ROC-AUC=0.6558, AP=0.0449, Precision=0.000, Recall=0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader_graph, val_loader_graph, test_loader_graph = get_graph_loaders(\n",
    "    train_df, val_df, test_df, batch_size=cfg.batch_size, need_3d=False\n",
    ")\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        prev_dim = in_dim\n",
    "        for _ in range(num_layers):\n",
    "            conv = GCNConv(prev_dim, hidden_dim)\n",
    "            self.convs.append(conv)\n",
    "            prev_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.classifier(x).view(-1)\n",
    "\n",
    "\n",
    "depth_results_gnn = {}\n",
    "\n",
    "for n_layers in [1, 2, 3]:\n",
    "    print(f\"\\n=== Training GCN ({n_layers} layers) ===\")\n",
    "    model = GCNModel(in_dim=train_loader_graph.dataset[0].x.shape[1],\n",
    "                     hidden_dim=64, num_layers=n_layers, dropout=0.1).to(device)\n",
    "\n",
    "    pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 30\n",
    "    cfg_local.patience = 6\n",
    "\n",
    "    hist = run_training(model, train_loader_graph, val_loader_graph, opt, sch,\n",
    "                        loss_fn, cfg_local, model_name=f\"gcn_{n_layers}L\", device=device)\n",
    "\n",
    "    # Load best checkpoint\n",
    "    ckpt_path = Path(cfg.out_dir) / f\"gcn_{n_layers}L\" / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "    if ckpt_path.exists():\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state\"])\n",
    "        print(f\"Loaded best checkpoint (epoch {state['epoch']}, val ROC-AUC={state['metric']:.4f})\")\n",
    "\n",
    "    # Evaluate on test\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_graph:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(batch.y.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    depth_results_gnn[n_layers] = metrics\n",
    "    print(f\"=== TEST METRICS (GCN {n_layers}L) ===\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== GNN Depth Results (Test set) ===\")\n",
    "for L, m in depth_results_gnn.items():\n",
    "    print(f\"{L}-layer: ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, \"\n",
    "          f\"Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3b2fd41-5648-4ca8-9cf2-e4bf5bd756ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training CNN (dropout=0.0) ===\n",
      "Epoch 1/20 — train_loss=5.4127 val_loss=2.6246 val_ROC-AUC=0.7235 val_AP=0.0517 time=4.0s\n",
      "  -> new best val_ROC-AUC=0.7235 saved checkpoint\n",
      "Epoch 2/20 — train_loss=3.9597 val_loss=2.9584 val_ROC-AUC=0.8375 val_AP=0.3036 time=3.0s\n",
      "  -> new best val_ROC-AUC=0.8375 saved checkpoint\n",
      "Epoch 3/20 — train_loss=3.1135 val_loss=4.6060 val_ROC-AUC=0.9001 val_AP=0.5622 time=3.2s\n",
      "  -> new best val_ROC-AUC=0.9001 saved checkpoint\n",
      "Epoch 4/20 — train_loss=2.8828 val_loss=1.2993 val_ROC-AUC=0.9356 val_AP=0.7194 time=3.2s\n",
      "  -> new best val_ROC-AUC=0.9356 saved checkpoint\n",
      "Epoch 5/20 — train_loss=2.4286 val_loss=0.5144 val_ROC-AUC=0.9374 val_AP=0.7576 time=3.0s\n",
      "  -> new best val_ROC-AUC=0.9374 saved checkpoint\n",
      "Epoch 6/20 — train_loss=2.0808 val_loss=2.0803 val_ROC-AUC=0.9006 val_AP=0.7485 time=3.0s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 7/20 — train_loss=1.5530 val_loss=1.0112 val_ROC-AUC=0.9473 val_AP=0.7612 time=3.2s\n",
      "  -> new best val_ROC-AUC=0.9473 saved checkpoint\n",
      "Epoch 8/20 — train_loss=1.7388 val_loss=0.6763 val_ROC-AUC=0.9343 val_AP=0.7725 time=3.1s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 9/20 — train_loss=1.2459 val_loss=1.2635 val_ROC-AUC=0.9441 val_AP=0.7544 time=3.0s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 10/20 — train_loss=1.2849 val_loss=2.2601 val_ROC-AUC=0.9138 val_AP=0.7450 time=3.3s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 11/20 — train_loss=1.1463 val_loss=1.3907 val_ROC-AUC=0.9394 val_AP=0.7583 time=3.0s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 12/20 — train_loss=0.2979 val_loss=1.4808 val_ROC-AUC=0.9232 val_AP=0.7544 time=3.1s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 12 (best epoch 7 val_ROC-AUC=0.9473304473304472)\n",
      "Training finished. Best epoch: 7 best_metric: 0.9473304473304472\n",
      "Loaded best checkpoint (epoch 7, val ROC-AUC=0.9473)\n",
      "=== TEST METRICS (CNN dropout=0.0) ===\n",
      "ROC-AUC: 0.7103\n",
      "AP: 0.1633\n",
      "Accuracy: 0.8502\n",
      "Precision: 0.0299\n",
      "Recall: 0.2500\n",
      "ConfusionMatrix: [[401, 65], [6, 2]]\n",
      "\n",
      "=== Training CNN (dropout=0.1) ===\n",
      "Epoch 1/20 — train_loss=5.0695 val_loss=3.5006 val_ROC-AUC=0.7924 val_AP=0.1307 time=3.5s\n",
      "  -> new best val_ROC-AUC=0.7924 saved checkpoint\n",
      "Epoch 2/20 — train_loss=3.7893 val_loss=1.2040 val_ROC-AUC=0.9533 val_AP=0.7422 time=3.3s\n",
      "  -> new best val_ROC-AUC=0.9533 saved checkpoint\n",
      "Epoch 3/20 — train_loss=2.9653 val_loss=0.4647 val_ROC-AUC=0.9578 val_AP=0.7426 time=3.3s\n",
      "  -> new best val_ROC-AUC=0.9578 saved checkpoint\n",
      "Epoch 4/20 — train_loss=2.4385 val_loss=2.1551 val_ROC-AUC=0.9118 val_AP=0.7693 time=3.3s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/20 — train_loss=2.2542 val_loss=3.5057 val_ROC-AUC=0.8862 val_AP=0.7542 time=3.5s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 6/20 — train_loss=1.9321 val_loss=2.1194 val_ROC-AUC=0.8939 val_AP=0.7651 time=3.3s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 7/20 — train_loss=2.1738 val_loss=3.0216 val_ROC-AUC=0.9006 val_AP=0.7547 time=3.4s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 8/20 — train_loss=0.9178 val_loss=2.0772 val_ROC-AUC=0.9084 val_AP=0.7668 time=4.1s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 8 (best epoch 3 val_ROC-AUC=0.9577922077922079)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9577922077922079\n",
      "Loaded best checkpoint (epoch 3, val ROC-AUC=0.9578)\n",
      "=== TEST METRICS (CNN dropout=0.1) ===\n",
      "ROC-AUC: 0.7291\n",
      "AP: 0.2328\n",
      "Accuracy: 0.8059\n",
      "Precision: 0.0227\n",
      "Recall: 0.2500\n",
      "ConfusionMatrix: [[380, 86], [6, 2]]\n",
      "\n",
      "=== Training CNN (dropout=0.3) ===\n",
      "Epoch 1/20 — train_loss=5.8478 val_loss=4.6391 val_ROC-AUC=0.6714 val_AP=0.0380 time=4.0s\n",
      "  -> new best val_ROC-AUC=0.6714 saved checkpoint\n",
      "Epoch 2/20 — train_loss=5.2132 val_loss=5.8830 val_ROC-AUC=0.7956 val_AP=0.0718 time=4.2s\n",
      "  -> new best val_ROC-AUC=0.7956 saved checkpoint\n",
      "Epoch 3/20 — train_loss=3.8403 val_loss=5.0369 val_ROC-AUC=0.8995 val_AP=0.1939 time=3.4s\n",
      "  -> new best val_ROC-AUC=0.8995 saved checkpoint\n",
      "Epoch 4/20 — train_loss=4.0900 val_loss=1.4646 val_ROC-AUC=0.9737 val_AP=0.7378 time=3.2s\n",
      "  -> new best val_ROC-AUC=0.9737 saved checkpoint\n",
      "Epoch 5/20 — train_loss=3.6850 val_loss=1.6692 val_ROC-AUC=0.9675 val_AP=0.6893 time=3.4s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 6/20 — train_loss=2.6342 val_loss=1.9255 val_ROC-AUC=0.9529 val_AP=0.7134 time=3.2s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 7/20 — train_loss=2.7749 val_loss=1.7441 val_ROC-AUC=0.9105 val_AP=0.7248 time=3.5s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 8/20 — train_loss=2.4218 val_loss=1.9273 val_ROC-AUC=0.9113 val_AP=0.7895 time=3.3s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 9/20 — train_loss=1.4441 val_loss=1.4249 val_ROC-AUC=0.8925 val_AP=0.7600 time=3.3s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 9 (best epoch 4 val_ROC-AUC=0.9736652236652237)\n",
      "Training finished. Best epoch: 4 best_metric: 0.9736652236652237\n",
      "Loaded best checkpoint (epoch 4, val ROC-AUC=0.9737)\n",
      "=== TEST METRICS (CNN dropout=0.3) ===\n",
      "ROC-AUC: 0.7323\n",
      "AP: 0.0541\n",
      "Accuracy: 0.9831\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[466, 0], [8, 0]]\n",
      "\n",
      "=== Training CNN (dropout=0.5) ===\n",
      "Epoch 1/20 — train_loss=7.3753 val_loss=3.8958 val_ROC-AUC=0.6420 val_AP=0.0403 time=3.3s\n",
      "  -> new best val_ROC-AUC=0.6420 saved checkpoint\n",
      "Epoch 2/20 — train_loss=5.3876 val_loss=4.6890 val_ROC-AUC=0.6784 val_AP=0.0919 time=3.2s\n",
      "  -> new best val_ROC-AUC=0.6784 saved checkpoint\n",
      "Epoch 3/20 — train_loss=5.4763 val_loss=5.4890 val_ROC-AUC=0.8241 val_AP=0.1212 time=3.3s\n",
      "  -> new best val_ROC-AUC=0.8241 saved checkpoint\n",
      "Epoch 4/20 — train_loss=5.0332 val_loss=3.9247 val_ROC-AUC=0.8945 val_AP=0.1641 time=3.3s\n",
      "  -> new best val_ROC-AUC=0.8945 saved checkpoint\n",
      "Epoch 5/20 — train_loss=4.5140 val_loss=3.2228 val_ROC-AUC=0.9351 val_AP=0.5352 time=3.4s\n",
      "  -> new best val_ROC-AUC=0.9351 saved checkpoint\n",
      "Epoch 6/20 — train_loss=4.6669 val_loss=3.0855 val_ROC-AUC=0.9179 val_AP=0.5728 time=3.2s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 7/20 — train_loss=4.2232 val_loss=3.0214 val_ROC-AUC=0.9044 val_AP=0.5392 time=5.1s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 8/20 — train_loss=4.0519 val_loss=2.3749 val_ROC-AUC=0.9448 val_AP=0.6593 time=3.8s\n",
      "  -> new best val_ROC-AUC=0.9448 saved checkpoint\n",
      "Epoch 9/20 — train_loss=3.7710 val_loss=1.2949 val_ROC-AUC=0.9315 val_AP=0.7199 time=3.6s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 10/20 — train_loss=3.3855 val_loss=2.1174 val_ROC-AUC=0.9075 val_AP=0.6952 time=3.5s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 11/20 — train_loss=3.2732 val_loss=1.5282 val_ROC-AUC=0.9408 val_AP=0.7286 time=3.3s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 12/20 — train_loss=3.6211 val_loss=4.2034 val_ROC-AUC=0.9241 val_AP=0.7394 time=3.2s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 13/20 — train_loss=2.6206 val_loss=1.4891 val_ROC-AUC=0.9437 val_AP=0.7828 time=3.2s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 13 (best epoch 8 val_ROC-AUC=0.9448051948051948)\n",
      "Training finished. Best epoch: 8 best_metric: 0.9448051948051948\n",
      "Loaded best checkpoint (epoch 8, val ROC-AUC=0.9448)\n",
      "=== TEST METRICS (CNN dropout=0.5) ===\n",
      "ROC-AUC: 0.6349\n",
      "AP: 0.0534\n",
      "Accuracy: 0.9810\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[465, 1], [8, 0]]\n",
      "\n",
      "=== CNN Dropout Sweep: Test Summary ===\n",
      "dropout=0.0: ROC-AUC=0.7103, AP=0.1633, Precision=0.030, Recall=0.250\n",
      "dropout=0.1: ROC-AUC=0.7291, AP=0.2328, Precision=0.023, Recall=0.250\n",
      "dropout=0.3: ROC-AUC=0.7323, AP=0.0541, Precision=0.000, Recall=0.000\n",
      "dropout=0.5: ROC-AUC=0.6349, AP=0.0534, Precision=0.000, Recall=0.000\n",
      "\n",
      "Saved JSON to: runs_hw1/cnn_dropout_sweep/cnn_dropout_results.json\n"
     ]
    }
   ],
   "source": [
    "train_loader_seq, val_loader_seq, test_loader_seq = get_sequence_loaders(\n",
    "    train_df, val_df, test_df, batch_size=cfg.batch_size\n",
    ")\n",
    "\n",
    "# Lightweight CNN with multi-kernel convs; dropout applied after pooled concat\n",
    "class CNNSmiles(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=cfg.embed_dim, channels=cfg.cnn_channels,\n",
    "                 kernel_sizes=cfg.cnn_kernel_sizes, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=emb_dim, out_channels=channels, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 * channels * len(kernel_sizes), 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T) indices\n",
    "        x = self.embed(x)               \n",
    "        x = x.transpose(1, 2)           \n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            h = torch.relu(conv(x))       \n",
    "            h_max = torch.amax(h, dim=2)   \n",
    "            h_avg = torch.mean(h, dim=2) \n",
    "            feats.append(torch.cat([h_max, h_avg], dim=1)) \n",
    "        h = torch.cat(feats, dim=1)       \n",
    "        # compress (2C*K) -> (C*K) with a linear layer is optional;\n",
    "        # here, simply dropout then classifier handles the size.\n",
    "        h = self.dropout(h)\n",
    "        return self.classifier(h).view(-1)\n",
    "\n",
    "# Where to save\n",
    "out_dir = Path(cfg.out_dir) / \"cnn_dropout_sweep\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "drop_values = [0.0, 0.1, 0.3, 0.5]\n",
    "results = {}\n",
    "\n",
    "for dp in drop_values:\n",
    "    print(f\"\\n=== Training CNN (dropout={dp}) ===\")\n",
    "    model = CNNSmiles(vocab_size=len(vocab),\n",
    "                      emb_dim=128,        \n",
    "                      channels=128,         \n",
    "                      kernel_sizes=(3,5,7),\n",
    "                      dropout=dp,\n",
    "                      pad_idx=0).to(device)\n",
    "\n",
    "    pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 20\n",
    "    cfg_local.patience = 5\n",
    "    tag = f\"cnn_drop_{str(dp).replace('.','_')}\"\n",
    "    _ = run_training(model, train_loader_seq, val_loader_seq, opt, sch, loss_fn, cfg_local,\n",
    "                     model_name=tag, device=device)\n",
    "\n",
    "    # Load best checkpoint\n",
    "    ckpt = Path(cfg.out_dir) / tag / \"checkpoints\" / \"checkpoint_best.pt\"\n",
    "    if ckpt.exists():\n",
    "        state = torch.load(ckpt, map_location=device)\n",
    "        model.load_state_dict(state[\"model_state\"])\n",
    "        print(f\"Loaded best checkpoint (epoch {state['epoch']}, val ROC-AUC={state['metric']:.4f})\")\n",
    "\n",
    "    # Evaluate on test\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_seq:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_scores.append(probs); all_targets.append(yb.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores); all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    results[dp] = metrics\n",
    "    print(f\"=== TEST METRICS (CNN dropout={dp}) ===\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Save & print summary\n",
    "(summary_path := out_dir / \"cnn_dropout_results.json\").write_text(json.dumps(results, indent=2))\n",
    "print(\"\\n=== CNN Dropout Sweep: Test Summary ===\")\n",
    "for dp in drop_values:\n",
    "    m = results[dp]\n",
    "    print(f\"dropout={dp:>3}: ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, \"\n",
    "          f\"Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")\n",
    "print(f\"\\nSaved JSON to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f913aa1e-8977-4f02-a848-c220c0ac79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training CNN with weight_decay=0.0 ===\n",
      "Epoch 1/15 — train_loss=0.1765 val_loss=0.1508 val_ROC-AUC=0.7258 val_AP=0.0469 time=3.5s\n",
      "  -> new best val_ROC-AUC=0.7258 saved checkpoint\n",
      "Epoch 2/15 — train_loss=0.1392 val_loss=0.0958 val_ROC-AUC=0.9307 val_AP=0.5195 time=4.8s\n",
      "  -> new best val_ROC-AUC=0.9307 saved checkpoint\n",
      "Epoch 3/15 — train_loss=0.1096 val_loss=0.0816 val_ROC-AUC=0.9625 val_AP=0.7452 time=2.9s\n",
      "  -> new best val_ROC-AUC=0.9625 saved checkpoint\n",
      "Epoch 4/15 — train_loss=0.1012 val_loss=0.0545 val_ROC-AUC=0.9295 val_AP=0.7435 time=2.7s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/15 — train_loss=0.0712 val_loss=0.0483 val_ROC-AUC=0.9596 val_AP=0.7690 time=2.6s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 6/15 — train_loss=0.0459 val_loss=0.0548 val_ROC-AUC=0.9398 val_AP=0.7093 time=2.5s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 7/15 — train_loss=0.0349 val_loss=0.0739 val_ROC-AUC=0.9417 val_AP=0.7034 time=2.5s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 8/15 — train_loss=0.0152 val_loss=0.0761 val_ROC-AUC=0.8855 val_AP=0.7168 time=3.2s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 8 (best epoch 3 val_ROC-AUC=0.9624819624819625)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9624819624819625\n",
      "Test ROC-AUC=0.6744, AP=0.1764\n",
      "\n",
      "=== Training CNN with weight_decay=0.0001 ===\n",
      "Epoch 1/15 — train_loss=0.1826 val_loss=0.1250 val_ROC-AUC=0.8018 val_AP=0.0749 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.8018 saved checkpoint\n",
      "Epoch 2/15 — train_loss=0.1440 val_loss=0.0794 val_ROC-AUC=0.9434 val_AP=0.6886 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.9434 saved checkpoint\n",
      "Epoch 3/15 — train_loss=0.1064 val_loss=0.0768 val_ROC-AUC=0.9798 val_AP=0.7851 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.9798 saved checkpoint\n",
      "Epoch 4/15 — train_loss=0.0932 val_loss=0.0572 val_ROC-AUC=0.9610 val_AP=0.7736 time=2.5s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/15 — train_loss=0.0823 val_loss=0.0471 val_ROC-AUC=0.9511 val_AP=0.7583 time=2.4s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 6/15 — train_loss=0.0614 val_loss=0.0509 val_ROC-AUC=0.9349 val_AP=0.7492 time=2.5s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 7/15 — train_loss=0.0444 val_loss=0.0514 val_ROC-AUC=0.9497 val_AP=0.7609 time=2.6s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 8/15 — train_loss=0.0226 val_loss=0.0547 val_ROC-AUC=0.9396 val_AP=0.7565 time=2.8s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 8 (best epoch 3 val_ROC-AUC=0.9797979797979798)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9797979797979798\n",
      "Test ROC-AUC=0.7245, AP=0.1838\n",
      "\n",
      "=== Training CNN with weight_decay=0.001 ===\n",
      "Epoch 1/15 — train_loss=0.1740 val_loss=0.1382 val_ROC-AUC=0.7431 val_AP=0.0476 time=2.6s\n",
      "  -> new best val_ROC-AUC=0.7431 saved checkpoint\n",
      "Epoch 2/15 — train_loss=0.1397 val_loss=0.0970 val_ROC-AUC=0.9401 val_AP=0.4004 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.9401 saved checkpoint\n",
      "Epoch 3/15 — train_loss=0.1129 val_loss=0.0639 val_ROC-AUC=0.9585 val_AP=0.7113 time=2.5s\n",
      "  -> new best val_ROC-AUC=0.9585 saved checkpoint\n",
      "Epoch 4/15 — train_loss=0.0792 val_loss=0.0496 val_ROC-AUC=0.9370 val_AP=0.7849 time=2.5s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 5/15 — train_loss=0.0678 val_loss=0.0508 val_ROC-AUC=0.9468 val_AP=0.7693 time=2.5s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 6/15 — train_loss=0.0579 val_loss=0.0613 val_ROC-AUC=0.9333 val_AP=0.7517 time=2.4s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 7/15 — train_loss=0.0464 val_loss=0.0532 val_ROC-AUC=0.9232 val_AP=0.7431 time=2.6s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 8/15 — train_loss=0.0302 val_loss=0.0530 val_ROC-AUC=0.9102 val_AP=0.7404 time=2.5s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 8 (best epoch 3 val_ROC-AUC=0.9585137085137085)\n",
      "Training finished. Best epoch: 3 best_metric: 0.9585137085137085\n",
      "Test ROC-AUC=0.7414, AP=0.1873\n",
      "\n",
      "=== CNN L2 Regularization Results (Test set) ===\n",
      "weight_decay=0.0: ROC-AUC=0.6744, AP=0.1764, Precision=0.167, Recall=0.125\n",
      "weight_decay=0.0001: ROC-AUC=0.7245, AP=0.1838, Precision=0.200, Recall=0.250\n",
      "weight_decay=0.001: ROC-AUC=0.7414, AP=0.1873, Precision=0.125, Recall=0.125\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(cfg.out_dir) / \"cnn_l2_sweep\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "weight_decays = [0.0, 1e-4, 1e-3]\n",
    "results = {}\n",
    "\n",
    "for wd in weight_decays:\n",
    "    print(f\"\\n=== Training CNN with weight_decay={wd} ===\")\n",
    "\n",
    "    model = CNNFingerprint(vocab_size, embed_dim=128, num_filters=128, kernel_sizes=(3,5,7),\n",
    "                       dropout=0.1).to(device)\n",
    "\n",
    "    loss_fn = get_loss_fn(\"classification\", device=device)\n",
    "    opt = Adam(model.parameters(), lr=cfg.lr, weight_decay=wd)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=3)\n",
    "    cfg_local = deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 15\n",
    "    cfg_local.patience = 5\n",
    "\n",
    "    hist = run_training(model, train_loader_seq, val_loader_seq, opt, scheduler,\n",
    "                        loss_fn, cfg_local, model_name=f\"cnn_l2_{wd}\", device=device)\n",
    "\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_seq:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb).view(-1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    results[wd] = metrics\n",
    "\n",
    "    print(f\"Test ROC-AUC={metrics['ROC-AUC']:.4f}, AP={metrics['AP']:.4f}\")\n",
    "\n",
    "print(\"\\n=== CNN L2 Regularization Results (Test set) ===\")\n",
    "for wd, m in results.items():\n",
    "    print(f\"weight_decay={wd}: ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, \"\n",
    "          f\"Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b849c77f-449a-43a7-a913-b944b90cb9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer (constant) ===\n",
      "Epoch 1/15 — train_loss=2.7354 val_loss=1.1927 val_ROC-AUC=0.3494 val_AP=0.0195 time=15.4s\n",
      "  -> new best val_ROC-AUC=0.3494 saved checkpoint\n",
      "Epoch 2/15 — train_loss=3.3391 val_loss=1.1882 val_ROC-AUC=0.3595 val_AP=0.0198 time=19.4s\n",
      "  -> new best val_ROC-AUC=0.3595 saved checkpoint\n",
      "Epoch 3/15 — train_loss=3.3635 val_loss=1.1635 val_ROC-AUC=0.4037 val_AP=0.0212 time=15.2s\n",
      "  -> new best val_ROC-AUC=0.4037 saved checkpoint\n",
      "Epoch 4/15 — train_loss=3.1914 val_loss=1.1523 val_ROC-AUC=0.4913 val_AP=0.0253 time=15.0s\n",
      "  -> new best val_ROC-AUC=0.4913 saved checkpoint\n",
      "Epoch 5/15 — train_loss=3.1389 val_loss=1.0972 val_ROC-AUC=0.6858 val_AP=0.0686 time=13.7s\n",
      "  -> new best val_ROC-AUC=0.6858 saved checkpoint\n",
      "Epoch 6/15 — train_loss=2.8054 val_loss=1.0992 val_ROC-AUC=0.6966 val_AP=0.0894 time=14.2s\n",
      "  -> new best val_ROC-AUC=0.6966 saved checkpoint\n",
      "Epoch 7/15 — train_loss=2.9911 val_loss=1.0652 val_ROC-AUC=0.7758 val_AP=0.1141 time=16.7s\n",
      "  -> new best val_ROC-AUC=0.7758 saved checkpoint\n",
      "Epoch 8/15 — train_loss=2.8809 val_loss=1.0783 val_ROC-AUC=0.7347 val_AP=0.1239 time=13.5s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 9/15 — train_loss=2.6799 val_loss=1.0738 val_ROC-AUC=0.7401 val_AP=0.1754 time=12.6s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 10/15 — train_loss=2.6045 val_loss=1.0702 val_ROC-AUC=0.7211 val_AP=0.1945 time=13.8s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Epoch 11/15 — train_loss=2.4925 val_loss=1.0151 val_ROC-AUC=0.7522 val_AP=0.2311 time=12.5s\n",
      "EarlyStopping: metric did not improve (4/5)\n",
      "Epoch 12/15 — train_loss=2.5378 val_loss=0.9873 val_ROC-AUC=0.7707 val_AP=0.2482 time=12.5s\n",
      "EarlyStopping: metric did not improve (5/5)\n",
      "Early stopping at epoch 12 (best epoch 7 val_ROC-AUC=0.7757936507936508)\n",
      "Training finished. Best epoch: 7 best_metric: 0.7757936507936508\n",
      "\n",
      "=== CONSTANT TEST METRICS ===\n",
      "ROC-AUC: 0.5684\n",
      "AP: 0.2852\n",
      "Accuracy: 0.9156\n",
      "Precision: 0.0789\n",
      "Recall: 0.3750\n",
      "ConfusionMatrix: [[431, 35], [5, 3]]\n",
      "\n",
      "=== Training Transformer (step) ===\n",
      "Epoch 1/15 — train_loss=2.8373 val_loss=1.2397 val_ROC-AUC=0.3400 val_AP=0.0192 time=12.5s\n",
      "  -> new best val_ROC-AUC=0.3400 saved checkpoint\n",
      "Epoch 2/15 — train_loss=3.4599 val_loss=1.2076 val_ROC-AUC=0.3445 val_AP=0.0193 time=12.4s\n",
      "  -> new best val_ROC-AUC=0.3445 saved checkpoint\n",
      "Epoch 3/15 — train_loss=3.4537 val_loss=1.2036 val_ROC-AUC=0.3575 val_AP=0.0197 time=12.6s\n",
      "  -> new best val_ROC-AUC=0.3575 saved checkpoint\n",
      "Epoch 4/15 — train_loss=3.3418 val_loss=1.2043 val_ROC-AUC=0.3714 val_AP=0.0201 time=12.5s\n",
      "  -> new best val_ROC-AUC=0.3714 saved checkpoint\n",
      "Epoch 5/15 — train_loss=3.0987 val_loss=1.2192 val_ROC-AUC=0.3891 val_AP=0.0207 time=13.9s\n",
      "  -> new best val_ROC-AUC=0.3891 saved checkpoint\n",
      "Epoch 6/15 — train_loss=3.1768 val_loss=1.1663 val_ROC-AUC=0.4462 val_AP=0.0229 time=13.2s\n",
      "  -> new best val_ROC-AUC=0.4462 saved checkpoint\n",
      "Epoch 7/15 — train_loss=3.1574 val_loss=1.1868 val_ROC-AUC=0.4486 val_AP=0.0231 time=17.0s\n",
      "  -> new best val_ROC-AUC=0.4486 saved checkpoint\n",
      "Epoch 8/15 — train_loss=2.8203 val_loss=1.1863 val_ROC-AUC=0.4780 val_AP=0.0245 time=15.7s\n",
      "  -> new best val_ROC-AUC=0.4780 saved checkpoint\n",
      "Epoch 9/15 — train_loss=2.6777 val_loss=1.1963 val_ROC-AUC=0.4708 val_AP=0.0243 time=32.2s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 10/15 — train_loss=2.6286 val_loss=1.1958 val_ROC-AUC=0.4940 val_AP=0.0254 time=16.3s\n",
      "  -> new best val_ROC-AUC=0.4940 saved checkpoint\n",
      "Epoch 11/15 — train_loss=2.5347 val_loss=1.1876 val_ROC-AUC=0.5101 val_AP=0.0263 time=14.5s\n",
      "  -> new best val_ROC-AUC=0.5101 saved checkpoint\n",
      "Epoch 12/15 — train_loss=2.4518 val_loss=1.1672 val_ROC-AUC=0.5575 val_AP=0.0302 time=16.2s\n",
      "  -> new best val_ROC-AUC=0.5575 saved checkpoint\n",
      "Epoch 13/15 — train_loss=2.4919 val_loss=1.1755 val_ROC-AUC=0.5556 val_AP=0.0305 time=19.3s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 14/15 — train_loss=2.3451 val_loss=1.1684 val_ROC-AUC=0.5731 val_AP=0.0346 time=29.0s\n",
      "  -> new best val_ROC-AUC=0.5731 saved checkpoint\n",
      "Epoch 15/15 — train_loss=2.4680 val_loss=1.1352 val_ROC-AUC=0.6190 val_AP=0.0657 time=16.5s\n",
      "  -> new best val_ROC-AUC=0.6190 saved checkpoint\n",
      "Training finished. Best epoch: 15 best_metric: 0.6190476190476191\n",
      "\n",
      "=== STEP TEST METRICS ===\n",
      "ROC-AUC: 0.5341\n",
      "AP: 0.0386\n",
      "Accuracy: 0.9747\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[462, 4], [8, 0]]\n",
      "\n",
      "=== Training Transformer (cosine) ===\n",
      "Epoch 1/15 — train_loss=2.5024 val_loss=1.2234 val_ROC-AUC=0.3382 val_AP=0.0192 time=15.8s\n",
      "  -> new best val_ROC-AUC=0.3382 saved checkpoint\n",
      "Epoch 2/15 — train_loss=3.4733 val_loss=1.2034 val_ROC-AUC=0.3535 val_AP=0.0196 time=13.8s\n",
      "  -> new best val_ROC-AUC=0.3535 saved checkpoint\n",
      "Epoch 3/15 — train_loss=3.4244 val_loss=1.1980 val_ROC-AUC=0.3741 val_AP=0.0202 time=15.2s\n",
      "  -> new best val_ROC-AUC=0.3741 saved checkpoint\n",
      "Epoch 4/15 — train_loss=3.4695 val_loss=1.2051 val_ROC-AUC=0.3891 val_AP=0.0207 time=14.9s\n",
      "  -> new best val_ROC-AUC=0.3891 saved checkpoint\n",
      "Epoch 5/15 — train_loss=3.4759 val_loss=1.1730 val_ROC-AUC=0.4331 val_AP=0.0222 time=16.1s\n",
      "  -> new best val_ROC-AUC=0.4331 saved checkpoint\n",
      "Epoch 6/15 — train_loss=3.1830 val_loss=1.1654 val_ROC-AUC=0.4655 val_AP=0.0236 time=19.8s\n",
      "  -> new best val_ROC-AUC=0.4655 saved checkpoint\n",
      "Epoch 7/15 — train_loss=3.2180 val_loss=1.1359 val_ROC-AUC=0.5485 val_AP=0.0283 time=17.5s\n",
      "  -> new best val_ROC-AUC=0.5485 saved checkpoint\n",
      "Epoch 8/15 — train_loss=3.0721 val_loss=1.1310 val_ROC-AUC=0.5732 val_AP=0.0301 time=13.8s\n",
      "  -> new best val_ROC-AUC=0.5732 saved checkpoint\n",
      "Epoch 9/15 — train_loss=2.9382 val_loss=1.1230 val_ROC-AUC=0.5996 val_AP=0.0321 time=13.6s\n",
      "  -> new best val_ROC-AUC=0.5996 saved checkpoint\n",
      "Epoch 10/15 — train_loss=2.9248 val_loss=1.1214 val_ROC-AUC=0.6043 val_AP=0.0326 time=12.8s\n",
      "  -> new best val_ROC-AUC=0.6043 saved checkpoint\n",
      "Epoch 11/15 — train_loss=2.8694 val_loss=1.1214 val_ROC-AUC=0.6043 val_AP=0.0326 time=13.1s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 12/15 — train_loss=2.9155 val_loss=1.1207 val_ROC-AUC=0.6073 val_AP=0.0329 time=12.9s\n",
      "  -> new best val_ROC-AUC=0.6073 saved checkpoint\n",
      "Epoch 13/15 — train_loss=2.7402 val_loss=1.1239 val_ROC-AUC=0.6052 val_AP=0.0330 time=13.3s\n",
      "EarlyStopping: metric did not improve (1/5)\n",
      "Epoch 14/15 — train_loss=2.8966 val_loss=1.1292 val_ROC-AUC=0.5994 val_AP=0.0330 time=17.2s\n",
      "EarlyStopping: metric did not improve (2/5)\n",
      "Epoch 15/15 — train_loss=2.7777 val_loss=1.1494 val_ROC-AUC=0.5776 val_AP=0.0319 time=13.5s\n",
      "EarlyStopping: metric did not improve (3/5)\n",
      "Training finished. Best epoch: 12 best_metric: 0.6073232323232323\n",
      "\n",
      "=== COSINE TEST METRICS ===\n",
      "ROC-AUC: 0.4965\n",
      "AP: 0.0499\n",
      "Accuracy: 0.9831\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "ConfusionMatrix: [[466, 0], [8, 0]]\n",
      "\n",
      "=== Transformer LR Schedule Comparison Summary ===\n",
      "constant   | ROC-AUC=0.5684, AP=0.2852, Precision=0.079, Recall=0.375\n",
      "step       | ROC-AUC=0.5341, AP=0.0386, Precision=0.000, Recall=0.000\n",
      "cosine     | ROC-AUC=0.4965, AP=0.0499, Precision=0.000, Recall=0.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "\n",
    "results = {}\n",
    "\n",
    "def train_transformer_with_scheduler(scheduler_name):\n",
    "    print(f\"\\n=== Training Transformer ({scheduler_name}) ===\")\n",
    "\n",
    "    # Build model using correct args\n",
    "    model = TransformerSmiles(\n",
    "        vocab_size=vocab_size,\n",
    "        emb_dim=128,\n",
    "        nhead=4,\n",
    "        ff_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        max_len=cfg.max_len\n",
    "    ).to(device)\n",
    "\n",
    "    # Weighted BCE loss \n",
    "    pos_weight = get_pos_weight_from_df(train_df, cfg.target_cols[0]).to(device)\n",
    "    loss_fn = get_loss_fn(\"classification\", pos_weight=pos_weight, device=device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # Scheduler options\n",
    "    if scheduler_name == \"constant\":\n",
    "        scheduler = None\n",
    "    elif scheduler_name == \"step\":\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    elif scheduler_name == \"cosine\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler: {scheduler_name}\")\n",
    "\n",
    "    # Training configuration\n",
    "    cfg_local = copy.deepcopy(cfg)\n",
    "    cfg_local.max_epochs = 15\n",
    "    cfg_local.patience = 5\n",
    "\n",
    "    # Train\n",
    "    history = run_training(\n",
    "        model,\n",
    "        train_loader_seq,\n",
    "        val_loader_seq,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss_fn,\n",
    "        cfg_local,\n",
    "        model_name=f\"transformer_{scheduler_name}\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_scores, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader_seq:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb).view(-1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(probs)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    metrics = compute_classification_metrics(all_targets, all_scores)\n",
    "    results[scheduler_name] = metrics\n",
    "    print(f\"\\n=== {scheduler_name.upper()} TEST METRICS ===\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (float, int)):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "# Run all three scheduler variants\n",
    "for sched in [\"constant\", \"step\", \"cosine\"]:\n",
    "    train_transformer_with_scheduler(sched)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Transformer LR Schedule Comparison Summary\")\n",
    "for name, m in results.items():\n",
    "    print(f\"{name:10s} | ROC-AUC={m['ROC-AUC']:.4f}, AP={m['AP']:.4f}, Precision={m['Precision']:.3f}, Recall={m['Recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc12c384-5aa3-4af2-be0d-409e6e07698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots for MLP (Fingerprints)\n",
      "Saved plots for CNN (SMILES)\n",
      "Saved plots for RNN (SMILES)\n",
      "Saved plots for Transformer (SMILES)\n",
      "Saved plots for GNN\n"
     ]
    }
   ],
   "source": [
    "models_to_plot = {\n",
    "    \"MLP (Fingerprints)\": \"fp_mlp\",\n",
    "    \"CNN (SMILES)\": \"cnn_smiles\",\n",
    "    \"RNN (SMILES)\": \"rnn_smiles\",\n",
    "    \"Transformer (SMILES)\": \"transformer_smiles\",\n",
    "    \"GNN\": \"gnn_gcn_mean\",\n",
    "}\n",
    "\n",
    "out_dir = Path(\"runs_hw1\")\n",
    "\n",
    "for title, subdir in models_to_plot.items():\n",
    "    hist_path = out_dir / subdir / \"training_history.json\"\n",
    "    if not hist_path.exists():\n",
    "        print(f\"Skipping {title}: no history file found.\")\n",
    "        continue\n",
    "\n",
    "    hist = json.load(open(hist_path))\n",
    "    epochs = hist[\"epoch\"]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title} – Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / subdir / f\"{subdir}_loss_curve.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, hist[\"train_ROC-AUC\"], label=\"Train ROC-AUC\")\n",
    "    plt.plot(epochs, hist[\"val_ROC-AUC\"], label=\"Val ROC-AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ROC-AUC\")\n",
    "    plt.title(f\"{title} – Training vs Validation ROC-AUC\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / subdir / f\"{subdir}_roc_curve.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved plots for {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455cac0-d196-4280-8cc0-0e85afffa421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
